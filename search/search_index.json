{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Airflow Examples","text":"<p>You know Python. You can write functions, loop through data, call APIs. But at some point you need to run those scripts on a schedule, retry them when they fail, chain them together in the right order, and know what happened when something breaks at 3 AM. That is what Apache Airflow is for.</p> <p>This project teaches Airflow 3.x through 107 progressive examples -- starting from a single \"hello world\" and building up to multi-API data pipelines. Every example is a real, runnable DAG. You do not need to know Docker, Bash, SQL, or any Airflow concepts to start. The pages below explain everything from scratch.</p>"},{"location":"#where-to-start","title":"Where to Start","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>What Bash, Docker, and operators are. Core Airflow concepts: DAGs, tasks, sensors, XComs, connections, and the architecture behind <code>make run</code>.</p>"},{"location":"#dag-basics","title":"DAG Basics","text":"<p>Your first DAG, DAG parameters, task dependencies, scheduling (cron, presets, timedelta), catchup/backfill, and data-aware scheduling with Assets.</p>"},{"location":"#core-operators","title":"Core Operators","text":"<p>Operator reference (Bash, Python, Docker, Sensors), BashOperator deep dive, TaskFlow decorator variants (<code>@task.bash</code>, <code>@task.short_circuit</code>, <code>@task.virtualenv</code>), EmptyOperator, BranchPythonOperator, and FileSensor.</p>"},{"location":"#provider-operators","title":"Provider Operators","text":"<p>HTTP operators and sensors, SQL operators, email notifications, SSH commands, LatestOnlyOperator, and deferrable operators with custom triggers.</p>"},{"location":"#pipelines","title":"Pipelines","text":"<p>Python ETL pipelines with <code>@task</code>, multi-source dynamic pipelines, error handling patterns, and Docker pipelines (custom images, shared volumes, GPU workloads).</p>"},{"location":"#advanced-concepts","title":"Advanced Concepts","text":"<p>Retries with exponential backoff, scheduling features, custom timetables, advanced dynamic task mapping, multiple asset dependencies, custom hooks/sensors, and incremental processing.</p>"},{"location":"#dag-examples-reference","title":"DAG Examples Reference","text":"<p>Detailed walkthrough of all 32 core example DAGs (01--32), covering fundamentals, TaskFlow, branching, sensors, task groups, dynamic mapping, assets, Docker operators, and more.</p>"},{"location":"#data-pipelines","title":"Data Pipelines","text":"<p>Parquet aggregation, DHIS2 metadata pipelines (organisation units, data elements, indicators, GeoJSON geometry, parallel exports), and file-based ETL (CSV landing zones, JSON ingestion, multi-file batch, error handling, incremental processing).</p>"},{"location":"#quality-testing","title":"Quality &amp; Testing","text":"<p>Data quality dimensions (schema, accuracy, freshness, completeness, consistency), alerting and SLA monitoring, webhook notifications, failure escalation, and DAG testing patterns (testable architecture, mocking, <code>dag.test()</code>, parameterized pipelines).</p>"},{"location":"#api-pipelines","title":"API Pipelines","text":"<p>Weather and climate APIs (Open-Meteo forecast, air quality, marine, flood), geographic and economic APIs (REST Countries, World Bank, Frankfurter), geophysical APIs (USGS earthquakes, UK carbon intensity), global health indicators (WHO, World Bank), and advanced multi-API data engineering (staging layers, quality on live data, SCD Type 2).</p>"},{"location":"#integrations","title":"Integrations","text":"<p>dbt integration, S3 object storage with RustFS (boto3 and ObjectStoragePath), Human-in-the-Loop operators, and variable-driven scheduling.</p>"},{"location":"#reference","title":"Reference","text":"<p>Executors (Local, Celery, Kubernetes), Airflow 3.x migration notes, architecture diagrams, infrastructure details, and external links.</p>"},{"location":"#rest-api-cli","title":"REST API &amp; CLI","text":"<p>Typer CLI wrapping the Airflow REST API for managing DAGs, connections, variables, and pools.</p>"},{"location":"advanced/","title":"Advanced Concepts","text":""},{"location":"advanced/#advanced-retries-and-timeouts","title":"Advanced Retries and Timeouts","text":""},{"location":"advanced/#exponential-backoff","title":"Exponential Backoff","text":"<p>Instead of fixed delays between retries, exponential backoff doubles the wait each time:</p> <pre><code>from datetime import timedelta\n\ntask = PythonOperator(\n    task_id=\"api_call\",\n    python_callable=call_api,\n    retries=5,\n    retry_delay=timedelta(seconds=2),\n    retry_exponential_backoff=True,    # 2s, 4s, 8s, 16s, 30s\n    max_retry_delay=timedelta(seconds=30),\n)\n</code></pre>"},{"location":"advanced/#execution_timeout","title":"execution_timeout","text":"<p>Kills a task with SIGTERM if it exceeds the timeout. Use for tasks that might hang:</p> <pre><code>task = PythonOperator(\n    task_id=\"etl\",\n    python_callable=run_etl,\n    execution_timeout=timedelta(minutes=30),\n)\n</code></pre>"},{"location":"advanced/#dag-level-callbacks","title":"DAG-Level Callbacks","text":"<p>Fire when the entire DAG run completes (not per-task):</p> <pre><code>def dag_success(context):\n    print(f\"DAG {context['dag_run'].dag_id} succeeded!\")\n\ndef dag_failure(context):\n    print(f\"DAG {context['dag_run'].dag_id} failed!\")\n\nwith DAG(\n    dag_id=\"my_dag\",\n    on_success_callback=dag_success,\n    on_failure_callback=dag_failure,\n):\n    ...\n</code></pre> <p>See: <code>dags/51_advanced_retries.py</code></p>"},{"location":"advanced/#scheduling-features","title":"Scheduling Features","text":""},{"location":"advanced/#depends_on_past","title":"depends_on_past","text":"<p>When <code>True</code>, a task instance will not run until the same task in the previous DAG run has succeeded:</p> <pre><code>extract = PythonOperator(\n    task_id=\"extract\",\n    python_callable=extract_data,\n    depends_on_past=True,  # Won't run until previous run's extract succeeded\n)\n</code></pre>"},{"location":"advanced/#wait_for_downstream","title":"wait_for_downstream","text":"<p>Stricter than <code>depends_on_past</code>: waits for both the task itself AND all its downstream tasks from the previous run to complete:</p> <pre><code>transform = PythonOperator(\n    task_id=\"transform\",\n    python_callable=transform_data,\n    wait_for_downstream=True,\n)\n</code></pre>"},{"location":"advanced/#max_active_runs","title":"max_active_runs","text":"<p>Limits how many DAG runs can execute simultaneously:</p> <pre><code>with DAG(\n    dag_id=\"sequential_pipeline\",\n    max_active_runs=1,  # Only one run at a time\n    schedule=\"0 6 * * *\",\n):\n    ...\n</code></pre>"},{"location":"advanced/#cron-expressions","title":"Cron Expressions","text":"Expression Description <code>0 6 * * *</code> Daily at 06:00 UTC <code>*/15 * * * *</code> Every 15 minutes <code>0 0 1 * *</code> First day of each month <code>0 12 * * MON</code> Monday at noon <code>0 6 * * 1-5</code> Weekdays at 06:00 <code>@hourly</code> Every hour (preset) <code>@daily</code> Every day at midnight (preset) <code>timedelta(hours=2)</code> Every 2 hours (timedelta) <p>See: <code>dags/52_scheduling_features.py</code></p>"},{"location":"advanced/#custom-timetables","title":"Custom Timetables","text":"<p>For scheduling logic that can't be expressed with cron or timedelta, create a custom <code>Timetable</code> subclass:</p> <pre><code>from airflow.timetables.base import DagRunInfo, DataInterval, Timetable\nfrom pendulum import DateTime\n\nclass WeekdayTimetable(Timetable):\n    def next_dagrun_info(self, *, last_automated_dagrun, restriction):\n        # Skip weekends, advance to next Monday\n        ...\n        return DagRunInfo.interval(start=start, end=end)\n\n    def infer_manual_data_interval(self, *, run_after):\n        # Handle manually triggered runs\n        ...\n</code></pre> <p>Register via a plugin:</p> <pre><code>class TimetablePlugin(AirflowPlugin):\n    name = \"custom_timetables\"\n    timetables = [WeekdayTimetable]\n</code></pre> <p>Use cases: business days only, market trading hours, skip holidays, fiscal calendar periods.</p> <p>See: <code>dags/53_custom_timetable.py</code></p>"},{"location":"advanced/#advanced-dynamic-task-mapping","title":"Advanced Dynamic Task Mapping","text":""},{"location":"advanced/#expand_kwargs-zip-style","title":"expand_kwargs (Zip-Style)","text":"<p><code>expand_kwargs()</code> maps a list of dicts. Each dict provides keyword arguments for one mapped task instance. This is the \"zip\" pattern -- parameters are paired together:</p> <pre><code>@task\ndef generate_configs() -&gt; list[dict[str, object]]:\n    return [\n        {\"station_id\": \"oslo_01\", \"lat\": 59.91, \"lon\": 10.75},\n        {\"station_id\": \"bergen_01\", \"lat\": 60.39, \"lon\": 5.32},\n    ]\n\n@task\ndef process(station_id: str, lat: float, lon: float) -&gt; dict:\n    return {\"station\": station_id, \"lat\": lat, \"lon\": lon}\n\nconfigs = generate_configs()\nresults = process.expand_kwargs(configs)\n</code></pre>"},{"location":"advanced/#combined-expand-patterns","title":"Combined Expand Patterns","text":"<p>Use <code>.partial()</code> for static arguments and <code>.expand()</code> for dynamic ones:</p> <pre><code># Same variable for all instances, different dates\nextract.partial(variable=\"temperature\").expand(date=dates)\n\n# Same date for all instances, different variables\nextract.partial(date=\"2024-01-01\").expand(variable=variables)\n</code></pre> <p>See: <code>dags/54_advanced_dynamic_mapping.py</code></p>"},{"location":"advanced/#multiple-asset-dependencies","title":"Multiple Asset Dependencies","text":""},{"location":"advanced/#multi-outlet-producers","title":"Multi-Outlet Producers","text":"<p>A single task can update multiple assets:</p> <pre><code>from airflow.sdk import Asset\n\ntemp_data = Asset(\"s3://bucket/temperature.parquet\")\nhumidity_data = Asset(\"s3://bucket/humidity.parquet\")\n\nproduce_all = PythonOperator(\n    task_id=\"produce_all\",\n    python_callable=produce_weather,\n    outlets=[temp_data, humidity_data],  # Updates both assets\n)\n</code></pre>"},{"location":"advanced/#and-logic-consumer","title":"AND-Logic Consumer","text":"<p>A consumer DAG that waits for ALL specified assets to update:</p> <pre><code>with DAG(\n    dag_id=\"combined_analysis\",\n    schedule=[temp_data, humidity_data],  # Runs when BOTH update\n):\n    ...\n</code></pre> <p>See: <code>dags/55_multiple_assets.py</code></p>"},{"location":"advanced/#custom-hooks-and-sensors","title":"Custom Hooks and Sensors","text":""},{"location":"advanced/#custom-hook-basehook","title":"Custom Hook (BaseHook)","text":"<p>Hooks encapsulate reusable connection logic shared across operators and sensors:</p> <pre><code>from airflow.hooks.base import BaseHook\n\nclass WeatherApiHook(BaseHook):\n    conn_name_attr = \"weather_conn_id\"\n    default_conn_name = \"weather_default\"\n    conn_type = \"http\"\n    hook_name = \"Weather API\"\n\n    def __init__(self, weather_conn_id=\"weather_default\"):\n        super().__init__()\n        self.weather_conn_id = weather_conn_id\n\n    def get_conn(self):\n        conn = self.get_connection(self.weather_conn_id)\n        return {\"host\": conn.host, \"token\": conn.password}\n\n    def check_data_available(self, station_id, date):\n        # Call external API\n        return True\n</code></pre>"},{"location":"advanced/#custom-sensor-basesensoroperator","title":"Custom Sensor (BaseSensorOperator)","text":"<p>Sensors wait for an external condition using a hook:</p> <pre><code>from airflow.sensors.base import BaseSensorOperator\n\nclass WeatherDataSensor(BaseSensorOperator):\n    template_fields = (\"station_id\", \"check_date\")\n\n    def __init__(self, station_id, check_date, **kwargs):\n        super().__init__(**kwargs)\n        self.station_id = station_id\n        self.check_date = check_date\n\n    def poke(self, context):\n        hook = WeatherApiHook()\n        return hook.check_data_available(self.station_id, self.check_date)\n</code></pre> <p>See: <code>dags/56_custom_hook_and_sensor.py</code>, <code>src/airflow_examples/hooks.py</code>, <code>src/airflow_examples/sensors.py</code></p>"},{"location":"advanced/#incremental-processing-pattern","title":"Incremental Processing Pattern","text":"<p>Process only data for the current execution date, making the pipeline idempotent:</p> <pre><code>@task\ndef extract(**context):\n    date = context[\"ds\"]  # \"2024-01-15\"\n    # Only extract this date's data\n    return db.query(f\"SELECT * FROM raw WHERE date = '{date}'\")\n\n@task\ndef load(data, **context):\n    date = context[\"ds\"]\n    # Idempotent upsert: safe to re-run\n    db.execute(\"\"\"\n        INSERT INTO prod (...) VALUES (...)\n        ON CONFLICT (date, station) DO UPDATE SET ...\n    \"\"\")\n</code></pre> <p>Key principles:</p> <ul> <li>Filter by <code>{{ ds }}</code>: Each run processes only its own date partition</li> <li>Idempotent loads: Use <code>INSERT ... ON CONFLICT DO UPDATE</code> (upsert) or <code>TRUNCATE</code> + <code>INSERT</code></li> <li>Re-runnable: Any failed run can be safely re-executed without duplicates</li> <li>Backfill-friendly: Works with <code>airflow dags backfill --start-date --end-date</code></li> </ul> <p>See: <code>dags/56_custom_hook_and_sensor.py</code></p>"},{"location":"cli/","title":"REST API &amp; CLI","text":"<p>Airflow has a web UI, and it is great for browsing DAGs, checking logs, and manually triggering runs. But at some point you will want to do these things from a script -- trigger a DAG from a CI/CD pipeline, bulk-unpause all your DAGs after a deployment, or poll a run until it finishes so your next step can proceed. That is what the REST API is for.</p> <p>This page covers two things:</p> <ol> <li>The Airflow 3 REST API -- raw HTTP endpoints you can hit with curl or any HTTP client</li> <li>The <code>af</code> CLI -- a Typer wrapper around that API, installed with this project, that saves    you from typing auth headers over and over</li> </ol> <p>We will start with curl so you understand what is actually happening over the wire, then show the <code>af</code> shorthand for each operation.</p>"},{"location":"cli/#connecting","title":"Connecting","text":""},{"location":"cli/#base-url","title":"Base URL","text":"<p>With the Docker Compose setup from this project, the API lives at:</p> <pre><code>http://localhost:8081/api/v2/\n</code></pre> <p>Port 8081 is the web server port set in the <code>docker-compose.yml</code>. The <code>/api/v2/</code> prefix is new in Airflow 3 -- Airflow 2 used <code>/api/v1/</code>.</p> <p>Airflow ships a Swagger UI at http://localhost:8081/api/v2/docs where you can explore every endpoint interactively.</p>"},{"location":"cli/#authentication-jwt-tokens","title":"Authentication: JWT Tokens","text":"<p>Airflow 3 uses JWT (JSON Web Token) authentication. You POST your credentials to <code>/auth/token</code> and get back an access token. Then you pass that token in the <code>Authorization</code> header on every subsequent request.</p> <pre><code># Step 1: Get a token\nTOKEN=$(curl -s -X POST http://localhost:8081/auth/token \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"admin\"}' \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['access_token'])\")\n\n# Step 2: Use the token\ncurl -s http://localhost:8081/api/v2/dags \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>The token response looks like:</p> <pre><code>{\n  \"access_token\": \"eyJhbGciOiJIUzI1NiIs...\",\n  \"token_type\": \"bearer\"\n}\n</code></pre> <p>This is a change from Airflow 2, which used basic auth (username/password on every request) or session cookies.</p>"},{"location":"cli/#unauthenticated-endpoints","title":"Unauthenticated Endpoints","text":"<p>Two endpoints do not require a token:</p> <pre><code># Airflow version\ncurl -s http://localhost:8081/api/v2/version\n\n# Component health (scheduler, triggerer, dag_processor)\ncurl -s http://localhost:8081/api/v2/monitor/health\n</code></pre> <p>These are useful for health checks in Docker Compose, Kubernetes, or monitoring scripts.</p>"},{"location":"cli/#the-af-cli","title":"The <code>af</code> CLI","text":"<p>The <code>af</code> command is a Typer CLI that wraps every API call shown on this page. It handles JWT authentication transparently -- you never have to manually fetch tokens.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>It is already installed when you set up this project:</p> <pre><code>uv sync\n</code></pre> <p>The entry point is defined in <code>pyproject.toml</code> under <code>[project.scripts]</code>, so <code>uv sync</code> makes the <code>af</code> command available in your virtualenv.</p>"},{"location":"cli/#global-options","title":"Global Options","text":"<pre><code>af --help\n</code></pre> Option Env Var Default Description <code>--base-url</code> <code>AIRFLOW_URL</code> <code>http://localhost:8081</code> Airflow web server URL <code>--username</code> <code>AIRFLOW_USER</code> <code>admin</code> Username for token auth <code>--password</code> <code>AIRFLOW_PASS</code> <code>admin</code> Password for token auth"},{"location":"cli/#command-groups","title":"Command Groups","text":"<pre><code>af version              Show Airflow version\naf health               Show component health\n\naf dags list            List all DAGs\naf dags info &lt;id&gt;       Show DAG details\naf dags pause &lt;id&gt;      Pause a DAG\naf dags unpause &lt;id&gt;    Unpause a DAG\naf dags trigger &lt;id&gt;    Trigger a DAG run [--wait] [--verbose/-v] [--conf] [--timeout] [--interval]\naf dags delete &lt;id&gt;     Delete a DAG\n\naf runs list &lt;dag&gt;      List runs for a DAG\naf runs get &lt;dag&gt; &lt;run&gt; Show run details\naf runs tasks &lt;dag&gt; &lt;run&gt;  List task instances\naf runs logs &lt;dag&gt; &lt;run&gt; &lt;task&gt;  Show task logs\naf runs wait &lt;dag&gt; &lt;run&gt;   Poll until terminal state\n\naf vars list            List all variables\naf vars get &lt;key&gt;       Get variable value\naf vars set &lt;key&gt; &lt;val&gt; Create or update variable\naf vars delete &lt;key&gt;    Delete variable\n\naf pools list           List all pools\naf pools get &lt;name&gt;     Get pool details\naf pools set &lt;name&gt; &lt;slots&gt;  Create or update pool\naf pools delete &lt;name&gt;  Delete pool\n\naf conns list           List all connections\naf conns get &lt;id&gt;       Get connection details\n\naf xcoms list &lt;dag&gt; &lt;task&gt; [run]  List XCom entries\naf xcoms list &lt;dag&gt; &lt;task&gt; --latest  List XCom entries (latest run)\naf xcoms get &lt;dag&gt; &lt;task&gt; [run]   Get XCom value (default key: return_value)\naf xcoms get &lt;dag&gt; &lt;task&gt; --latest  Get XCom value (latest run)\n</code></pre>"},{"location":"cli/#version-health","title":"Version &amp; Health","text":"<p>These are the simplest endpoints and do not require authentication.</p>"},{"location":"cli/#get-apiv2version","title":"GET /api/v2/version","text":"<pre><code>curl -s http://localhost:8081/api/v2/version\n</code></pre> <pre><code>{\n  \"version\": \"3.0.1\",\n  \"git_version\": null\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af version\n# 3.0.1\n</code></pre>"},{"location":"cli/#get-apiv2monitorhealth","title":"GET /api/v2/monitor/health","text":"<pre><code>curl -s http://localhost:8081/api/v2/monitor/health | python3 -m json.tool\n</code></pre> <pre><code>{\n  \"metadatabase\": { \"status\": \"healthy\" },\n  \"scheduler\": {\n    \"status\": \"healthy\",\n    \"latest_scheduler_heartbeat\": \"2025-01-15T10:30:00+00:00\"\n  },\n  \"triggerer\": {\n    \"status\": \"healthy\",\n    \"latest_triggerer_heartbeat\": \"2025-01-15T10:30:00+00:00\"\n  },\n  \"dag_processor\": {\n    \"status\": \"healthy\",\n    \"latest_dag_processor_heartbeat\": \"2025-01-15T10:30:00+00:00\"\n  }\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af health\n</code></pre>"},{"location":"cli/#dags","title":"DAGs","text":""},{"location":"cli/#list-dags","title":"List DAGs","text":"<p>GET /api/v2/dags</p> <pre><code>curl -s http://localhost:8081/api/v2/dags \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>The response includes a <code>dags</code> array and a <code>total_entries</code> count. You can filter with query parameters:</p> Parameter Example Description <code>limit</code> <code>?limit=10</code> Max results (default 100) <code>offset</code> <code>?offset=10</code> Pagination offset <code>paused</code> <code>?paused=true</code> Only paused / unpaused DAGs <code>dag_id_pattern</code> <code>?dag_id_pattern=001%</code> Filter by DAG ID pattern <pre><code># Only unpaused DAGs, first 5\ncurl -s \"http://localhost:8081/api/v2/dags?paused=false&amp;limit=5\" \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>With the CLI:</p> <pre><code>af dags list\n</code></pre>"},{"location":"cli/#dag-details","title":"DAG Details","text":"<p>GET /api/v2/dags/{dag_id}</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/001_hello_world \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <pre><code>{\n  \"dag_id\": \"001_hello_world\",\n  \"description\": \"Hello World -- your first DAG.\",\n  \"is_paused\": false,\n  \"owners\": [\"airflow\"],\n  \"timetable_summary\": null,\n  \"tags\": [{\"name\": \"basics\"}],\n  \"...\": \"...\"\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af dags info 001_hello_world\n</code></pre>"},{"location":"cli/#pause-unpause","title":"Pause / Unpause","text":"<p>PATCH /api/v2/dags/{dag_id}</p> <pre><code># Pause\ncurl -s -X PATCH http://localhost:8081/api/v2/dags/001_hello_world \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"is_paused\": true}'\n\n# Unpause\ncurl -s -X PATCH http://localhost:8081/api/v2/dags/001_hello_world \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"is_paused\": false}'\n</code></pre> <p>With the CLI:</p> <pre><code>af dags pause 001_hello_world\naf dags unpause 001_hello_world\n</code></pre>"},{"location":"cli/#trigger-a-dag-run","title":"Trigger a DAG Run","text":"<p>POST /api/v2/dags/{dag_id}/dagRuns</p> <p>This is the most common API operation. You must provide a <code>logical_date</code> -- Airflow 3 requires it and will return 422 if you omit it.</p> <pre><code>curl -s -X POST http://localhost:8081/api/v2/dags/001_hello_world/dagRuns \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"logical_date\": \"2025-01-15T12:00:00Z\"\n  }'\n</code></pre> <p>To pass configuration to the DAG:</p> <pre><code>curl -s -X POST http://localhost:8081/api/v2/dags/001_hello_world/dagRuns \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"logical_date\": \"2025-01-15T12:00:00Z\",\n    \"conf\": {\"key\": \"value\"}\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"dag_run_id\": \"manual__2025-01-15T12:00:00+00:00\",\n  \"dag_id\": \"001_hello_world\",\n  \"logical_date\": \"2025-01-15T12:00:00+00:00\",\n  \"state\": \"queued\",\n  \"conf\": {},\n  \"...\": \"...\"\n}\n</code></pre> <p>With the CLI (it generates <code>logical_date</code> automatically):</p> <pre><code>af dags trigger 001_hello_world\naf dags trigger 001_hello_world --conf '{\"key\": \"value\"}'\n</code></pre>"},{"location":"cli/#trigger-options","title":"Trigger Options","text":"Flag Default Description <code>--wait</code> off Poll until terminal state, then print task logs <code>--verbose</code> / <code>-v</code> off Show full task logs (requires <code>--wait</code>) <code>--conf</code> none JSON config string passed to the DAG run <code>--timeout</code> 300 Max seconds to wait (requires <code>--wait</code>) <code>--interval</code> 5 Poll interval in seconds (requires <code>--wait</code>)"},{"location":"cli/#wait-mode","title":"Wait Mode","text":"<p><code>--wait</code> triggers the run, polls until it reaches a terminal state (<code>success</code>, <code>failed</code>, or <code>upstream_failed</code>), and then prints logs for every task instance:</p> <pre><code>af dags trigger 110_dhis2_connection_basics --wait\n</code></pre> <p>Output:</p> <pre><code>Triggered: 110_dhis2_connection_basics\n  run_id: manual__2025-01-15T12:00:00+00:00\n  state:  queued\n  [  0s] state=running\n  [  5s] state=running\n  [ 10s] state=success\nTerminal state: success\n\n--- show_connection [success] ---\n  12:00:03  Connection details (password redacted):\n  12:00:03    conn_id: dhis2_default\n  12:00:03    conn_type: http\n  ...\n\n--- fetch_org_unit_count [success] ---\n  12:00:05  Organisation unit count: 1575\n</code></pre> <p>By default, logs show only task stdout output, warnings, and errors. Add <code>--verbose</code> / <code>-v</code> to include all log entries (Airflow internals, task lifecycle events):</p> <pre><code>af dags trigger 110_dhis2_connection_basics --wait --verbose\n</code></pre>"},{"location":"cli/#auto-unpause","title":"Auto-Unpause","text":"<p>If the DAG is paused when you trigger it, <code>af dags trigger</code> will automatically unpause it so the run executes, then re-pause it after the run completes (or if <code>--wait</code> is not used, after the trigger returns). This means you do not need to manually unpause DAGs before triggering:</p> <pre><code># Even if 110_dhis2_connection_basics is paused, this works:\naf dags trigger 110_dhis2_connection_basics --wait\n# Output includes:\n#   Unpaused: 110_dhis2_connection_basics\n#   Triggered: 110_dhis2_connection_basics\n#   ...\n#   Re-paused: 110_dhis2_connection_basics\n</code></pre> <p>logical_date must be unique</p> <p>Each DAG run needs a unique <code>logical_date</code>. If you try to trigger with a date that already has a run, you will get a 409 Conflict. The <code>af</code> CLI avoids this by using the current UTC timestamp.</p>"},{"location":"cli/#delete-a-dag","title":"Delete a DAG","text":"<p>DELETE /api/v2/dags/{dag_id}</p> <pre><code>curl -s -X DELETE http://localhost:8081/api/v2/dags/001_hello_world \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>With the CLI:</p> <pre><code>af dags delete 001_hello_world\n</code></pre> <p>This removes the DAG metadata from the database. The DAG file on disk is not affected -- if the scheduler finds it on the next parse cycle, it will recreate the DAG entry.</p>"},{"location":"cli/#dag-runs","title":"DAG Runs","text":""},{"location":"cli/#list-runs","title":"List Runs","text":"<p>GET /api/v2/dags/{dag_id}/dagRuns</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/001_hello_world/dagRuns \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>Response:</p> <pre><code>{\n  \"dag_runs\": [\n    {\n      \"dag_run_id\": \"manual__2025-01-15T12:00:00+00:00\",\n      \"state\": \"success\",\n      \"logical_date\": \"2025-01-15T12:00:00+00:00\",\n      \"start_date\": \"2025-01-15T12:00:01+00:00\",\n      \"end_date\": \"2025-01-15T12:00:05+00:00\"\n    }\n  ],\n  \"total_entries\": 1\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af runs list 001_hello_world\n</code></pre>"},{"location":"cli/#run-details","title":"Run Details","text":"<p>GET /api/v2/dags/{dag_id}/dagRuns/{run_id}</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/001_hello_world/dagRuns/manual__2025-01-15T12:00:00+00:00 \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>With the CLI:</p> <pre><code>af runs get 001_hello_world \"manual__2025-01-15T12:00:00+00:00\"\n</code></pre>"},{"location":"cli/#task-instances","title":"Task Instances","text":"<p>GET /api/v2/dags/{dag_id}/dagRuns/{run_id}/taskInstances</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/001_hello_world/dagRuns/manual__2025-01-15T12:00:00+00:00/taskInstances \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>Response:</p> <pre><code>{\n  \"task_instances\": [\n    {\n      \"task_id\": \"say_hello\",\n      \"state\": \"success\",\n      \"try_number\": 1,\n      \"start_date\": \"2025-01-15T12:00:01+00:00\",\n      \"end_date\": \"2025-01-15T12:00:02+00:00\"\n    }\n  ],\n  \"total_entries\": 1\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af runs tasks 001_hello_world \"manual__2025-01-15T12:00:00+00:00\"\n</code></pre>"},{"location":"cli/#task-logs","title":"Task Logs","text":"<p>GET /api/v2/dags/{dag_id}/dagRuns/{run_id}/taskInstances/{task_id}/logs/{try_number}</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/001_hello_world/dagRuns/manual__2025-01-15T12:00:00+00:00/taskInstances/say_hello/logs/1 \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>This returns plain text (the task log output), not JSON.</p> <p>With the CLI:</p> <pre><code>af runs logs 001_hello_world \"manual__2025-01-15T12:00:00+00:00\" say_hello\naf runs logs 001_hello_world \"manual__2025-01-15T12:00:00+00:00\" say_hello --try 2\n</code></pre>"},{"location":"cli/#waiting-for-a-run-to-finish","title":"Waiting for a Run to Finish","text":"<p>There is no built-in streaming endpoint in the current Airflow 3 API, so the standard approach is to poll the run status until it reaches a terminal state (<code>success</code>, <code>failed</code>, or <code>upstream_failed</code>).</p> <p>The <code>af</code> CLI has a <code>wait</code> command that does this for you:</p> <pre><code>af runs wait 001_hello_world \"manual__2025-01-15T12:00:00+00:00\" --timeout 300 --interval 5\n</code></pre> <p>Output:</p> <pre><code>  [  0s] state=running\n  [  5s] state=running\n  [ 10s] state=success\nTerminal state: success\n</code></pre> <p>If you want to do this with curl in a script:</p> <pre><code>TOKEN=$(curl -s -X POST http://localhost:8081/auth/token \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"admin\"}' \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['access_token'])\")\n\nRUN_ID=\"manual__2025-01-15T12:00:00+00:00\"\n\nwhile true; do\n  STATE=$(curl -s http://localhost:8081/api/v2/dags/001_hello_world/dagRuns/$RUN_ID \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    | python3 -c \"import sys,json; print(json.load(sys.stdin)['state'])\")\n\n  echo \"state=$STATE\"\n\n  case $STATE in\n    success|failed|upstream_failed) break ;;\n  esac\n\n  sleep 5\ndone\n</code></pre>"},{"location":"cli/#variables","title":"Variables","text":"<p>Variables are key-value pairs stored in the Airflow database. DAGs can read them at parse time or runtime. The API lets you manage them without touching the UI.</p>"},{"location":"cli/#list-variables","title":"List Variables","text":"<p>GET /api/v2/variables</p> <pre><code>curl -s http://localhost:8081/api/v2/variables \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <pre><code>{\n  \"variables\": [\n    { \"key\": \"dag_107_schedule\", \"value\": \"@daily\" }\n  ],\n  \"total_entries\": 1\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af vars list\n</code></pre>"},{"location":"cli/#get-a-variable","title":"Get a Variable","text":"<p>GET /api/v2/variables/{key}</p> <pre><code>curl -s http://localhost:8081/api/v2/variables/dag_107_schedule \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <pre><code>{ \"key\": \"dag_107_schedule\", \"value\": \"@daily\" }\n</code></pre> <p>With the CLI:</p> <pre><code>af vars get dag_107_schedule\n</code></pre>"},{"location":"cli/#create-a-variable","title":"Create a Variable","text":"<p>POST /api/v2/variables</p> <pre><code>curl -s -X POST http://localhost:8081/api/v2/variables \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"key\": \"my_api_key\", \"value\": \"abc123\"}'\n</code></pre> <p>With the CLI:</p> <pre><code>af vars set my_api_key abc123\n</code></pre> <p>The <code>af vars set</code> command is smart -- it tries PATCH first (update), and if the variable does not exist (404), it falls back to POST (create).</p>"},{"location":"cli/#update-a-variable","title":"Update a Variable","text":"<p>PATCH /api/v2/variables/{key}</p> <pre><code>curl -s -X PATCH http://localhost:8081/api/v2/variables/dag_107_schedule \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"key\": \"dag_107_schedule\", \"value\": \"@hourly\"}'\n</code></pre> <p>With the CLI:</p> <pre><code>af vars set dag_107_schedule @hourly\n</code></pre> <p>Quirk: key is required in the PATCH body</p> <p>Even though the key is in the URL path, the Airflow 3 API also requires it in the request body. If you omit it, you get a 422 validation error.</p>"},{"location":"cli/#delete-a-variable","title":"Delete a Variable","text":"<p>DELETE /api/v2/variables/{key}</p> <pre><code>curl -s -X DELETE http://localhost:8081/api/v2/variables/dag_107_schedule \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>With the CLI:</p> <pre><code>af vars delete dag_107_schedule\n</code></pre>"},{"location":"cli/#pools","title":"Pools","text":"<p>Pools limit how many tasks can run concurrently for a given resource. If you have an API that only allows 3 concurrent requests, you create a pool with 3 slots and assign your tasks to it.</p>"},{"location":"cli/#list-pools","title":"List Pools","text":"<p>GET /api/v2/pools</p> <pre><code>curl -s http://localhost:8081/api/v2/pools \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <pre><code>{\n  \"pools\": [\n    {\n      \"name\": \"default_pool\",\n      \"slots\": 128,\n      \"running_slots\": 2,\n      \"queued_slots\": 0\n    }\n  ],\n  \"total_entries\": 1\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af pools list\n</code></pre>"},{"location":"cli/#get-a-pool","title":"Get a Pool","text":"<p>GET /api/v2/pools/{name}</p> <pre><code>curl -s http://localhost:8081/api/v2/pools/default_pool \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>With the CLI:</p> <pre><code>af pools get default_pool\n</code></pre>"},{"location":"cli/#create-a-pool","title":"Create a Pool","text":"<p>POST /api/v2/pools</p> <pre><code>curl -s -X POST http://localhost:8081/api/v2/pools \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"api_pool\", \"slots\": 3}'\n</code></pre> <p>With the CLI:</p> <pre><code>af pools set api_pool 3\n</code></pre> <p>Like <code>af vars set</code>, the <code>af pools set</code> command tries PATCH first, then falls back to POST.</p>"},{"location":"cli/#update-a-pool","title":"Update a Pool","text":"<p>PATCH /api/v2/pools/{name}</p> <pre><code>curl -s -X PATCH http://localhost:8081/api/v2/pools/api_pool \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"api_pool\", \"slots\": 5}'\n</code></pre> <p>With the CLI:</p> <pre><code>af pools set api_pool 5\n</code></pre> <p>Pool name is not modifiable</p> <p>In Airflow 3 you cannot rename a pool via PATCH. The <code>name</code> field in the body must match the name in the URL. If you need a different name, delete the old pool and create a new one.</p>"},{"location":"cli/#delete-a-pool","title":"Delete a Pool","text":"<p>DELETE /api/v2/pools/{name}</p> <pre><code>curl -s -X DELETE http://localhost:8081/api/v2/pools/api_pool \\\n  -H \"Authorization: Bearer $TOKEN\"\n</code></pre> <p>With the CLI:</p> <pre><code>af pools delete api_pool\n</code></pre>"},{"location":"cli/#connections","title":"Connections","text":"<p>Connections store credentials and host information for external systems (databases, APIs, cloud services). The API exposes read-only access -- you can list and inspect connections but creating/modifying them is typically done via the UI or environment variables.</p>"},{"location":"cli/#list-connections","title":"List Connections","text":"<p>GET /api/v2/connections</p> <pre><code>curl -s http://localhost:8081/api/v2/connections \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <pre><code>{\n  \"connections\": [\n    {\n      \"connection_id\": \"postgres_default\",\n      \"conn_type\": \"postgres\",\n      \"host\": \"postgres\",\n      \"port\": 5432\n    }\n  ],\n  \"total_entries\": 1\n}\n</code></pre> <p>With the CLI:</p> <pre><code>af conns list\n</code></pre>"},{"location":"cli/#get-a-connection","title":"Get a Connection","text":"<p>GET /api/v2/connections/{connection_id}</p> <pre><code>curl -s http://localhost:8081/api/v2/connections/postgres_default \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>With the CLI:</p> <pre><code>af conns get postgres_default\n</code></pre>"},{"location":"cli/#xcoms","title":"XComs","text":"<p>XCom (\"cross-communication\") entries are how tasks pass data to each other. When a PythonOperator returns a value, Airflow stores it as an XCom entry with the key <code>return_value</code>. The API lets you inspect these values without opening the web UI.</p>"},{"location":"cli/#list-xcom-entries","title":"List XCom Entries","text":"<p>GET /api/v2/dags/{dag_id}/dagRuns/{run_id}/taskInstances/{task_id}/xcomEntries</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/002_python_operator/dagRuns/manual__2025-01-15T12:00:00+00:00/taskInstances/greet/xcomEntries \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <p>Response:</p> <pre><code>{\n  \"xcom_entries\": [\n    {\n      \"key\": \"return_value\",\n      \"value\": \"Hello, Airflow!\",\n      \"timestamp\": \"2025-01-15T12:00:05+00:00\"\n    }\n  ],\n  \"total_entries\": 1\n}\n</code></pre> <p>With the CLI:</p> <pre><code># With an explicit run ID\naf xcoms list 002_python_operator greet \"manual__2025-01-15T12:00:00+00:00\"\n\n# Or use --latest to automatically pick the most recent run\naf xcoms list 002_python_operator greet --latest\n</code></pre>"},{"location":"cli/#get-an-xcom-value","title":"Get an XCom Value","text":"<p>GET /api/v2/dags/{dag_id}/dagRuns/{run_id}/taskInstances/{task_id}/xcomEntries/{key}</p> <pre><code>curl -s http://localhost:8081/api/v2/dags/002_python_operator/dagRuns/manual__2025-01-15T12:00:00+00:00/taskInstances/greet/xcomEntries/return_value \\\n  -H \"Authorization: Bearer $TOKEN\" | python3 -m json.tool\n</code></pre> <pre><code>{\n  \"key\": \"return_value\",\n  \"value\": \"Hello, Airflow!\",\n  \"timestamp\": \"2025-01-15T12:00:05+00:00\",\n  \"dag_id\": \"002_python_operator\",\n  \"run_id\": \"manual__2025-01-15T12:00:00+00:00\",\n  \"task_id\": \"greet\"\n}\n</code></pre> <p>With the CLI:</p> <pre><code># Default key is return_value\naf xcoms get 002_python_operator greet \"manual__2025-01-15T12:00:00+00:00\"\n\n# Use --latest instead of a run ID\naf xcoms get 002_python_operator greet --latest\n\n# Explicit key\naf xcoms get 002_python_operator greet --latest --key return_value\n\n# Full JSON response\naf xcoms get 002_python_operator greet --latest --json\n</code></pre>"},{"location":"cli/#airflow-3-api-changes","title":"Airflow 3 API Changes","text":"<p>If you are coming from Airflow 2, here are the key differences:</p> What Changed Airflow 2 Airflow 3 Base path <code>/api/v1/</code> <code>/api/v2/</code> Authentication Basic auth or session cookies JWT tokens via <code>/auth/token</code> Trigger field <code>execution_date</code> <code>logical_date</code> (required) Validation errors 400 Bad Request 422 Unprocessable Entity Pool rename Allowed via PATCH Not allowed -- name is immutable Variable PATCH body Only <code>value</code> needed <code>key</code> required in body alongside <code>value</code> Swagger UI <code>/api/v1/ui/</code> <code>/api/v2/docs</code> <p>The <code>execution_date</code> to <code>logical_date</code> rename is the most common thing that breaks existing scripts. If you see 422 errors after upgrading, check that your trigger payloads use <code>logical_date</code>.</p>"},{"location":"cli/#recipes","title":"Recipes","text":""},{"location":"cli/#trigger-wait-check","title":"Trigger + Wait + Check","text":"<p>The most common pattern: trigger a DAG, wait for it to finish, then check the result.</p> <p>With <code>af</code>:</p> <pre><code>af dags trigger 001_hello_world --wait\n</code></pre> <p>This single command triggers the run, polls until it completes, and prints task logs. It exits with code 0 on success and code 1 on failure, making it suitable for CI/CD scripts. Add <code>--verbose</code> to see full Airflow log output instead of just task stdout.</p> <p>Manual alternative -- if you need more control over each step, you can still do it in three commands:</p> <pre><code># Trigger and capture the run ID\naf dags trigger 001_hello_world\n# Output:\n#   Triggered: 001_hello_world\n#   run_id: manual__2025-01-15T12:00:00+00:00\n#   state:  queued\n\n# Wait for completion\naf runs wait 001_hello_world \"manual__2025-01-15T12:00:00+00:00\"\n\n# Check task results\naf runs tasks 001_hello_world \"manual__2025-01-15T12:00:00+00:00\"\n</code></pre> <p>With curl in a script:</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nBASE=\"http://localhost:8081\"\n\n# Authenticate\nTOKEN=$(curl -s -X POST \"$BASE/auth/token\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"admin\"}' \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['access_token'])\")\n\nAUTH=\"Authorization: Bearer $TOKEN\"\n\n# Trigger\nRESPONSE=$(curl -s -X POST \"$BASE/api/v2/dags/001_hello_world/dagRuns\" \\\n  -H \"$AUTH\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"logical_date\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\"}\")\n\nRUN_ID=$(echo \"$RESPONSE\" | python3 -c \"import sys,json; print(json.load(sys.stdin)['dag_run_id'])\")\necho \"Triggered run: $RUN_ID\"\n\n# Poll\nwhile true; do\n  STATE=$(curl -s \"$BASE/api/v2/dags/001_hello_world/dagRuns/$RUN_ID\" \\\n    -H \"$AUTH\" \\\n    | python3 -c \"import sys,json; print(json.load(sys.stdin)['state'])\")\n  echo \"state=$STATE\"\n  case $STATE in\n    success) echo \"Done.\"; exit 0 ;;\n    failed|upstream_failed) echo \"Failed.\"; exit 1 ;;\n  esac\n  sleep 5\ndone\n</code></pre>"},{"location":"cli/#cicd-trigger-from-github-actions","title":"CI/CD: Trigger from GitHub Actions","text":"<pre><code># .github/workflows/trigger-dag.yml\nname: Trigger Airflow DAG\n\non:\n  push:\n    branches: [main]\n\njobs:\n  trigger:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Get Airflow token\n        id: auth\n        run: |\n          TOKEN=$(curl -s -X POST ${{ secrets.AIRFLOW_URL }}/auth/token \\\n            -H \"Content-Type: application/json\" \\\n            -d '{\"username\": \"${{ secrets.AIRFLOW_USER }}\", \"password\": \"${{ secrets.AIRFLOW_PASS }}\"}' \\\n            | python3 -c \"import sys,json; print(json.load(sys.stdin)['access_token'])\")\n          echo \"token=$TOKEN\" &gt;&gt; \"$GITHUB_OUTPUT\"\n\n      - name: Trigger DAG\n        run: |\n          curl -s -X POST ${{ secrets.AIRFLOW_URL }}/api/v2/dags/my_deploy_dag/dagRuns \\\n            -H \"Authorization: Bearer ${{ steps.auth.outputs.token }}\" \\\n            -H \"Content-Type: application/json\" \\\n            -d \"{\\\"logical_date\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\", \\\"conf\\\": {\\\"commit\\\": \\\"${{ github.sha }}\\\"}}\"\n</code></pre>"},{"location":"cli/#bulk-unpause-all-dags","title":"Bulk Unpause All DAGs","text":"<p>With <code>af</code> and a bit of shell:</p> <pre><code>af dags list 2&gt;/dev/null | tail -n +3 | awk '{print $1}' | while read dag_id; do\n  af dags unpause \"$dag_id\"\ndone\n</code></pre> <p>With curl:</p> <pre><code>TOKEN=$(curl -s -X POST http://localhost:8081/auth/token \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"username\": \"admin\", \"password\": \"admin\"}' \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['access_token'])\")\n\ncurl -s http://localhost:8081/api/v2/dags?paused=true&amp;limit=500 \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  | python3 -c \"\nimport sys, json\nfor dag in json.load(sys.stdin)['dags']:\n    print(dag['dag_id'])\n\" | while read dag_id; do\n  curl -s -X PATCH \"http://localhost:8081/api/v2/dags/$dag_id\" \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"is_paused\": false}'\n  echo \"Unpaused: $dag_id\"\ndone\n</code></pre>"},{"location":"dag-basics/","title":"DAG Basics","text":""},{"location":"dag-basics/#your-first-dag","title":"Your First DAG","text":"<p>A DAG file is just a Python file that Airflow reads. It needs three things:</p> <ol> <li>Imports -- bring in Airflow's tools</li> <li>A DAG definition -- give it a name and configure when it runs</li> <li>At least one task -- the actual work</li> </ol> <p>Here is the simplest possible DAG. It prints \"Hello, Airflow!\" to the logs:</p> <pre><code>from datetime import datetime\nfrom airflow.sdk import DAG\nfrom airflow.providers.standard.operators.bash import BashOperator\n\nwith DAG(\n    dag_id=\"my_first_dag\",       # Unique name, shows up in the web UI\n    start_date=datetime(2024, 1, 1),\n    schedule=None,               # Only runs when you click \"Trigger\" (no automatic schedule)\n    catchup=False,               # Do not try to run for past dates\n    tags=[\"example\"],            # Labels for filtering in the UI\n) as dag:\n    task = BashOperator(\n        task_id=\"hello\",                          # Unique name within this DAG\n        bash_command='echo \"Hello, Airflow!\"',    # The shell command to run\n    )\n</code></pre> <p>Most examples in this project use the <code>@task</code> decorator instead of <code>BashOperator</code> -- it lets you write plain Python functions and skip the operator boilerplate entirely. See DAG 04 for the first example of that pattern.</p>"},{"location":"dag-basics/#dag-parameters-explained","title":"DAG Parameters Explained","text":"Parameter Description Common Values <code>dag_id</code> Unique identifier for the DAG <code>\"my_pipeline\"</code> <code>start_date</code> Earliest date the DAG can be scheduled <code>datetime(2024, 1, 1)</code> <code>schedule</code> How often the DAG runs <code>None</code>, <code>\"@daily\"</code>, <code>\"@hourly\"</code>, <code>\"0 6 * * *\"</code> <code>catchup</code> Whether to backfill missed runs <code>True</code> / <code>False</code> <code>default_args</code> Default arguments applied to all tasks <code>{\"retries\": 1, \"retry_delay\": timedelta(minutes=5)}</code> <code>tags</code> Labels for filtering in the UI <code>[\"etl\", \"production\"]</code> <code>params</code> Per-run parameters with defaults <code>{\"env\": \"staging\", \"batch_size\": 100}</code> <code>max_active_runs</code> Max concurrent DAG runs <code>1</code>, <code>3</code>, <code>16</code> <code>description</code> Human-readable description shown in UI <code>\"Daily ETL pipeline\"</code>"},{"location":"dag-basics/#task-dependencies-who-runs-before-whom","title":"Task Dependencies (Who Runs Before Whom)","text":"<p>The <code>&gt;&gt;</code> operator tells Airflow the order. Think of it as \"then\":</p> <pre><code># \"Run A, then B, then C\"\na &gt;&gt; b &gt;&gt; c\n\n# \"Run A, then run B, C, and D in parallel\"\na &gt;&gt; [b, c, d]\n\n# \"Wait for A, B, and C to all finish, then run D\"\n[a, b, c] &gt;&gt; d\n\n# Same thing using chain() if you find it more readable\nfrom airflow.sdk.bases.chain import chain\nchain(a, b, c, d)\n</code></pre> <p>If you do not set any dependencies, all tasks run at the same time (in parallel).</p>"},{"location":"dag-basics/#when-does-a-dag-run","title":"When Does a DAG Run?","text":"<p>Every DAG in this project uses <code>schedule=None</code> -- meaning they only run when you click \"Trigger\" in the web UI (or when <code>make run</code> triggers them for you). In production, you would set a schedule so Airflow runs your workflow automatically.</p>"},{"location":"dag-basics/#manual-trigger-schedulenone","title":"Manual Trigger (schedule=None)","text":"<p>All 107 examples use this mode. The DAG sits idle until you trigger it:</p> <pre><code>with DAG(\n    dag_id=\"manual_dag\",\n    schedule=None,          # Never auto-scheduled\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n) as dag:\n    ...\n</code></pre> <p>When to use:</p> <ul> <li>Development and testing</li> <li>Event-driven workflows (triggered by external systems via API)</li> <li>One-off data migrations</li> <li>Ad-hoc analysis pipelines</li> </ul> <p>How to trigger:</p> <ul> <li>Web UI: Click \"Trigger DAG\" button</li> <li>CLI: <code>airflow dags trigger manual_dag</code></li> <li>API: <code>POST /api/v1/dags/manual_dag/dagRuns</code></li> <li>Another DAG: <code>TriggerDagRunOperator</code></li> </ul>"},{"location":"dag-basics/#scheduled-runs","title":"Scheduled Runs","text":"<p>Airflow supports cron expressions, presets, and timedelta schedules:</p> <pre><code># Cron expression: every day at 6 AM UTC\nwith DAG(dag_id=\"daily_6am\", schedule=\"0 6 * * *\", ...):\n    ...\n\n# Preset: once per day at midnight\nwith DAG(dag_id=\"daily_midnight\", schedule=\"@daily\", ...):\n    ...\n\n# Preset: once per hour\nwith DAG(dag_id=\"hourly\", schedule=\"@hourly\", ...):\n    ...\n\n# Timedelta: every 30 minutes\nfrom datetime import timedelta\nwith DAG(dag_id=\"every_30min\", schedule=timedelta(minutes=30), ...):\n    ...\n</code></pre> <p>Available presets:</p> Preset Cron Equivalent Description <code>@once</code> -- Run exactly once <code>@hourly</code> <code>0 * * * *</code> Every hour <code>@daily</code> <code>0 0 * * *</code> Every day at midnight <code>@weekly</code> <code>0 0 * * 0</code> Every Sunday at midnight <code>@monthly</code> <code>0 0 1 * *</code> First of every month <code>@yearly</code> <code>0 0 1 1 *</code> January 1st"},{"location":"dag-basics/#catchup-and-backfill","title":"Catchup and Backfill","text":"<p>When <code>catchup=True</code> (default), Airflow creates DAG runs for every missed interval between <code>start_date</code> and now:</p> <pre><code># This would create ~365 DAG runs if deployed today with catchup=True\nwith DAG(\n    dag_id=\"backfill_demo\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n    catchup=True,           # Will backfill all missed dates\n) as dag:\n    ...\n</code></pre> <p>Best practices:</p> <ul> <li>Set <code>catchup=False</code> for most DAGs (especially during development)</li> <li>Use <code>catchup=True</code> intentionally when you need historical data processing</li> <li>Use <code>max_active_runs=1</code> to prevent overwhelming your infrastructure during backfills</li> <li>Use <code>airflow dags backfill</code> CLI for controlled historical reruns</li> </ul>"},{"location":"dag-basics/#data-aware-scheduling-with-assets","title":"Data-Aware Scheduling with Assets","text":"<p>Airflow 3.x introduced <code>Asset</code> (formerly <code>Dataset</code>) for event-driven scheduling based on data dependencies:</p> <pre><code>from airflow.sdk import Asset\n\nweather_data = Asset(\"s3://bucket/weather.parquet\")\n\n# Producer: declares it creates the asset\nwith DAG(dag_id=\"producer\", schedule=\"@daily\", ...):\n    produce = PythonOperator(\n        task_id=\"produce\",\n        python_callable=create_weather_data,\n        outlets=[weather_data],       # This DAG produces this asset\n    )\n\n# Consumer: triggered automatically when asset is updated\nwith DAG(dag_id=\"consumer\", schedule=[weather_data], ...):\n    consume = PythonOperator(\n        task_id=\"consume\",\n        python_callable=process_weather_data,\n    )\n</code></pre>"},{"location":"dag-reference/","title":"DAG Examples Reference","text":"<p>Each section below describes one example DAG. They are numbered in learning order -- start at 01 and work your way through. DAGs 01-10 cover the fundamentals every Airflow user needs. The later sections (Docker, SQL, APIs) are independent and can be explored in any order based on what interests you.</p>"},{"location":"dag-reference/#01-hello-world","title":"01 -- Hello World","text":"<p>The simplest possible DAG. Two <code>BashOperator</code> tasks connected with <code>&gt;&gt;</code>:</p> <pre><code>hello &gt;&gt; date\n</code></pre> <p>See: <code>dags/01_hello_world.py</code></p>"},{"location":"dag-reference/#02-pythonoperator","title":"02 -- PythonOperator","text":"<p>Run Python callables as tasks with <code>op_args</code> and <code>op_kwargs</code>:</p> <pre><code>PythonOperator(\n    task_id=\"greet\",\n    python_callable=greet,\n    op_args=[\"Airflow\"],\n    op_kwargs={\"greeting\": \"Welcome\"},\n)\n</code></pre> <p>See: <code>dags/02_python_operator.py</code></p>"},{"location":"dag-reference/#03-task-dependencies","title":"03 -- Task Dependencies","text":"<p>All dependency patterns: <code>&gt;&gt;</code>, <code>&lt;&lt;</code>, <code>chain()</code>, <code>cross_downstream()</code>, fan-in, fan-out:</p> <pre><code>start &gt;&gt; middle &gt;&gt; end\nfan_out_source &gt;&gt; [fan_out_a, fan_out_b, fan_out_c]\n[fan_out_a, fan_out_b, fan_out_c] &gt;&gt; fan_in_sink\nchain(chain_a, chain_b, chain_c)\ncross_downstream([cross_a, cross_b], [cross_x, cross_y])\n</code></pre> <p>See: <code>dags/03_task_dependencies.py</code></p>"},{"location":"dag-reference/#04-taskflow-api","title":"04 -- TaskFlow API","text":"<p>The <code>@task</code> decorator turns Python functions into Airflow tasks with automatic XCom passing:</p> <pre><code>@task\ndef extract() -&gt; dict:\n    return {\"values\": [1, 2, 3]}\n\n@task\ndef transform(data: dict) -&gt; dict:\n    return {\"values\": [v * 2 for v in data[\"values\"]]}\n\nraw = extract()\nprocessed = transform(raw)  # Automatic XCom!\n</code></pre> <p>See: <code>dags/04_taskflow_api.py</code></p>"},{"location":"dag-reference/#05-xcoms","title":"05 -- XComs","text":"<p>Manual cross-task communication with <code>ti.xcom_push()</code> and <code>ti.xcom_pull()</code>:</p> <pre><code>ti.xcom_push(key=\"greeting\", value=\"Hello!\")\nvalue = ti.xcom_pull(task_ids=\"push_task\", key=\"greeting\")\n</code></pre> <p>See: <code>dags/05_xcoms.py</code></p>"},{"location":"dag-reference/#06-branching","title":"06 -- Branching","text":"<p>Conditional execution paths with <code>@task.branch</code>:</p> <pre><code>@task.branch\ndef choose_branch():\n    if condition:\n        return \"path_a\"\n    return \"path_b\"\n</code></pre> <p>See: <code>dags/06_branching.py</code></p>"},{"location":"dag-reference/#07-trigger-rules","title":"07 -- Trigger Rules","text":"<p>Control when tasks run based on upstream task states:</p> Rule Runs When <code>all_success</code> All upstream tasks succeeded (default) <code>one_success</code> At least one upstream task succeeded <code>all_done</code> All upstream tasks completed (any state) <code>none_failed</code> No upstream task failed (skipped is ok) <code>all_skipped</code> All upstream tasks were skipped <p>See: <code>dags/07_trigger_rules.py</code></p>"},{"location":"dag-reference/#08-jinja-templating","title":"08 -- Jinja Templating","text":"<p>Airflow renders Jinja templates in operator parameters at execution time:</p> <pre><code>BashOperator(\n    task_id=\"templated\",\n    bash_command='echo \"Date: {{ ds }}, Param: {{ params.env }}\"',\n)\n</code></pre> <p>Commonly used template variables:</p> Variable Description <code>{{ ds }}</code> Logical date as <code>YYYY-MM-DD</code> <code>{{ logical_date }}</code> Full datetime object <code>{{ params.key }}</code> DAG parameter value <code>{{ macros.ds_add(ds, 7) }}</code> Date arithmetic <code>{{ dag.dag_id }}</code> Current DAG ID <code>{{ task.task_id }}</code> Current task ID <code>{{ run_id }}</code> Current DAG run ID <p>See: <code>dags/08_templating.py</code></p>"},{"location":"dag-reference/#09-task-groups","title":"09 -- Task Groups","text":"<p>Organize complex DAGs into collapsible groups in the UI:</p> <pre><code>with TaskGroup(\"extract\") as extract_group:\n    extract_users = BashOperator(task_id=\"users\", ...)\n    extract_orders = BashOperator(task_id=\"orders\", ...)\n    # Task IDs become \"extract.users\", \"extract.orders\"\n</code></pre> <p>Supports nesting: <code>transform.clean.users</code>, <code>transform.enrich.join</code>.</p> <p>See: <code>dags/09_task_groups.py</code></p>"},{"location":"dag-reference/#10-dynamic-task-mapping","title":"10 -- Dynamic Task Mapping","text":"<p>Create tasks dynamically at runtime based on data:</p> <pre><code>@task\ndef generate_files() -&gt; list[str]:\n    return [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n\n@task\ndef process_file(filename: str) -&gt; dict:\n    return {\"file\": filename, \"rows\": 100}\n\nfiles = generate_files()\nresults = process_file.expand(filename=files)  # Creates 3 mapped tasks\n</code></pre> <p>See: <code>dags/10_dynamic_tasks.py</code></p>"},{"location":"dag-reference/#11-sensors","title":"11 -- Sensors","text":"<p>Wait for external conditions before proceeding:</p> <ul> <li>TimeDeltaSensor: Wait for a time offset from the logical date</li> <li>FileSensor: Wait for a file to appear on disk</li> <li>poke mode: Holds worker slot, checks periodically</li> <li>reschedule mode: Releases worker between checks</li> </ul> <p>See: <code>dags/11_sensors.py</code></p>"},{"location":"dag-reference/#12-retries-and-callbacks","title":"12 -- Retries and Callbacks","text":"<p>Configure automatic retries and hook into task lifecycle events:</p> <pre><code>PythonOperator(\n    task_id=\"flaky_task\",\n    python_callable=my_func,\n    retries=3,\n    retry_delay=timedelta(minutes=5),\n    on_success_callback=notify_success,\n    on_failure_callback=notify_failure,\n    on_retry_callback=log_retry,\n)\n</code></pre> <p>See: <code>dags/12_retries_and_callbacks.py</code></p>"},{"location":"dag-reference/#13-custom-operators","title":"13 -- Custom Operators","text":"<p>Build reusable operators by subclassing <code>BaseOperator</code>:</p> <pre><code>from airflow.sdk import BaseOperator\n\nclass SquareOperator(BaseOperator):\n    def __init__(self, number: int, **kwargs):\n        super().__init__(**kwargs)\n        self.number = number\n\n    def execute(self, context):\n        result = self.number ** 2\n        return result  # Automatically pushed to XCom\n</code></pre> <p>See: <code>dags/13_custom_operators.py</code></p>"},{"location":"dag-reference/#14-assets","title":"14 -- Assets","text":"<p>Data-aware scheduling with <code>Asset</code> (Airflow 3.x):</p> <pre><code>weather = Asset(\"s3://bucket/weather.parquet\")\n\n# Producer declares outlets\nproduce = PythonOperator(outlets=[weather], ...)\n\n# Consumer scheduled by asset update\nwith DAG(schedule=[weather], ...):\n    consume = PythonOperator(...)\n</code></pre> <p>See: <code>dags/14_assets.py</code></p>"},{"location":"dag-reference/#15-dag-dependencies","title":"15 -- DAG Dependencies","text":"<p>Trigger other DAGs with <code>TriggerDagRunOperator</code>:</p> <pre><code>TriggerDagRunOperator(\n    task_id=\"trigger\",\n    trigger_dag_id=\"downstream_dag\",\n    wait_for_completion=True,\n)\n</code></pre> <p>See: <code>dags/15_dag_dependencies.py</code></p>"},{"location":"dag-reference/#16-pools-and-priority","title":"16 -- Pools and Priority","text":"<p>Resource management for concurrent task execution:</p> <pre><code>BashOperator(\n    task_id=\"db_query\",\n    pool=\"database_pool\",       # Limits concurrent DB connections\n    priority_weight=10,         # Higher = executed first in queue\n    ...)\n</code></pre> <p>See: <code>dags/16_pools_and_priority.py</code></p>"},{"location":"dag-reference/#17-variables-and-params","title":"17 -- Variables and Params","text":"<ul> <li>Variables: Global key-value store persisted in the metadata DB</li> <li>Params: Per-run configuration with type validation and defaults</li> </ul> <pre><code># Variables (global, persistent)\nfrom airflow.models import Variable\nVariable.set(\"api_url\", \"https://api.example.com\")\nurl = Variable.get(\"api_url\")\n\n# Params (per-run, with defaults)\nwith DAG(params={\"env\": \"staging\", \"batch_size\": 100}, ...):\n    ...\n</code></pre> <p>See: <code>dags/17_variables_and_params.py</code></p>"},{"location":"dag-reference/#18-short-circuit","title":"18 -- Short Circuit","text":"<p>Skip downstream tasks conditionally:</p> <pre><code>check = ShortCircuitOperator(\n    task_id=\"check\",\n    python_callable=lambda: file_exists(\"/data/input.csv\"),\n)\ncheck &gt;&gt; expensive_task  # Skipped if check returns False\n</code></pre> <p>See: <code>dags/18_short_circuit.py</code></p>"},{"location":"dag-reference/#19-setup-and-teardown","title":"19 -- Setup and Teardown","text":"<p>Resource lifecycle management ensuring cleanup always runs:</p> <pre><code>resource = create_resource()    # Setup\nwork = use_resource(resource)   # Work\ncleanup = cleanup_resource()    # Teardown (runs even if work fails)\n\nresource &gt;&gt; work &gt;&gt; cleanup\n</code></pre> <p>See: <code>dags/19_setup_teardown.py</code></p>"},{"location":"dag-reference/#20-complex-pipeline","title":"20 -- Complex Pipeline","text":"<p>Multi-stage ETL combining TaskGroups, dynamic mapping, and callbacks:</p> <pre><code>start -&gt; [extract_group] -&gt; [transform_group] -&gt; [load_group] -&gt; [notify_group] -&gt; end\n</code></pre> <p>See: <code>dags/20_complex_pipeline.py</code></p>"},{"location":"dag-reference/#21-docker-hello-world","title":"21 -- Docker Hello World","text":"<p>The simplest DockerOperator DAG. Runs commands inside Alpine containers:</p> <pre><code>from airflow.providers.docker.operators.docker import DockerOperator\n\nhello = DockerOperator(\n    task_id=\"hello\",\n    image=\"alpine:3.20\",\n    command='echo \"Hello from inside a Docker container!\"',\n    auto_remove=\"success\",\n    docker_url=\"unix://var/run/docker.sock\",\n    network_mode=\"bridge\",\n)\n</code></pre> <p>Key parameters every DockerOperator needs:</p> Parameter Description <code>image</code> Docker image to pull and run <code>command</code> Command to execute inside the container <code>auto_remove</code> <code>\"success\"</code> removes container after success, <code>\"force\"</code> always removes <code>docker_url</code> Docker daemon socket path <code>network_mode</code> <code>\"bridge\"</code> (isolated) or <code>\"host\"</code> (shares host network) <p>See: <code>dags/21_docker_hello.py</code></p>"},{"location":"dag-reference/#22-docker-python-scripts","title":"22 -- Docker Python Scripts","text":"<p>Run Python code inside containers. Demonstrates inline scripts, data processing, and installing packages at runtime:</p> <pre><code># Run inline Python\nDockerOperator(\n    task_id=\"inline_script\",\n    image=\"python:3.12-slim\",\n    command='python -c \"import sys; print(f\\'Python {sys.version}\\')\"',\n    ...\n)\n\n# Install packages and use them\nDockerOperator(\n    task_id=\"with_package\",\n    image=\"python:3.12-slim\",\n    command='bash -c \"pip install humanize &amp;&amp; python -c \\'import humanize; print(humanize.naturalsize(1e9))\\'\"',\n    ...\n)\n</code></pre> <p>See: <code>dags/22_docker_python_script.py</code></p>"},{"location":"dag-reference/#23-docker-volumes","title":"23 -- Docker Volumes","text":"<p>Bind mounts let containers share data through the host filesystem. This is the primary pattern for multi-step Docker pipelines:</p> <pre><code>from docker.types import Mount\n\nSHARED_MOUNT = Mount(source=\"/tmp/airflow-demo\", target=\"/data\", type=\"bind\")\n\n# Step 1: Write data\ncreate_data = DockerOperator(\n    task_id=\"create_data\",\n    image=\"alpine:3.20\",\n    command='sh -c \"echo \\'Alice,85\\' &gt; /data/scores.csv\"',\n    mounts=[SHARED_MOUNT],\n    ...\n)\n\n# Step 2: Process data (reads from same mount)\nprocess_data = DockerOperator(\n    task_id=\"process_data\",\n    image=\"python:3.12-slim\",\n    command='python -c \"import csv; rows = list(csv.reader(open(\\'/data/scores.csv\\')))\"',\n    mounts=[SHARED_MOUNT],\n    ...\n)\n</code></pre> <p>Mount types:</p> Type Description Use Case <code>bind</code> Maps a host directory into the container Sharing data between steps <code>volume</code> Docker-managed named volume Persistent data across container restarts <code>tmpfs</code> In-memory filesystem Scratch space, sensitive data <p>See: <code>dags/23_docker_volumes.py</code></p>"},{"location":"dag-reference/#24-docker-environment-variables","title":"24 -- Docker Environment Variables","text":"<p>Pass configuration to containers via environment variables. Supports both static values and Airflow template-rendered values:</p> <pre><code># Static env vars\nDockerOperator(\n    task_id=\"static_env\",\n    image=\"alpine:3.20\",\n    command='sh -c \"echo APP_ENV=$APP_ENV\"',\n    environment={\n        \"APP_ENV\": \"staging\",\n        \"LOG_LEVEL\": \"INFO\",\n    },\n    ...\n)\n\n# Templated env vars (Airflow injects values at runtime)\nDockerOperator(\n    task_id=\"templated_env\",\n    image=\"alpine:3.20\",\n    command='sh -c \"echo EXECUTION_DATE=$EXECUTION_DATE\"',\n    environment={\n        \"EXECUTION_DATE\": \"{{ ds }}\",\n        \"DAG_ID\": \"{{ dag.dag_id }}\",\n    },\n    ...\n)\n</code></pre> <p>See: <code>dags/24_docker_env_and_secrets.py</code></p>"},{"location":"dag-reference/#25-docker-etl-pipeline","title":"25 -- Docker ETL Pipeline","text":"<p>A complete 5-stage ETL pipeline where every stage runs in its own container. Demonstrates a realistic pattern: extract raw data, validate, transform, report, and cleanup -- all connected through a shared volume:</p> <pre><code>extract (python:3.12) -&gt; validate (python:3.12) -&gt; transform (python:3.12) -&gt; report (python:3.12) -&gt; cleanup (alpine)\n</code></pre> <p>Each stage reads input from and writes output to <code>/pipeline/</code> via a bind mount.</p> <p>See: <code>dags/25_docker_pipeline.py</code></p>"},{"location":"dag-reference/#26-docker-custom-images","title":"26 -- Docker Custom Images","text":"<p>Run tasks in different language runtimes. Demonstrates BusyBox, Node.js, Ruby, and custom working directory configuration:</p> <pre><code># Node.js task\nDockerOperator(\n    task_id=\"nodejs_task\",\n    image=\"node:22-alpine\",\n    command='node -e \"console.log(\\'Hello from Node.js\\')\"',\n    ...\n)\n\n# Ruby task\nDockerOperator(\n    task_id=\"ruby_task\",\n    image=\"ruby:3.3-alpine\",\n    command='ruby -e \"puts \\'Hello from Ruby\\'\"',\n    ...\n)\n\n# Custom working directory\nDockerOperator(\n    task_id=\"workdir_task\",\n    image=\"alpine:3.20\",\n    command='sh -c \"pwd &amp;&amp; ls -la\"',\n    working_dir=\"/opt\",\n    ...\n)\n</code></pre> <p>See: <code>dags/26_docker_custom_image.py</code></p>"},{"location":"dag-reference/#27-docker-networking","title":"27 -- Docker Networking","text":"<p>Explore container networking: bridge mode isolation, host connectivity checks, and DNS resolution inside containers:</p> <pre><code># Bridge mode: container gets its own network namespace\nDockerOperator(\n    task_id=\"bridge_mode\",\n    image=\"alpine:3.20\",\n    command='sh -c \"hostname -i &amp;&amp; cat /etc/resolv.conf\"',\n    network_mode=\"bridge\",\n    ...\n)\n</code></pre> <p>Network modes:</p> Mode Description <code>bridge</code> Default. Container gets its own IP, isolated from host <code>host</code> Container shares the host's network stack <code>none</code> No networking <code>container:&lt;id&gt;</code> Share another container's network namespace <p>See: <code>dags/27_docker_network.py</code></p>"},{"location":"dag-reference/#28-docker-resource-constraints","title":"28 -- Docker Resource Constraints","text":"<p>Set CPU and memory limits to prevent containers from consuming all host resources:</p> <pre><code># Memory-limited task\nDockerOperator(\n    task_id=\"memory_limited\",\n    image=\"python:3.12-slim\",\n    command='python -c \"data = list(range(100000))\"',\n    mem_limit=\"128m\",\n    ...\n)\n\n# CPU-limited task\nDockerOperator(\n    task_id=\"cpu_limited\",\n    image=\"python:3.12-slim\",\n    command='python -c \"sum(i**2 for i in range(500000))\"',\n    cpus=0.5,\n    ...\n)\n</code></pre> <p>Resource parameters:</p> Parameter Description Example <code>mem_limit</code> Maximum memory <code>\"128m\"</code>, <code>\"2g\"</code> <code>cpus</code> CPU core limit (fractional) <code>0.5</code>, <code>2.0</code> <code>mem_reservation</code> Soft memory limit <code>\"64m\"</code> <p>See: <code>dags/28_docker_resources.py</code></p>"},{"location":"dag-reference/#29-docker-xcom","title":"29 -- Docker XCom","text":"<p>Pass data between Docker containers using XCom. The container's stdout is captured and pushed to XCom when <code>retrieve_output=True</code>:</p> <pre><code># Docker task pushes last stdout line to XCom\ngenerate = DockerOperator(\n    task_id=\"generate_data\",\n    image=\"python:3.12-slim\",\n    command='python -c \"import json; print(json.dumps({\\'values\\': [10, 20, 30]}))\"',\n    retrieve_output=True,\n    retrieve_output_path=\"/tmp/xcom_output\",\n    ...\n)\n\n# Python task reads the XCom value\ndef show_output(**context):\n    result = context[\"ti\"].xcom_pull(task_ids=\"generate_data\")\n    print(f\"Docker output: {result}\")\n</code></pre> <p>See: <code>dags/29_docker_xcom.py</code></p>"},{"location":"dag-reference/#30-docker-mixed-pipeline","title":"30 -- Docker Mixed Pipeline","text":"<p>Real-world DAGs often mix execution modes. Some tasks need container isolation (specific dependencies, language runtimes), while others run fine natively:</p> <pre><code>DockerOperator (extract) -&gt; PythonOperator (validate) -&gt; BashOperator (check) -&gt; DockerOperator (transform) -&gt; PythonOperator (aggregate)\n</code></pre> <p>When to use each:</p> Operator Best For <code>DockerOperator</code> Tasks needing specific deps, language isolation, reproducibility <code>PythonOperator</code> Lightweight Python logic, validation, aggregation <code>BashOperator</code> Quick system checks, simple shell commands <p>See: <code>dags/30_docker_mixed_pipeline.py</code></p>"},{"location":"dag-reference/#31-docker-dynamic-images","title":"31 -- Docker Dynamic Images","text":"<p>Dynamic task mapping with DockerOperator. Run the same command across different container images in parallel:</p> <pre><code>@task\ndef list_images() -&gt; list[str]:\n    return [\"python:3.12-slim\", \"python:3.11-slim\", \"node:22-alpine\", \"ruby:3.3-alpine\"]\n\n# .partial() sets shared params, .expand() maps over the variable one\nversion_check = DockerOperator.partial(\n    task_id=\"version_check\",\n    command='sh -c \"uname -a\"',\n    auto_remove=\"success\",\n    ...\n).expand(image=list_images())\n</code></pre> <p>This creates 4 mapped task instances at runtime, one per image.</p> <p>See: <code>dags/31_docker_dynamic_images.py</code></p>"},{"location":"dag-reference/#32-docker-compose-from-airflow","title":"32 -- Docker Compose from Airflow","text":"<p>Use BashOperator to run <code>docker compose</code> and <code>docker run</code> commands. Useful for orchestrating multi-container setups or running integration tests:</p> <pre><code># Run docker compose for integration tests\nBashOperator(\n    task_id=\"integration_test\",\n    bash_command=\"docker compose -f test.yml up --abort-on-container-exit\",\n)\n\n# Run a one-off container\nBashOperator(\n    task_id=\"docker_run\",\n    bash_command='docker run --rm alpine:3.20 echo \"Hello\"',\n)\n</code></pre> <p>See: <code>dags/32_docker_compose_task.py</code></p>"},{"location":"data-pipelines/","title":"Data Pipelines","text":""},{"location":"data-pipelines/#data-pipelines-parquet-aggregation","title":"Data Pipelines: Parquet Aggregation","text":"<p>A realistic data pipeline that reads Parquet, performs aggregations with pandas, and writes CSV:</p> <pre><code>@task\ndef generate_parquet(data_dir: str) -&gt; str:\n    import pandas as pd\n    import numpy as np\n\n    df = pd.DataFrame({\n        \"station\": np.random.choice([\"oslo_01\", \"bergen_01\"], 200),\n        \"temperature_c\": np.round(np.random.normal(10, 8, 200), 1),\n        \"humidity_pct\": np.round(np.random.uniform(40, 95, 200), 1),\n    })\n    path = f\"{data_dir}/weather.parquet\"\n    df.to_parquet(path, index=False)\n    return path\n\n@task\ndef aggregate_by_station(parquet_path: str) -&gt; str:\n    import pandas as pd\n\n    df = pd.read_parquet(parquet_path)\n    agg = df.groupby(\"station\").agg(\n        temp_mean=(\"temperature_c\", \"mean\"),\n        temp_min=(\"temperature_c\", \"min\"),\n        temp_max=(\"temperature_c\", \"max\"),\n    ).round(2)\n\n    csv_path = parquet_path.replace(\".parquet\", \"_summary.csv\")\n    agg.to_csv(csv_path)\n    return csv_path\n</code></pre> <p>The pipeline runs three parallel aggregations (by station, by date, cross-tabulation) from the same Parquet source, then produces a final report.</p> <p>See: <code>dags/57_parquet_aggregation.py</code></p>"},{"location":"data-pipelines/#dhis2-metadata-pipelines","title":"DHIS2 Metadata Pipelines","text":"<p>DAGs 58--62 and 110--111 are a domain-specific detour into health data. You can safely skip them if you are not working with health information systems -- the general patterns (API calls, JSON flattening, pandas transforms) are covered again in DAGs 81-100 with more general APIs. DAGs 110--111 also demonstrate using Airflow Connections for credential management, which applies to any external API.</p> <p>DHIS2 (District Health Information Software 2) is the world's largest health information management system, used in 100+ countries. These examples fetch data from a public test server and demonstrate patterns you would use against any REST API: nested JSON flattening, derived columns, multi-format output, and parallel pipeline orchestration.</p>"},{"location":"data-pipelines/#what-is-dhis2","title":"What is DHIS2?","text":"<p>DHIS2 organizes health data around three core metadata types:</p> Metadata Type Description Play Server Records Organisation Units Health facilities, districts, regions -- arranged in a hierarchy (country &gt; province &gt; district &gt; facility) ~1575 Data Elements Individual data points collected at facilities (e.g., \"Malaria cases\", \"BCG doses given\") ~1037 Indicators Calculated metrics derived from data elements (e.g., \"BCG coverage %\" = doses / target population) ~77 <p>Organisation units can also carry geometry (Point for facilities, Polygon/MultiPolygon for administrative boundaries), making them a source of spatial data.</p>"},{"location":"data-pipelines/#dhis2-rest-api","title":"DHIS2 REST API","text":"<p>The DHIS2 Web API follows a consistent pattern across all metadata endpoints:</p> <pre><code>GET /api/{resource}?paging=false&amp;fields={fieldSpec}\n</code></pre> <p>Key API parameters:</p> Parameter Description Example <code>paging</code> <code>false</code> disables pagination, returns all records in one response <code>paging=false</code> <code>fields</code> Controls which fields are returned. <code>:owner</code> returns all fields owned by the object (excludes computed fields) <code>fields=:owner</code> <code>filter</code> Server-side filtering (not used in these examples -- we filter client-side with pandas) <code>filter=level:eq:4</code> <p>Authentication: HTTP Basic Auth. The play server uses <code>admin</code>/<code>district</code>:</p> <pre><code>import requests\n\nresp = requests.get(\n    \"https://play.im.dhis2.org/dev/api/organisationUnits\",\n    auth=(\"admin\", \"district\"),\n    params={\"paging\": \"false\", \"fields\": \":owner\"},\n    timeout=60,\n)\n</code></pre> <p>Response structure: The JSON response wraps the record list under a key matching the endpoint name:</p> <pre><code>{\n  \"organisationUnits\": [\n    {\"id\": \"abc123\", \"name\": \"Sierra Leone\", \"level\": 1, ...},\n    {\"id\": \"def456\", \"name\": \"Bo\", \"level\": 2, \"parent\": {\"id\": \"abc123\"}, ...}\n  ]\n}\n</code></pre> <p>This means the extraction step is always: <code>data = resp.json()</code> then <code>records = data[\"organisationUnits\"]</code>.</p>"},{"location":"data-pipelines/#shared-helper-module","title":"Shared Helper Module","text":"<p>All DHIS2 DAGs share <code>src/airflow_examples/dhis2.py</code> to avoid duplicating API logic. Credentials are read from the <code>dhis2_default</code> Airflow connection at runtime instead of being hardcoded:</p> <pre><code>\"\"\"DHIS2 API helpers for metadata pipeline examples.\"\"\"\n\nfrom typing import Any\n\nimport httpx\nfrom airflow.exceptions import AirflowFailException\nfrom airflow.sdk.bases.hook import BaseHook\n\nfrom airflow_examples.config import OUTPUT_BASE\n\nOUTPUT_DIR = str(OUTPUT_BASE / \"dhis2_exports\")\n\n\ndef _get_dhis2_config() -&gt; tuple[str, tuple[str, str]]:\n    \"\"\"Read DHIS2 base URL and credentials from the Airflow connection.\"\"\"\n    try:\n        conn = BaseHook.get_connection(\"dhis2_default\")\n    except Exception as exc:\n        raise AirflowFailException(\n            \"Connection 'dhis2_default' not found. \"\n            \"Add it via the Airflow UI or CLI before running DHIS2 DAGs.\"\n        ) from exc\n    base_url = (conn.host or \"\").rstrip(\"/\")\n    credentials = (conn.login or \"\", conn.password or \"\")\n    return base_url, credentials\n\n\ndef fetch_metadata(endpoint: str, fields: str = \":owner\") -&gt; list[dict[str, Any]]:\n    \"\"\"Fetch all records from a DHIS2 metadata endpoint.\"\"\"\n    base_url, credentials = _get_dhis2_config()\n    url = f\"{base_url}/api/{endpoint}\"\n    resp = httpx.get(\n        url,\n        auth=credentials,\n        params={\"paging\": \"false\", \"fields\": fields},\n        timeout=60,\n    )\n    resp.raise_for_status()\n    data: dict[str, Any] = resp.json()\n    key = endpoint.split(\"?\")[0]\n    result: list[dict[str, Any]] = data[key]\n    return result\n</code></pre> <p>Design decisions:</p> <ul> <li>Connection-managed credentials: The base URL, username, and password live in the   <code>dhis2_default</code> Airflow connection rather than as Python constants. This means credentials   are stored in one place (the Airflow metadata database or environment variables), never   committed to source control, and can be changed without redeploying DAG code.</li> <li><code>AirflowFailException</code>: If the connection is missing, the task raises   <code>AirflowFailException</code> instead of a generic error. This tells Airflow to mark the task as   <code>failed</code> immediately without retrying -- there is no point retrying when the connection   does not exist yet.</li> <li><code>paging=false</code>: The play server has small datasets (~1000s of records). For production DHIS2   instances with millions of records, you'd paginate with <code>page=1&amp;pageSize=1000</code> and loop.</li> <li><code>fields=:owner</code>: Returns all fields the object \"owns\" (not computed/derived fields). This is   the most common field spec for metadata exports. You can also request specific fields:   <code>fields=id,name,level,parent</code> (used in DAG 61 for geometry).</li> <li><code>timeout=60</code>: The play server can be slow. Production code should use retries (Airflow's   built-in <code>retries</code> parameter handles this at the task level).</li> <li>Type annotations: <code>list[dict[str, Any]]</code> satisfies mypy strict mode. The DHIS2 API returns   heterogeneous dicts, so <code>Any</code> values are appropriate.</li> </ul> <p>Why a shared module instead of inline code? The fetch logic is identical across all DHIS2 DAGs -- only the endpoint name changes. Centralizing it in a helper module means:</p> <ol> <li>Single place to update if the API URL or auth changes</li> <li>DAG files focus on transform logic, not HTTP boilerplate</li> <li>The helper is importable from both DAGs and tests</li> </ol> <p>See: <code>src/airflow_examples/dhis2.py</code></p>"},{"location":"data-pipelines/#json-flattening-patterns","title":"JSON Flattening Patterns","text":"<p>DHIS2 returns deeply nested JSON. Most fields are flat strings, but several contain nested objects or arrays that need flattening before tabular output:</p>"},{"location":"data-pipelines/#nested-object-references","title":"Nested Object References","text":"<p>Many DHIS2 objects reference their parent or related objects as nested dicts:</p> <pre><code>{\n  \"id\": \"abc123\",\n  \"name\": \"Bo District\",\n  \"parent\": {\"id\": \"xyz789\"},\n  \"categoryCombo\": {\"id\": \"combo1\"},\n  \"createdBy\": {\"username\": \"admin\", \"id\": \"user1\"},\n  \"indicatorType\": {\"id\": \"type1\", \"name\": \"Per cent\"}\n}\n</code></pre> <p>The flattening pattern extracts the nested value with a safe accessor:</p> <pre><code># Extract parent ID from nested dict (or None if missing)\ndf[\"parent_id\"] = df[\"parent\"].apply(\n    lambda p: p[\"id\"] if isinstance(p, dict) else None\n)\n\n# Extract nested field with .get() for optional keys\ndf[\"created_by\"] = df[\"createdBy\"].apply(\n    lambda c: c.get(\"username\") if isinstance(c, dict) else None\n)\n\n# Extract multiple fields from the same nested object\ndf[\"indicator_type_id\"] = df[\"indicatorType\"].apply(\n    lambda t: t[\"id\"] if isinstance(t, dict) else None\n)\ndf[\"indicator_type_name\"] = df[\"indicatorType\"].apply(\n    lambda t: t.get(\"name\") if isinstance(t, dict) else None\n)\n</code></pre> <p>Why <code>isinstance(p, dict)</code> guards? Some records may have <code>None</code> or missing parent fields. Without the guard, <code>None[\"id\"]</code> raises <code>TypeError</code>. The <code>isinstance</code> check handles both <code>None</code> and unexpected types gracefully.</p>"},{"location":"data-pipelines/#path-based-hierarchy-depth","title":"Path-Based Hierarchy Depth","text":"<p>Organisation units have a <code>path</code> field encoding their position in the hierarchy:</p> <pre><code>\"/abc123\"                     -&gt; depth 0 (country)\n\"/abc123/def456\"              -&gt; depth 1 (province)\n\"/abc123/def456/ghi789\"       -&gt; depth 2 (district)\n\"/abc123/def456/ghi789/jkl0\"  -&gt; depth 3 (facility)\n</code></pre> <p>Extracting depth is a simple string operation:</p> <pre><code>df[\"hierarchy_depth\"] = df[\"path\"].apply(\n    lambda p: p.count(\"/\") - 1 if isinstance(p, str) else 0\n)\n</code></pre> <p>The <code>-1</code> accounts for the leading <code>/</code> -- a path like <code>/abc123</code> has one slash but represents depth 0 (the root level).</p>"},{"location":"data-pipelines/#array-length-columns","title":"Array Length Columns","text":"<p>Some fields are arrays (translations, data element groups). Counting them creates useful summary columns:</p> <pre><code>df[\"translation_count\"] = df[\"translations\"].apply(\n    lambda t: len(t) if isinstance(t, list) else 0\n)\n</code></pre>"},{"location":"data-pipelines/#boolean-derived-columns","title":"Boolean Derived Columns","text":"<p>Create boolean flags from nullable fields for easy filtering and aggregation:</p> <pre><code># True if the record has a code assigned, False otherwise\ndf[\"has_code\"] = df[\"code\"].notna()\n</code></pre> <p>This is more useful than the raw <code>code</code> column for summary statistics: <code>df[\"has_code\"].sum()</code> gives you the count of coded elements instantly.</p>"},{"location":"data-pipelines/#output-formats-csv-vs-parquet-vs-geojson","title":"Output Formats: CSV vs Parquet vs GeoJSON","text":"<p>These DAGs demonstrate three output formats, each suited to different use cases:</p>"},{"location":"data-pipelines/#csv-dags-58-60","title":"CSV (DAGs 58, 60)","text":"<pre><code>df.to_csv(csv_path, index=False)\n</code></pre> Pros Cons Human-readable in any text editor No type preservation (everything becomes strings) Universal compatibility Larger file size than binary formats Easy to inspect and debug Slow to read for large datasets Git-diffable No nested data support <p>Best for: Small-to-medium datasets, sharing with non-technical users, inspection/debugging, data that will be imported into spreadsheets or other tools.</p>"},{"location":"data-pipelines/#parquet-dag-59","title":"Parquet (DAG 59)","text":"<pre><code>df.to_parquet(parquet_path, index=False)\n</code></pre> Pros Cons Columnar format -- fast reads for analytical queries Binary format, not human-readable Type preservation (int, float, bool, datetime, string) Requires pyarrow or fastparquet to read Efficient compression (often 2-10x smaller than CSV) Not git-diffable Predicate pushdown -- read only the columns you need Overkill for tiny datasets <p>Best for: Analytical pipelines, large datasets, data that feeds into pandas/Spark/DuckDB, preserving column types across pipeline stages.</p> <p>Reading Parquet back is straightforward and preserves all types:</p> <pre><code>df = pd.read_parquet(parquet_path)\n# Boolean columns are still bool, ints are still int -- no type coercion needed\n</code></pre>"},{"location":"data-pipelines/#geojson-dag-61","title":"GeoJSON (DAG 61)","text":"<pre><code>import json\n\ngeojson = {\n    \"type\": \"FeatureCollection\",\n    \"features\": features,\n}\nwith open(geojson_path, \"w\") as f:\n    json.dump(geojson, f, indent=2)\n</code></pre> Pros Cons Standard format for geographic data Verbose -- large files for complex geometries Supported by QGIS, Mapbox, Leaflet, GitHub map preview Slower to parse than binary formats (Shapefile, GeoParquet) JSON-based -- no special libraries needed to create No indexing -- full scan for spatial queries Human-readable geometry representation Not suitable for raster data <p>Best for: Sharing geographic data, web map visualization, datasets with mixed geometry types, when you want to avoid heavyweight GIS dependencies.</p>"},{"location":"data-pipelines/#geojson-construction-without-geopandas","title":"GeoJSON Construction Without GeoPandas","text":"<p>DAG 61 builds GeoJSON manually using only the <code>json</code> standard library module. This avoids pulling in <code>geopandas</code> (which depends on GDAL/GEOS C libraries) for a simple metadata export.</p>"},{"location":"data-pipelines/#geojson-structure","title":"GeoJSON Structure","text":"<p>A GeoJSON file is a <code>FeatureCollection</code> containing <code>Feature</code> objects. Each feature has a <code>geometry</code> (the spatial shape) and <code>properties</code> (attribute data):</p> <pre><code>{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [10.75, 59.91]\n      },\n      \"properties\": {\n        \"id\": \"abc123\",\n        \"name\": \"Oslo Health Centre\",\n        \"level\": 4\n      }\n    },\n    {\n      \"type\": \"Feature\",\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [[[10.0, 59.0], [11.0, 59.0], [11.0, 60.0], [10.0, 60.0], [10.0, 59.0]]]\n      },\n      \"properties\": {\n        \"id\": \"def456\",\n        \"name\": \"Oslo District\",\n        \"level\": 3\n      }\n    }\n  ]\n}\n</code></pre> <p>Coordinate order: GeoJSON uses <code>[longitude, latitude]</code> (not <code>[lat, lon]</code>). This is the opposite of what most people expect. DHIS2 stores geometry in GeoJSON format natively, so no coordinate swapping is needed.</p>"},{"location":"data-pipelines/#geometry-types-in-dhis2","title":"Geometry Types in DHIS2","text":"<p>The play server's organisation units use three geometry types:</p> Type Count Represents <code>Point</code> ~601 Individual health facilities (lat/lon location) <code>Polygon</code> ~143 Administrative boundaries (districts, provinces) <code>MultiPolygon</code> ~22 Complex boundaries with disconnected parts (islands, exclaves) <p>The remaining ~809 org units have no geometry at all (data-only nodes in the hierarchy).</p>"},{"location":"data-pipelines/#building-features","title":"Building Features","text":"<p>The DHIS2 API returns geometry as a GeoJSON geometry object already -- no coordinate parsing needed. The construction pattern filters records and wraps each in a Feature:</p> <pre><code>features = []\nfor record in records:\n    geom = record.get(\"geometry\")\n    if not geom:\n        continue  # Skip units without geometry (~809 of ~1575)\n\n    parent = record.get(\"parent\")\n    parent_id = parent[\"id\"] if isinstance(parent, dict) else None\n\n    feature = {\n        \"type\": \"Feature\",\n        \"geometry\": geom,  # Already valid GeoJSON geometry from DHIS2\n        \"properties\": {\n            \"id\": record[\"id\"],\n            \"name\": record.get(\"name\", \"\"),\n            \"shortName\": record.get(\"shortName\", \"\"),\n            \"level\": record.get(\"level\"),\n            \"parent_id\": parent_id,\n        },\n    }\n    features.append(feature)\n</code></pre>"},{"location":"data-pipelines/#computing-a-bounding-box","title":"Computing a Bounding Box","text":"<p>To summarize the spatial extent, extract coordinates from Point geometries:</p> <pre><code>lats, lons = [], []\nfor feat in features:\n    geom = feat[\"geometry\"]\n    if geom[\"type\"] == \"Point\":\n        coords = geom[\"coordinates\"]\n        lons.append(coords[0])  # GeoJSON: [lon, lat]\n        lats.append(coords[1])\n\nif lats and lons:\n    print(f\"Lat: {min(lats):.4f} to {max(lats):.4f}\")\n    print(f\"Lon: {min(lons):.4f} to {max(lons):.4f}\")\n</code></pre> <p>For Polygon/MultiPolygon geometries, you'd need to recurse into the nested coordinate arrays -- but Point coordinates are sufficient for a quick bounding box of facility locations.</p>"},{"location":"data-pipelines/#regex-expression-parsing","title":"Regex Expression Parsing","text":"<p>DHIS2 indicators are calculated from formulas that reference data elements using <code>#{...}</code> syntax:</p> <pre><code>Numerator:   #{fbfJHSPpUQD.pq2XI5kz2BY} + #{fbfJHSPpUQD.PT59n8BQbqM}\nDenominator: #{h0xKKjijTdI}\n</code></pre> <p>Each <code>#{...}</code> token is an operand referencing a data element (and optionally a category option combo, separated by <code>.</code>). DAG 60 uses regex to extract and count these operands:</p> <pre><code>import re\n\ndef count_operands(expr: object) -&gt; int:\n    \"\"\"Count #{...} operand references in a DHIS2 expression.\"\"\"\n    if not isinstance(expr, str):\n        return 0\n    return len(re.findall(r\"#\\{[^}]+\\}\", expr))\n</code></pre> <p>Regex breakdown:</p> Part Meaning <code>#\\{</code> Literal <code>#{</code> (braces escaped because <code>{</code> is a regex quantifier) <code>[^}]+</code> One or more characters that are NOT <code>}</code> (the operand ID) <code>\\}</code> Literal closing <code>}</code>"},{"location":"data-pipelines/#complexity-scoring","title":"Complexity Scoring","text":"<p>The expression complexity score combines operand counts with operator counts to estimate how \"involved\" an indicator formula is:</p> <pre><code>def complexity_score(row: pd.Series) -&gt; int:\n    \"\"\"Score = total operands + arithmetic operators.\"\"\"\n    score = int(row[\"numerator_operands\"]) + int(row[\"denominator_operands\"])\n    for field in [\"numerator\", \"denominator\"]:\n        expr = row.get(field)\n        if isinstance(expr, str):\n            score += expr.count(\"+\") + expr.count(\"-\") + expr.count(\"*\")\n    return score\n</code></pre> <p>This produces a rough distribution:</p> Complexity Meaning Typical Example 0 No operands (constant or empty) Fixed-value indicators 1--4 Simple ratio (1-2 operands per side) Coverage = doses / population 5--9 Multi-component (several data elements combined) Composite scores 10+ Complex formula (many operands, arithmetic) Weighted indices"},{"location":"data-pipelines/#parallel-fan-out-fan-in-pattern","title":"Parallel Fan-Out / Fan-In Pattern","text":"<p>DAG 62 demonstrates a common real-world pattern: fetch multiple independent data sources in parallel, transform each independently, then combine results in a single summary task.</p>"},{"location":"data-pipelines/#pipeline-structure","title":"Pipeline Structure","text":"<pre><code>                    \u250c\u2500 fetch_org_units \u2500\u2500&gt; transform_org_units \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                                                    \u2502\nstart \u2500\u2500&gt; parallel \u2500\u253c\u2500 fetch_data_elements \u2500\u2500&gt; transform_data_elements \u2500\u253c\u2500\u2500&gt; combined_report \u2500\u2500&gt; cleanup\n                    \u2502                                                    \u2502\n                    \u2514\u2500 fetch_indicators \u2500\u2500&gt; transform_indicators \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In TaskFlow API, the parallelism happens automatically -- Airflow's scheduler detects that the three fetch tasks have no dependencies on each other and runs them concurrently:</p> <pre><code>with DAG(dag_id=\"62_dhis2_combined_export\", ...):\n    # These three have no upstream dependencies -- they run in parallel\n    org_raw = fetch_org_units()\n    elem_raw = fetch_data_elements()\n    ind_raw = fetch_indicators()\n\n    # These depend on their respective fetches -- each starts as soon as its input is ready\n    org_csv = transform_org_units(records=org_raw)\n    elem_parquet = transform_data_elements(records=elem_raw)\n    ind_csv = transform_indicators(records=ind_raw)\n\n    # Fan-in: waits for ALL three transforms to complete\n    report = combined_report(\n        org_units_path=org_csv,\n        data_elements_path=elem_parquet,\n        indicators_path=ind_csv,\n    )\n\n    # Sequential: cleanup runs after report\n    cleanup_task = BashOperator(\n        task_id=\"cleanup\",\n        bash_command=f\"rm -rf {OUTPUT_DIR} &amp;&amp; echo 'Cleaned up {OUTPUT_DIR}'\",\n    )\n    report &gt;&gt; cleanup_task\n</code></pre> <p>How Airflow resolves the parallelism:</p> <ol> <li><code>fetch_org_units</code>, <code>fetch_data_elements</code>, <code>fetch_indicators</code> -- all three are schedulable    immediately (no <code>blockedBy</code>). With <code>LocalExecutor</code>, they run as concurrent processes.</li> <li><code>transform_org_units</code> starts as soon as <code>fetch_org_units</code> completes (it doesn't wait for the    other fetches).</li> <li><code>combined_report</code> has three upstream dependencies -- Airflow's default <code>trigger_rule=all_success</code>    means it waits for all three transforms to succeed.</li> <li><code>cleanup</code> runs last, after the report.</li> </ol> <p>Benefits of this pattern:</p> Benefit Description Faster execution Three API calls run concurrently instead of sequentially (~3x faster) Independent failure If <code>fetch_indicators</code> fails, <code>transform_org_units</code> still completes Clear data lineage The DAG graph shows exactly which data feeds into which output Easy to extend Adding a 4th data source is just another fetch/transform pair"},{"location":"data-pipelines/#multi-format-output","title":"Multi-Format Output","text":"<p>The combined export writes each dataset in the format best suited to its downstream use:</p> <pre><code># Org units -&gt; CSV (human-readable, simple tabular data)\norg_df.to_csv(f\"{OUTPUT_DIR}/combined_org_units.csv\", index=False)\n\n# Data elements -&gt; Parquet (preserves boolean types, efficient for analysis)\nelem_df.to_parquet(f\"{OUTPUT_DIR}/combined_data_elements.parquet\", index=False)\n\n# Indicators -&gt; CSV (small dataset, human-readable expressions)\nind_df.to_csv(f\"{OUTPUT_DIR}/combined_indicators.csv\", index=False)\n</code></pre>"},{"location":"data-pipelines/#58-organisation-units-to-csv","title":"58 -- Organisation Units to CSV","text":"<p>Fetches all organisation units (~1575), flattens nested parent references, createdBy usernames, and path-based hierarchy depth, counts translations per unit, and writes a flat CSV.</p> <p>Pipeline:</p> <pre><code>fetch (GET /api/organisationUnits) -&gt; transform (flatten + derive columns) -&gt; report (formatted summary)\n</code></pre> <p>Key transforms:</p> <pre><code>df[\"parent_id\"] = df[\"parent\"].apply(lambda p: p[\"id\"] if isinstance(p, dict) else None)\ndf[\"created_by\"] = df[\"createdBy\"].apply(lambda c: c.get(\"username\") if isinstance(c, dict) else None)\ndf[\"hierarchy_depth\"] = df[\"path\"].apply(lambda p: p.count(\"/\") - 1 if isinstance(p, str) else 0)\ndf[\"translation_count\"] = df[\"translations\"].apply(lambda t: len(t) if isinstance(t, list) else 0)\n</code></pre> <p>Output columns: <code>id</code>, <code>name</code>, <code>shortName</code>, <code>level</code>, <code>parent_id</code>, <code>created_by</code>, <code>hierarchy_depth</code>, <code>translation_count</code>, <code>openingDate</code></p> <p>Report output includes:</p> <ul> <li>Count by organisational level (country, province, district, facility)</li> <li>Top 10 org units sorted by level</li> <li>Opening date range across all units</li> <li>Hierarchy depth range and translation coverage</li> </ul> <p>See: <code>dags/58_dhis2_org_units.py</code></p>"},{"location":"data-pipelines/#59-data-elements-to-parquet","title":"59 -- Data Elements to Parquet","text":"<p>Fetches all data elements (~1037), flattens category combo references, adds boolean and computed columns, and writes as Parquet to preserve column types.</p> <p>Pipeline:</p> <pre><code>fetch (GET /api/dataElements) -&gt; transform (flatten + categorize) -&gt; report (summary stats)\n</code></pre> <p>Key transforms:</p> <pre><code>df[\"category_combo_id\"] = df[\"categoryCombo\"].apply(lambda c: c[\"id\"] if isinstance(c, dict) else None)\ndf[\"has_code\"] = df[\"code\"].notna()        # Boolean: was a code assigned?\ndf[\"name_length\"] = df[\"name\"].str.len()   # Numeric: name string length\n</code></pre> <p>Why Parquet for this dataset? Data elements have boolean (<code>has_code</code>) and integer (<code>name_length</code>) columns. CSV would coerce booleans to strings (<code>\"True\"</code>/<code>\"False\"</code>) and require type inference on read. Parquet preserves exact types:</p> <pre><code># Reading Parquet preserves types\ndf = pd.read_parquet(\"data_elements.parquet\")\nassert df[\"has_code\"].dtype == bool          # Stays bool, not object\nassert df[\"name_length\"].dtype == \"int64\"    # Stays int, not object\n</code></pre> <p>Report output includes:</p> <ul> <li>Value type breakdown (NUMBER, BOOLEAN, TEXT, etc.)</li> <li>Aggregation type breakdown (SUM, AVERAGE, COUNT, etc.)</li> <li>Domain type distribution (AGGREGATE vs TRACKER)</li> <li>Code assignment coverage</li> <li>Name length statistics (min, max, mean)</li> </ul> <p>See: <code>dags/59_dhis2_data_elements.py</code></p>"},{"location":"data-pipelines/#60-indicators-with-expression-parsing","title":"60 -- Indicators with Expression Parsing","text":"<p>Fetches all indicators (~77), flattens indicator type references, parses numerator and denominator expressions with regex, and computes a complexity score.</p> <p>Pipeline:</p> <pre><code>fetch (GET /api/indicators) -&gt; transform (parse expressions + score complexity) -&gt; report (ranked summary)\n</code></pre> <p>Key transforms:</p> <pre><code>import re\n\ndef count_operands(expr: object) -&gt; int:\n    if not isinstance(expr, str):\n        return 0\n    return len(re.findall(r\"#\\{[^}]+\\}\", expr))\n\ndf[\"numerator_operands\"] = df[\"numerator\"].apply(count_operands)\ndf[\"denominator_operands\"] = df[\"denominator\"].apply(count_operands)\ndf[\"expression_complexity\"] = df.apply(complexity_score, axis=1)\n</code></pre> <p>Report output includes:</p> <ul> <li>Complexity distribution (trivial / simple / moderate / complex / very complex)</li> <li>Top 5 most complex indicators with their scores</li> <li>Top 5 simplest non-trivial indicators</li> <li>Indicator type breakdown (Per cent, Per 1000, etc.)</li> </ul> <p>See: <code>dags/60_dhis2_indicators.py</code></p>"},{"location":"data-pipelines/#61-organisation-unit-geometry-to-geojson","title":"61 -- Organisation Unit Geometry to GeoJSON","text":"<p>Fetches org units with a targeted field list (only geometry-relevant fields), filters to the ~766 units that have spatial data, builds a GeoJSON FeatureCollection, and writes to disk.</p> <p>Pipeline:</p> <pre><code>fetch (GET /api/organisationUnits?fields=id,name,...,geometry) -&gt; transform (filter + build GeoJSON) -&gt; write_geojson -&gt; report\n</code></pre> <p>Key difference from DAG 58: This DAG requests specific fields (<code>id,name,shortName,level,parent,geometry</code>) instead of <code>:owner</code>. This is more efficient -- geometry data can be large (polygon coordinate arrays), so we skip irrelevant fields like <code>translations</code>, <code>openingDate</code>, etc.</p> <pre><code>records = fetch_metadata(\n    \"organisationUnits\",\n    fields=\"id,name,shortName,level,parent,geometry\",\n)\n</code></pre> <p>Report output includes:</p> <ul> <li>Total feature count</li> <li>Geometry type breakdown (Point: ~601, Polygon: ~143, MultiPolygon: ~22)</li> <li>Bounding box from Point geometries (lat/lon extent)</li> <li>Level distribution of features with geometry</li> </ul> <p>See: <code>dags/61_dhis2_org_unit_geometry.py</code></p>"},{"location":"data-pipelines/#62-combined-parallel-export","title":"62 -- Combined Parallel Export","text":"<p>Fetches all three metadata types in parallel, transforms each independently, writes multi-format output, and produces a combined summary report. A <code>BashOperator</code> cleanup task removes all temporary files.</p> <p>Pipeline:</p> <pre><code>fetch_org_units \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; transform_org_units (CSV) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\nfetch_data_elements \u2500\u2500\u2500\u2500&gt; transform_data_elements (Parquet) \u2500\u2500\u253c\u2500\u2500&gt; combined_report \u2500\u2500&gt; cleanup\nfetch_indicators \u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; transform_indicators (CSV) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Report output includes:</p> <ul> <li>Record counts per metadata type and total</li> <li>Org unit level distribution</li> <li>Data element domain type breakdown</li> <li>Indicator complexity summary (mean, max, trivial count)</li> <li>List of all output file paths</li> </ul> <p>See: <code>dags/62_dhis2_combined_export.py</code></p>"},{"location":"data-pipelines/#dhis2-with-airflow-connections","title":"DHIS2 with Airflow Connections","text":"<p>DAGs 110--111 revisit DHIS2 but use Airflow Connections to manage credentials instead of hardcoding them. If you skipped DAGs 58--62 because DHIS2 is not relevant to your work, you can still use these two DAGs as a reference for the Airflow Connection pattern -- the concepts apply to any external API.</p> <p>Key difference from DAGs 58--62: The earlier DAGs stored the DHIS2 base URL and credentials as Python constants in <code>dhis2.py</code>. DAGs 110--111 rely on a <code>dhis2_default</code> connection configured in Airflow (via the UI, CLI, or <code>compose.yml</code> environment variables). The shared helper <code>_get_dhis2_config()</code> reads the connection at runtime with <code>BaseHook.get_connection()</code>.</p> <p>Why connections matter:</p> Approach DAGs 58--62 DAGs 110--111 Credentials stored in Python source code Airflow metadata DB / env vars Changing the server URL Edit code, redeploy DAGs Update the connection, no code change Secret exposure risk Credentials in version control Credentials managed by Airflow Missing credentials <code>httpx</code> error at request time <code>AirflowFailException</code> before the request"},{"location":"data-pipelines/#110-connection-basics","title":"110 -- Connection Basics","text":"<p>Retrieves the <code>dhis2_default</code> connection details and fetches a simple org unit count to verify the connection works end-to-end.</p> <p>Pipeline:</p> <pre><code>show_connection -&gt; fetch_org_unit_count\n</code></pre> <p>Tasks:</p> Task Description <code>show_connection</code> Reads <code>dhis2_default</code> via <code>BaseHook.get_connection()</code> and prints connection fields (password redacted) <code>fetch_org_unit_count</code> Calls <code>fetch_metadata(\"organisationUnits\", fields=\"id\")</code> and prints the record count <pre><code>@task\ndef show_connection() -&gt; dict:\n    conn = BaseHook.get_connection(\"dhis2_default\")\n    info = {\n        \"conn_id\": conn.conn_id,\n        \"conn_type\": conn.conn_type,\n        \"host\": conn.host,\n        \"login\": conn.login,\n    }\n    print(f\"[{timestamp()}] Connection details (password redacted):\")\n    for key, value in info.items():\n        print(f\"  {key}: {value}\")\n    return info\n</code></pre> <p>See: <code>dags/110_dhis2_connection_basics.py</code></p>"},{"location":"data-pipelines/#111-analytics-data-values","title":"111 -- Analytics Data Values","text":"<p>Fetches analytics data (not metadata) from the DHIS2 <code>/api/analytics</code> endpoint, transforms the DHIS2 rows/headers response format into a pandas DataFrame, and writes to CSV.</p> <p>Pipeline:</p> <pre><code>fetch_data_values -&gt; transform -&gt; report\n</code></pre> <p>Tasks:</p> Task Description <code>fetch_data_values</code> Calls <code>/api/analytics</code> with dimension parameters for ANC visit data elements at the national level <code>transform</code> Parses the DHIS2 <code>headers</code>/<code>rows</code> response into a DataFrame and writes CSV <code>report</code> Reads the CSV back and prints summary statistics (row count, data element breakdown, value range) <p>DHIS2 analytics response format: Unlike metadata endpoints that return a list of objects, the analytics API returns a tabular structure with separate <code>headers</code> and <code>rows</code> arrays:</p> <pre><code>headers = [h[\"name\"] for h in analytics[\"headers\"]]\nrows = analytics[\"rows\"]\ndf = pd.DataFrame(rows, columns=headers)\n</code></pre> <p>See: <code>dags/111_dhis2_data_values.py</code></p>"},{"location":"data-pipelines/#file-based-etl-pipelines","title":"File-Based ETL Pipelines","text":"<p>DAGs 63--67 demonstrate production-grade file processing patterns: landing zone architecture, multi-format ingestion, error handling with quarantine, and incremental processing with manifests. These patterns apply to any batch file processing pipeline -- CSV uploads, JSON event streams, mixed-format data lakes, and incremental file watches.</p>"},{"location":"data-pipelines/#landing-zone-architecture","title":"Landing Zone Architecture","text":"<p>File-based ETL pipelines follow a directory-stage pattern where files move through well-defined zones:</p> <pre><code>Landing -&gt; Processing -&gt; Archive\n                    \\-&gt; Quarantine (bad files)\n</code></pre> Directory Purpose Lifecycle Landing Drop zone for incoming files; untouched raw data Files arrive here, are read, then moved out Processing Intermediate work area; transformed data written here Populated during pipeline run, consumed downstream Archive Timestamped copies of successfully processed originals Retained for audit trail and reprocessing Quarantine Bad files + companion <code>.reason</code> files explaining why Reviewed manually, fixed, or discarded <p>The shared helper module <code>src/airflow_examples/file_utils.py</code> provides constants and functions for this pattern:</p> <pre><code>LANDING_DIR = \"/tmp/airflow_landing\"\nPROCESSED_DIR = \"/tmp/airflow_processed\"\nARCHIVE_DIR = \"/tmp/airflow_archive\"\nQUARANTINE_DIR = \"/tmp/airflow_quarantine\"\n\ndef setup_dirs(*dirs):       # Create all directories\ndef archive_file(src, dir):  # Move with timestamp prefix (20250101T120000_data.csv)\ndef quarantine_file(src, dir, reason):  # Move + write .reason companion file\n</code></pre> <p>Why timestamp-prefixed archives? When the same filename arrives in multiple batches (e.g., daily <code>export.csv</code>), timestamp prefixes prevent overwrites and create a natural audit trail.</p>"},{"location":"data-pipelines/#file-detection-patterns","title":"File Detection Patterns","text":"<p>Three approaches for detecting new files, from simplest to most robust:</p> Pattern Use Case DAG Example <code>glob.glob(\"*.csv\")</code> Simple batch -- process everything in landing dir DAGs 63-66 Manifest-based diffing Incremental -- only process files not in manifest DAG 67 <code>FileSensor</code> Event-driven -- wait for specific file to appear DAG 50 <p>Glob is simplest: scan the landing directory, return all matching files. Good when you process everything and archive after. The downside is it requires cleanup to avoid reprocessing.</p> <p>Manifest-based tracking (DAG 67) maintains a JSON file listing previously processed filenames and timestamps. On each run, glob the directory, diff against the manifest, and only process new arrivals:</p> <pre><code># Load manifest, diff against directory listing\nmanifest = json.load(open(\"manifest.json\"))\nprocessed_names = {entry[\"filename\"] for entry in manifest[\"files\"]}\nnew_files = [f for f in glob.glob(\"*.csv\") if basename(f) not in processed_names]\n</code></pre> <p>This enables idempotent, incremental processing without moving or deleting source files.</p>"},{"location":"data-pipelines/#csv-and-json-parsing","title":"CSV and JSON Parsing","text":"<p>CSV parsing (DAG 63): Use <code>pd.read_csv()</code> with validation:</p> <pre><code>df = pd.read_csv(file_path)\nexpected = {\"station\", \"date\", \"temperature_f\", \"humidity_pct\", \"pressure_hpa\"}\nif not expected.issubset(set(df.columns)):\n    # Handle schema mismatch\n</code></pre> <p>JSON flattening (DAG 64): Use <code>pd.json_normalize()</code> for nested structures:</p> <pre><code># Nested JSON: {\"station\": {\"id\": \"oslo_01\", \"location\": {\"lat\": 59.91}}}\ndf = pd.json_normalize(events, sep=\"_\")\n# Result columns: station_id, station_location_lat, ...\n</code></pre> <p>The <code>sep=\"_\"</code> parameter controls how nested key paths join into column names.</p> <p>Mixed-format harmonization (DAG 65): When CSV and JSON use different column names for the same data, standardize with rename maps:</p> <pre><code>rename_map = {\"temperature_f\": \"temperature\", \"humidity_pct\": \"humidity\"}\ndf = df.rename(columns=rename_map)\n</code></pre>"},{"location":"data-pipelines/#quarantine-pattern","title":"Quarantine Pattern","text":"<p>DAG 66 demonstrates isolating errors at two levels:</p> <ol> <li>File-level: Completely unparseable files (corrupt CSV, invalid encoding) are moved to    quarantine with a <code>.reason</code> companion file</li> <li>Row-level: Parseable files with some bad rows split into good DataFrame + bad rows list</li> </ol> <pre><code>try:\n    df = pd.read_csv(file_path)\nexcept Exception as e:\n    quarantine_file(file_path, QUARANTINE_DIR, f\"Unparseable: {e}\")\n    continue\n\n# Row-level validation\nnumeric_temp = pd.to_numeric(df[\"temperature_f\"], errors=\"coerce\")\nvalid_mask = ~(numeric_temp.isna() &amp; df[\"temperature_f\"].notna())\ngood_df = df[valid_mask]\nbad_df = df[~valid_mask]\n</code></pre> <p>This pattern ensures that one bad file doesn't stop the entire batch, and bad data is preserved for investigation rather than silently dropped.</p>"},{"location":"data-pipelines/#per-dag-reference","title":"Per-DAG Reference","text":""},{"location":"data-pipelines/#63-csv-landing-zone","title":"63 -- CSV Landing Zone","text":"<p>Watch directory for CSVs, validate headers, convert temperature F-&gt;C, archive originals.</p> <pre><code>setup -&gt; detect_files -&gt; process_file -&gt; archive_originals -&gt; report -&gt; cleanup\n</code></pre> <p>See: <code>dags/63_csv_landing_zone.py</code></p>"},{"location":"data-pipelines/#64-json-event-ingestion","title":"64 -- JSON Event Ingestion","text":"<p>Parse nested JSON event files, flatten with <code>json_normalize</code>, write combined Parquet.</p> <pre><code>setup -&gt; detect_events -&gt; parse_and_normalize -&gt; archive_events -&gt; report -&gt; cleanup\n</code></pre> <p>See: <code>dags/64_json_event_ingestion.py</code></p>"},{"location":"data-pipelines/#65-multi-file-batch","title":"65 -- Multi-File Batch","text":"<p>Process mixed CSV+JSON, harmonize column names, merge and deduplicate.</p> <pre><code>setup -&gt; detect_and_classify -&gt; process_csv_batch -+\n                             -&gt; process_json_batch -+-&gt; merge -&gt; write_summary -&gt; report -&gt; cleanup\n</code></pre> <p>See: <code>dags/65_multi_file_batch.py</code></p>"},{"location":"data-pipelines/#66-error-handling-etl","title":"66 -- Error Handling ETL","text":"<p>Quarantine pattern: corrupt files dead-lettered, bad rows isolated, clean data flows through.</p> <pre><code>setup -&gt; detect -&gt; process_with_quarantine -&gt; write_clean_output --+\n                                           -&gt; write_quarantine_log +-&gt; quarantine_report -&gt; cleanup\n</code></pre> <p>See: <code>dags/66_error_handling_etl.py</code></p>"},{"location":"data-pipelines/#67-incremental-file-processing","title":"67 -- Incremental File Processing","text":"<p>Manifest-based tracking -- only process files not yet recorded in <code>manifest.json</code>.</p> <pre><code>setup -&gt; detect_new_files -&gt; process_new_files -&gt; update_manifest -&gt; report -&gt; cleanup\n</code></pre> <p>See: <code>dags/67_incremental_file_processing.py</code></p>"},{"location":"external-apis/","title":"API Pipelines","text":""},{"location":"external-apis/#weather-climate-apis","title":"Weather &amp; Climate APIs","text":"<p>DAGs 81--100 are where it all comes together. Instead of fake data, these pipelines call real public APIs (no API keys needed) to fetch weather forecasts, earthquake data, health statistics, and more. They combine everything from the earlier sections: <code>@task</code> functions, pandas transforms, error handling, and multi-step orchestration.</p> <p>DAGs 81--86 focus on weather and climate data. They use the <code>airflow_examples.apis</code> helper module for consistent API access.</p>"},{"location":"external-apis/#open-meteo-ecosystem","title":"Open-Meteo Ecosystem","text":"<p>Open-Meteo provides a family of free weather APIs with no authentication required. The project uses seven endpoints:</p> Endpoint Base URL Data Forecast <code>api.open-meteo.com/v1/forecast</code> Hourly/daily forecasts up to 16 days Archive <code>archive-api.open-meteo.com/v1/archive</code> Historical weather data back to 1940 Air Quality <code>air-quality-api.open-meteo.com/v1/air-quality</code> PM2.5, PM10, NO2, O3, European AQI Marine <code>marine-api.open-meteo.com/v1/marine</code> Wave height, swell, wave period Flood <code>flood-api.open-meteo.com/v1/flood</code> River discharge forecasts (GloFAS) Geocoding <code>geocoding-api.open-meteo.com/v1/search</code> City name to coordinates resolution Elevation <code>api.open-meteo.com/v1/elevation</code> Elevation for given coordinates <p>All Open-Meteo calls go through <code>fetch_open_meteo()</code> which automatically adds <code>timezone=auto</code>.</p>"},{"location":"external-apis/#dynamic-task-mapping-with-expand","title":"Dynamic Task Mapping with .expand()","text":"<p>DAG 81 demonstrates mapping a task across multiple cities using Airflow's dynamic task mapping:</p> <pre><code>from airflow_examples.apis import NORDIC_CITIES\n\n@task\ndef fetch_forecast(city: dict[str, object]) -&gt; dict[str, object]:\n    params = {\"latitude\": city[\"lat\"], \"longitude\": city[\"lon\"], ...}\n    return fetch_open_meteo(OPEN_METEO_FORECAST, params)\n\n# Creates 5 parallel task instances, one per city\nfetched = fetch_forecast.expand(city=NORDIC_CITIES)\ncombined = combine_forecasts(fetched)  # receives list of all results\n</code></pre> <p>The <code>.expand()</code> call creates one task instance per item in the list at runtime. Downstream tasks receiving the mapped output get a list of all results automatically.</p>"},{"location":"external-apis/#forecast-verification-methodology","title":"Forecast Verification Methodology","text":"<p>DAG 82 computes standard forecast accuracy metrics:</p> <ul> <li>MAE (Mean Absolute Error): Average of |forecast - actual|. Interpretable in the same units   as the variable (e.g., degrees Celsius).</li> <li>RMSE (Root Mean Squared Error): Penalizes large errors more heavily than MAE. Always &gt;= MAE.</li> <li>Bias: Mean of (forecast - actual). Positive = systematic over-prediction.</li> <li>Lead-time degradation: Accuracy typically worsens from day 1 to day 7. DAG 82 groups by   forecast lead time to quantify this effect.</li> </ul>"},{"location":"external-apis/#aqi-classification-and-who-thresholds","title":"AQI Classification and WHO Thresholds","text":"<p>DAG 83 classifies European AQI values using WHO 2021 air quality guidelines:</p> AQI Range Category PM2.5 (ug/m3) PM10 (ug/m3) NO2 (ug/m3) 0--20 Good &lt; 15 &lt; 45 &lt; 25 20--40 Fair 15--30 45--90 25--50 40--60 Moderate 30--55 90--180 50--100 60--80 Poor 55--110 180--360 100--200 80--100 Very Poor 110--220 360--720 200--400 &gt; 100 Extremely Poor &gt; 220 &gt; 720 &gt; 400 <p>The DAG identifies exceedance hours (values exceeding WHO guideline levels) and generates per-city health advisories based on the worst observed category.</p>"},{"location":"external-apis/#composite-risk-index","title":"Composite Risk Index","text":"<p>DAG 84 combines marine forecast data with river flood discharge into a weighted composite:</p> <pre><code>Marine Risk Score (0-100):    wave_height &lt; 1m = 0, 1-2m = 25, 2-4m = 50, 4-6m = 75, &gt; 6m = 100\nFlood Risk Score (0-100):     discharge/mean &lt; 1.5x = 0, 1.5-3x = 50, &gt; 3x = 100\nComposite = 0.6 * marine + 0.4 * flood\nCategories: Low (0-25), Moderate (25-50), High (50-75), Extreme (75-100)\n</code></pre>"},{"location":"external-apis/#per-dag-reference","title":"Per-DAG Reference","text":"DAG API Key Concept 81 Open-Meteo Forecast <code>.expand()</code> dynamic mapping, cross-city comparison 82 Forecast + Archive MAE, RMSE, bias, lead-time degradation 83 Air Quality WHO thresholds, AQI classification, health advisories 84 Marine + Flood Composite risk scoring, weighted index 85 Sunrise-Sunset Latitude-daylight correlation, seasonal variation 86 Geocoding + Elevation Disambiguation, coordinate-driven enrichment"},{"location":"external-apis/#geographic-economic-apis","title":"Geographic &amp; Economic APIs","text":"<p>DAGs 87--90 work with country, economic, and cross-API data. They demonstrate handling pagination, nested JSON, financial time series, and multi-source enrichment.</p>"},{"location":"external-apis/#rest-countries-nested-json","title":"REST Countries Nested JSON","text":"<p>The REST Countries API returns deeply nested structures:</p> <pre><code>{\n  \"name\": {\"common\": \"Norway\", \"official\": \"Kingdom of Norway\"},\n  \"languages\": {\"nno\": \"Norwegian Nynorsk\", \"nob\": \"Norwegian Bokmal\", \"smi\": \"Sami\"},\n  \"currencies\": {\"NOK\": {\"name\": \"Norwegian krone\", \"symbol\": \"kr\"}},\n  \"borders\": [\"FIN\", \"SWE\", \"RUS\"],\n  \"latlng\": [62.0, 10.0]\n}\n</code></pre> <p>DAG 87 flattens this into relational tables: - Flat country table: code, name, population, area, lat, lon - Bridge table (country_language): country, lang_code, language - Bridge table (country_currency): country, currency_code, currency_name - Edge list (borders): from, to (undirected graph of country borders)</p>"},{"location":"external-apis/#world-bank-api-pagination","title":"World Bank API Pagination","text":"<p>The World Bank API returns <code>[metadata, records]</code> per page. The <code>fetch_world_bank_paginated()</code> helper iterates all pages automatically:</p> <pre><code># Response format: [{\"page\": 1, \"pages\": 3, \"total\": 150}, [records...]]\nrecords = fetch_world_bank_paginated(\"NOR;SWE\", \"NY.GDP.PCAP.CD\", \"2000:2023\")\n</code></pre> <p>DAG 88 fetches three indicators (GDP, CO2, renewable energy), joins them on country+year, and computes Pearson correlations to explore relationships like GDP vs CO2 emissions.</p>"},{"location":"external-apis/#frankfurter-currency-time-series","title":"Frankfurter Currency Time Series","text":"<p>DAG 89 fetches a full year of EUR exchange rates and applies financial analysis:</p> <ul> <li>Log returns: <code>ln(price_t / price_{t-1})</code>, preferred over simple returns for statistical properties</li> <li>Rolling volatility: 30-day rolling standard deviation, annualized by multiplying by sqrt(252)</li> <li>Correlation matrix: how Nordic currencies co-move with EUR</li> <li>Event detection: dates with returns &gt; 2 standard deviations from the mean</li> </ul>"},{"location":"external-apis/#cross-api-enrichment-pattern","title":"Cross-API Enrichment Pattern","text":"<p>DAG 90 demonstrates the enrichment pattern: start with REST Countries base data, then use coordinates from each country's capital to call Open-Meteo for weather and air quality. The result is a unified profile combining static metadata with real-time environmental readings.</p>"},{"location":"external-apis/#per-dag-reference_1","title":"Per-DAG Reference","text":"DAG API Key Concept 87 REST Countries Nested JSON, bridge tables, border graph 88 World Bank Pagination, multi-indicator join, Pearson correlation 89 Frankfurter Log returns, rolling volatility, event detection 90 REST Countries + Open-Meteo Cross-API enrichment, unified profiles"},{"location":"external-apis/#geophysical-environmental-apis","title":"Geophysical &amp; Environmental APIs","text":"<p>DAGs 91--94 cover earthquake analysis, carbon intensity, hypothesis testing, and long-term climate trends.</p>"},{"location":"external-apis/#geojson-featurecollection","title":"GeoJSON FeatureCollection","text":"<p>The USGS Earthquake API returns GeoJSON:</p> <pre><code>{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {\"mag\": 5.2, \"place\": \"Pacific Ocean\", \"time\": 1700000000000},\n      \"geometry\": {\"type\": \"Point\", \"coordinates\": [-150.5, 60.1, 10.0]}\n    }\n  ]\n}\n</code></pre> <p>The <code>fetch_usgs_earthquakes()</code> helper flattens this to flat dicts with lat, lon, depth, magnitude, place, and time fields.</p>"},{"location":"external-apis/#gutenberg-richter-law","title":"Gutenberg-Richter Law","text":"<p>DAG 91 fits the Gutenberg-Richter frequency-magnitude relation:</p> <pre><code>log10(N) = a - b * M\n</code></pre> <p>Where N is the cumulative number of earthquakes with magnitude &gt;= M. The b-value (typically ~1.0) describes the ratio of small to large earthquakes. Regional variations in b-value can indicate different tectonic stress regimes.</p>"},{"location":"external-apis/#uk-carbon-intensity-data-model","title":"UK Carbon Intensity Data Model","text":"<p>DAG 92 uses the UK Carbon Intensity API which provides half-hourly data:</p> <ul> <li>Intensity: forecast and actual gCO2/kWh, with an index category</li> <li>Generation mix: percentage breakdown by fuel type (gas, wind, nuclear, solar, biomass, etc.)</li> </ul> <p>The DAG identifies cleanest/dirtiest hours (useful for scheduling energy-intensive workloads), compares weekday vs weekend patterns, and evaluates forecast accuracy.</p>"},{"location":"external-apis/#null-hypothesis-testing","title":"Null Hypothesis Testing","text":"<p>DAG 93 is a deliberate data science lesson. It fetches earthquakes and weather for Iceland, computes correlations, and confirms that weather does not predict earthquakes (r ~ 0).</p> <p>Key lessons: 1. Correlation does not imply causation 2. Testing obvious null hypotheses validates your statistical pipeline 3. Spurious correlations are common in large datasets (data dredging) 4. Always consider the physical mechanism before claiming a relationship</p>"},{"location":"external-apis/#chunked-historical-requests","title":"Chunked Historical Requests","text":"<p>DAG 94 fetches 20 years of daily climate data in 4-year chunks to respect API limits:</p> <pre><code>year_ranges = [(2004, 2007), (2008, 2011), (2012, 2015), (2016, 2019), (2020, 2024)]\nfor start_year, end_year in year_ranges:\n    data = fetch_open_meteo(OPEN_METEO_ARCHIVE, {..., \"start_date\": f\"{start_year}-01-01\"})\n</code></pre> <p>After concatenation, the DAG computes: - Linear regression temperature trend (degrees per decade) - Monthly climatology (long-term mean per month) - Anomalies from climatology (which years were unusually warm/cold) - Extreme event frequency (days above 30C or below -20C)</p>"},{"location":"external-apis/#per-dag-reference_2","title":"Per-DAG Reference","text":"DAG API Key Concept 91 USGS Earthquake GeoJSON parsing, Gutenberg-Richter, spatial binning 92 UK Carbon Intensity Generation mix, daily profiles, forecast accuracy 93 USGS + Open-Meteo Null hypothesis, spurious correlation lesson 94 Open-Meteo Archive Chunked requests, linear regression, seasonal decomposition"},{"location":"external-apis/#global-health-indicators","title":"Global Health Indicators","text":"<p>DAGs 95--97 work with WHO and World Bank health data, demonstrating OData APIs, cross-organization joins, and dimensional modeling.</p>"},{"location":"external-apis/#who-gho-odata-api","title":"WHO GHO OData API","text":"<p>The WHO Global Health Observatory (GHO) provides indicators via an OData endpoint:</p> <pre><code>GET https://ghoapi.azureedge.net/api/WHOSIS_000001?$filter=SpatialDim eq 'NOR'&amp;$top=100\n</code></pre> <p>Response fields include: - <code>SpatialDim</code>: ISO3 country code - <code>TimeDim</code>: year - <code>Dim1</code>: sex (BTSX = both sexes, FMLE = female, MLE = male) - <code>NumericValue</code>: the indicator value</p> <p>The <code>fetch_who_indicator()</code> helper constructs the OData URL with <code>$filter</code> and <code>$top</code> parameters.</p>"},{"location":"external-apis/#cross-organization-data-joining","title":"Cross-Organization Data Joining","text":"<p>DAG 96 joins World Bank health spending with WHO infant mortality. The challenge is that both organizations use ISO3 country codes but different API structures:</p> <ul> <li>World Bank: <code>{\"country\": {\"id\": \"NOR\"}, \"date\": \"2023\", \"value\": 85000}</code></li> <li>WHO: <code>{\"SpatialDim\": \"NOR\", \"TimeDim\": 2023, \"NumericValue\": 1.8}</code></li> </ul> <p>The join key is (country_code, year), requiring field-level normalization before merging.</p>"},{"location":"external-apis/#star-schema-design","title":"Star Schema Design","text":"<p>DAG 97 builds a classic star schema from multiple API sources:</p> <pre><code>dim_country (country_key, code, name, region, population, area)\ndim_time (time_key, year, decade, is_21st_century)\ndim_indicator (indicator_key, code, name, source_org, unit, category)\nfact_health_indicator (country_key, time_key, indicator_key, value)\n</code></pre> <p>Each dimension is populated from a different source (REST Countries, generated, manual definition), and the fact table combines World Bank + WHO indicators with surrogate key references. A composite health index normalizes each indicator to 0--100 and computes a weighted average.</p>"},{"location":"external-apis/#per-dag-reference_3","title":"Per-DAG Reference","text":"DAG API Key Concept 95 WHO GHO OData API, gender gap analysis, life expectancy trends 96 World Bank + WHO Cross-org join, log-linear regression, efficiency ranking 97 REST Countries + World Bank + WHO Star schema, composite health index"},{"location":"external-apis/#advanced-multi-api-data-engineering","title":"Advanced Multi-API Data Engineering","text":"<p>DAGs 98--100 bring together all the patterns from previous sections into increasingly sophisticated data engineering pipelines.</p>"},{"location":"external-apis/#multi-api-orchestration-with-staging-layers","title":"Multi-API Orchestration with Staging Layers","text":"<p>DAG 98 orchestrates 6 different APIs into a single dashboard:</p> <pre><code>REST Countries \u2500\u2510\nOpen-Meteo     \u2500\u2524\nAir Quality    \u2500\u253c\u2500&gt; Stage Raw \u2500&gt; Integrate \u2500&gt; Dashboard Metrics \u2500&gt; Report\nFrankfurter    \u2500\u2524\nWorld Bank     \u2500\u2518\n</code></pre> <p>The staging layer writes each raw API response to Parquet files before integration. This pattern provides: - Reproducibility: raw data preserved for re-processing - Debugging: inspect intermediate state at each layer - Idempotency: staging is overwritten, integration is rebuilt from staging</p>"},{"location":"external-apis/#quality-framework-on-live-data","title":"Quality Framework on Live Data","text":"<p>DAG 99 applies the <code>quality.py</code> framework (from DAGs 68--72) to live API responses:</p> <ul> <li>Schema validation: expected fields present in each API response</li> <li>Completeness: null rates within acceptable thresholds</li> <li>Statistical bounds: temperature within [-60, 60]C, precipitation &gt;= 0, etc.</li> <li>Cross-source integrity: country codes from REST Countries exist in World Bank data</li> </ul> <p>This demonstrates that quality checks are not just for batch files -- they apply equally to real-time API data.</p>"},{"location":"external-apis/#scd-type-2-implementation","title":"SCD Type 2 Implementation","text":"<p>DAG 100 (the capstone) implements Slowly Changing Dimension Type 2:</p> <pre><code>Previous snapshot:  NOR | Norway | population=5400000 | valid_from=2024-01-01 | valid_to=9999-12-31\nCurrent extract:    NOR | Norway | population=5450000\n\nAfter SCD Type 2:\n  NOR | Norway | pop=5400000 | valid_from=2024-01-01 | valid_to=2025-01-01 | is_current=False\n  NOR | Norway | pop=5450000 | valid_from=2025-01-01 | valid_to=9999-12-31 | is_current=True\n</code></pre> <p>Key components: - Surrogate keys: hash-based deterministic integer keys (not business keys) - Change detection: compare current extract with previous snapshot - SCD rows: old row gets <code>valid_to</code> set and <code>is_current=False</code>, new row inserted - Audit trail: extraction timestamp, row counts, change counts, schema version - Validation: referential integrity, no orphan keys, date range validity</p>"},{"location":"external-apis/#per-dag-reference_4","title":"Per-DAG Reference","text":"DAG APIs Key Concept 98 6 APIs (REST Countries, Open-Meteo x2, Frankfurter, World Bank) Staging layers, livability score 99 Open-Meteo + REST Countries + World Bank Quality framework on live API data 100 REST Countries + World Bank SCD Type 2, surrogate keys, audit trail (capstone)"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#before-we-start-things-you-might-not-know-yet","title":"Before We Start: Things You Might Not Know Yet","text":"<p>If you already know what Docker and Bash are, skip to Airflow 101.</p>"},{"location":"getting-started/#what-is-bash","title":"What is Bash?","text":"<p>Bash is the default command-line shell on most Linux and macOS systems. When you open a terminal and type <code>ls</code> or <code>cd</code>, you are using Bash. It is just a way to run commands by typing them.</p> <p>In Airflow, <code>BashOperator</code> lets you run a terminal command as a task. Think of it as:</p> <pre><code># Instead of this (in a terminal):\n#   echo \"Hello from Airflow\"\n#\n# You write this (in a DAG):\ntask = BashOperator(\n    task_id=\"say_hello\",\n    bash_command='echo \"Hello from Airflow\"',\n)\n</code></pre> <p>You do not need to be a Bash expert. Most examples use simple one-liners like <code>echo</code>, <code>rm</code>, or <code>python my_script.py</code>. If you can type a command in a terminal, you can use <code>BashOperator</code>.</p>"},{"location":"getting-started/#what-is-docker","title":"What is Docker?","text":"<p>Docker runs your code inside an isolated \"container\" -- a lightweight mini-computer with its own operating system, files, and installed packages. Think of it like a virtual environment (<code>venv</code>) but for everything, not just Python packages.</p> <p>Why does this project use Docker? Airflow itself has several pieces that need to run together: a web server, a scheduler, a database. Docker Compose starts all of these with a single command (<code>make run</code>) so you do not have to install PostgreSQL, Redis, or Airflow system-wide.</p> <pre><code>Your machine\n  +-- Docker ------------------------------------------+\n  |                                                    |\n  |  [PostgreSQL]  [Airflow Scheduler]  [Web Server]   |\n  |       |              |                   |         |\n  |       +--- all talk to each other -------+         |\n  |                                                    |\n  +----------------------------------------------------+\n</code></pre> <p>You interact with Docker through two commands:</p> <ul> <li><code>docker compose up</code> -- start everything</li> <li><code>docker compose down</code> -- stop everything</li> </ul> <p>The <code>Makefile</code> wraps these for you, so <code>make run</code> is all you need. DAGs 21-32 also show how to run your own code inside Docker containers as Airflow tasks -- but that is an advanced topic you can skip on your first read.</p>"},{"location":"getting-started/#what-is-an-operator","title":"What is an Operator?","text":"<p>An operator is just a Python class that knows how to do one specific thing. Airflow ships with many built-in operators so you do not have to write the plumbing yourself:</p> Operator What it does Python equivalent <code>BashOperator</code> Runs a terminal command <code>subprocess.run([\"echo\", \"hello\"])</code> <code>PythonOperator</code> Calls a Python function <code>my_function()</code> <code>DockerOperator</code> Runs code in a Docker container <code>docker run python:3.12 python script.py</code> <code>HttpOperator</code> Makes an HTTP request <code>requests.get(\"https://api.example.com\")</code> <code>SQLExecuteQueryOperator</code> Runs a SQL query <code>cursor.execute(\"SELECT * FROM ...\")</code> <code>EmailOperator</code> Sends an email <code>smtplib.SMTP(...).send_message(...)</code> <p>You can also skip operators entirely and use the <code>@task</code> decorator, which lets you write plain Python functions and Airflow handles the rest:</p> <pre><code>@task\ndef add_numbers(a, b):\n    return a + b\n\nresult = add_numbers(3, 4)  # Airflow tracks this as a task\n</code></pre> <p>Most examples in this project use the <code>@task</code> style because it feels natural if you know Python.</p>"},{"location":"getting-started/#airflow-101-core-concepts","title":"Airflow 101: Core Concepts","text":""},{"location":"getting-started/#what-is-airflow","title":"What is Airflow?","text":"<p>Imagine you have a Python script that:</p> <ol> <li>Downloads weather data from an API</li> <li>Cleans and transforms it with pandas</li> <li>Saves the results to a database</li> </ol> <p>You could run it manually, or set up a cron job. But what happens when step 1 fails because the API is down? What if step 2 takes longer than expected? How do you re-run just the failed step without re-downloading everything?</p> <p>Airflow solves these problems. You describe your workflow as a DAG (Directed Acyclic Graph) -- which is just a fancy name for \"a list of tasks and the order they run in.\" Airflow then:</p> <ul> <li>Runs tasks in the right order, respecting dependencies</li> <li>Retries failed tasks automatically (with configurable delays)</li> <li>Shows you what happened in a web dashboard with logs, timing, and status</li> <li>Schedules runs on a timer, or triggers them when new data arrives</li> <li>Scales from a laptop to a cluster of machines</li> </ul>"},{"location":"getting-started/#architecture-how-the-pieces-fit-together","title":"Architecture (How the Pieces Fit Together)","text":"<p>When you run <code>make run</code>, Docker starts five processes that work together:</p> <pre><code>You (browser)\n    |\n    v\n+------------------+\n|   Web Server     |  The dashboard you see at localhost:8081\n+------------------+\n        |\n+------------------+\n|   Scheduler      |  Reads your DAG files, decides what to run and when\n+------------------+\n        |\n+------------------+\n|   Executor       |  Actually runs your tasks (on this machine, or on workers)\n+------------------+\n        |\n+------------------+\n|   PostgreSQL     |  A database that remembers task status, logs, and results\n+------------------+\n        |\n+------------------+\n|   Triggerer      |  Handles tasks that are waiting for something (optional)\n+------------------+\n</code></pre> <p>You do not need to configure or think about these -- <code>make run</code> sets everything up. But it helps to know they exist when you read log messages or debug a problem.</p>"},{"location":"getting-started/#key-terminology","title":"Key Terminology","text":"<p>Do not memorize this table. Come back to it when you see a term you do not recognize.</p> Term Plain English DAG Your workflow -- a Python file that describes tasks and their order. \"Directed Acyclic Graph\" just means tasks flow in one direction and never loop back. Task A single step in your workflow. \"Download the data\", \"Clean the data\", \"Save the data\" are three tasks. Operator The type of a task. <code>BashOperator</code> means \"this task runs a terminal command.\" <code>@task</code> means \"this task runs a Python function.\" Sensor A task that waits. \"Wait until the file exists\", \"wait until the API responds.\" Then it lets downstream tasks continue. XCom How tasks pass data to each other. Task A returns a dict, Task B receives it. Short for \"cross-communication.\" DAG Run One execution of your workflow. If your DAG runs daily, each day creates a new DAG Run. Connection Saved credentials (database password, API key, SSH login) so you do not hardcode them in your DAGs. Variable A global key-value setting, like <code>{\"env\": \"staging\"}</code>, accessible from any DAG. Pool A limit on how many tasks can run at once. Useful when an API only allows 3 concurrent requests. Asset A data dependency. \"Run this DAG whenever the weather data is updated.\" New in Airflow 3.x. Backfill Running your DAG for past dates it missed. Useful when you deploy a new daily pipeline mid-month."},{"location":"integrations/","title":"Integrations","text":""},{"location":"integrations/#dbt-integration","title":"dbt Integration","text":"<p>DAGs 101--102 show how Airflow orchestrates dbt transformations. The pattern is: Airflow loads raw data into PostgreSQL, then triggers <code>dbt run</code> and <code>dbt test</code> via <code>BashOperator</code>.</p>"},{"location":"integrations/#how-it-works","title":"How It Works","text":"<ol> <li>DAG 101 fetches data from REST Countries and World Bank APIs, then inserts it into    PostgreSQL tables (<code>raw_countries</code>, <code>raw_indicators</code>).</li> <li>DAG 102 runs <code>dbt run</code> to build staging and mart models, then <code>dbt test</code> to validate    referential integrity and data quality.</li> </ol> <p>dbt handles the SQL transformations (staging views, dimensional models, aggregations) while Airflow handles scheduling, retries, and task dependencies.</p>"},{"location":"integrations/#per-dag-reference","title":"Per-DAG Reference","text":"DAG What It Does 101 Load REST Countries + World Bank data into PostgreSQL for dbt 102 Orchestrate <code>dbt run</code> and <code>dbt test</code> via BashOperator"},{"location":"integrations/#s3-object-storage","title":"S3 Object Storage","text":"<p>DAGs 103--105 demonstrate working with S3-compatible object storage (RustFS). The project includes a RustFS container that provides an S3-compatible API on <code>http://localhost:9001</code>.</p>"},{"location":"integrations/#raw-boto3-dags-103-104","title":"Raw boto3 (DAGs 103--104)","text":"<p>DAGs 103 and 104 use <code>boto3</code> directly -- creating clients with explicit endpoint URLs and credentials, using <code>put_object()</code> / <code>get_object()</code> for I/O, and managing byte buffers manually. This works but requires hardcoded connection details in every task.</p>"},{"location":"integrations/#objectstoragepath-dag-105","title":"ObjectStoragePath (DAG 105)","text":"<p>DAG 105 introduces Airflow's native <code>ObjectStoragePath</code> -- a pathlib-like abstraction that uses Airflow connections instead of hardcoded credentials. The <code>aws_default</code> connection (configured in <code>compose.yml</code>) provides the endpoint URL and credentials automatically.</p> <p>Key patterns:</p> <pre><code>from airflow.sdk import ObjectStoragePath\n\n# Connection-aware path (uses aws_default connection)\nbase = ObjectStoragePath(\"s3://aws_default@airflow-data/\")\n\n# Pathlib-like concatenation\npath = base / \"subdir\" / \"file.parquet\"\n\n# File-like read/write\nwith path.open(\"wb\") as f:\n    df.to_parquet(f)\nwith path.open(\"rb\") as f:\n    df = pd.read_parquet(f)\n\n# Listing and metadata\nfor child in base.iterdir():\n    print(child, child.stat().st_size)\n</code></pre> <p>Compared to raw boto3:</p> Feature boto3 (DAGs 103--104) ObjectStoragePath (DAG 105) Credentials Hardcoded per task Airflow connection (<code>aws_default</code>) Path building String concatenation <code>base / \"subdir\" / \"file\"</code> Read/write <code>get_object()</code> + BytesIO <code>path.open(\"rb\")</code> / <code>path.open(\"wb\")</code> Listing <code>list_objects_v2()</code> <code>path.iterdir()</code> Metadata <code>head_object()</code> <code>path.stat()</code>"},{"location":"integrations/#data-lake-pattern","title":"Data Lake Pattern","text":"<p>DAG 104 demonstrates the bronze/silver/gold data lake pattern:</p> <ul> <li>Bronze: raw API responses as JSON</li> <li>Silver: cleaned, normalized data as Parquet</li> <li>Gold: aggregated, analysis-ready data as Parquet</li> </ul> <p>DAG 105 follows the same layering with <code>ObjectStoragePath</code>, writing forecast data to an <code>objstore/</code> prefix and aggregated summaries to <code>objstore_gold/</code>.</p>"},{"location":"integrations/#per-dag-reference_1","title":"Per-DAG Reference","text":"DAG Approach Key Concept 103 boto3 Write/read Parquet round-trip to RustFS 104 boto3 Bronze/silver/gold data lake pattern 105 ObjectStoragePath Pathlib-like S3 API with Airflow connections"},{"location":"integrations/#human-in-the-loop-hitl","title":"Human-in-the-Loop (HITL)","text":"<p>Airflow 3.1 introduced Human-in-the-Loop operators that pause DAG execution and present a form in the Airflow web UI for human input. DAG 106 demonstrates all four HITL operators in a Nordic weather data quality review workflow.</p>"},{"location":"integrations/#hitl-operators","title":"HITL Operators","text":"Operator Purpose Key Parameters <code>HITLEntryOperator</code> Collect free-text input via <code>Param</code> form fields <code>params</code>, <code>subject</code>, <code>body</code> <code>HITLOperator</code> Single-select (or multi-select) from a list of options <code>options</code>, <code>defaults</code>, <code>multiple</code> <code>ApprovalOperator</code> Binary approve/reject gate; skips downstream on reject <code>defaults</code>, <code>fail_on_reject</code> <code>HITLBranchOperator</code> Route to different downstream tasks based on user choice <code>options</code>, <code>options_mapping</code>, <code>defaults</code> <p>All four operators inherit from <code>HITLOperator</code> and share common parameters:</p> <ul> <li><code>subject</code> -- headline shown in the Airflow UI</li> <li><code>body</code> -- descriptive text (supports Jinja2 templating and Markdown)</li> <li><code>execution_timeout</code> -- auto-selects <code>defaults</code> when timeout expires (essential for <code>airflow dags test</code>)</li> <li><code>defaults</code> -- pre-selected options used when timeout is reached or as initial selection in the UI</li> <li><code>notifiers</code> -- <code>BaseNotifier</code> subclasses called when the task starts waiting</li> </ul>"},{"location":"integrations/#auto-resolution-for-testing","title":"Auto-Resolution for Testing","text":"<p>Every HITL operator in DAG 106 sets <code>execution_timeout=timedelta(seconds=5)</code> with sensible defaults so the pipeline completes without human input during <code>airflow dags test</code>:</p> Operator Default <code>HITLEntryOperator</code> Param default <code>\"Auto-approved: no manual review\"</code> <code>HITLOperator</code> <code>[\"Good\"]</code> <code>ApprovalOperator</code> <code>\"Approve\"</code> <code>HITLBranchOperator</code> <code>[\"export_csv\"]</code>"},{"location":"integrations/#dag-106-task-flow","title":"DAG 106 Task Flow","text":"<pre><code>fetch_weather_data          (@task -- fetch Nordic weather from Open-Meteo)\n       |\n  add_analyst_note          (HITLEntryOperator -- free-text input via Param)\n       |\n  choose_quality_rating     (HITLOperator -- single-select: Excellent/Good/Acceptable/Poor)\n       |\n  approve_publication       (ApprovalOperator -- Approve/Reject gate)\n       |\n  choose_output_format      (HITLBranchOperator -- routes to one export task)\n     / | \\\nexport_csv  export_json  export_parquet\n     \\ | /\n  join_exports              (EmptyOperator, trigger_rule=\"none_failed_min_one_success\")\n       |\n  publish_report            (@task -- summary of all HITL decisions)\n</code></pre>"},{"location":"integrations/#per-dag-reference_2","title":"Per-DAG Reference","text":"DAG Key Concept 106 All four HITL operators with auto-resolution defaults"},{"location":"integrations/#variable-driven-scheduling","title":"Variable-Driven Scheduling","text":"<p>Airflow's REST API does not support changing a DAG's schedule directly. The recommended workaround is to read the schedule from an Airflow Variable, which can be updated via the API or the UI. DAG 107 demonstrates this pattern, building on DAG 17 (Variables &amp; Params) by showing how Variables can control when a DAG runs -- not just how it runs.</p>"},{"location":"integrations/#how-it-works_1","title":"How It Works","text":"<ol> <li>At parse time the DAG file calls <code>Variable.get(\"dag_107_schedule\",    default_var=\"@daily\")</code> and passes the result to <code>schedule=</code>.</li> <li>The scheduler re-parses DAG files periodically, so updating the Variable    changes the effective schedule on the next parse cycle.</li> <li>No code change or deployment is needed -- just update the Variable.</li> </ol>"},{"location":"integrations/#changing-the-schedule","title":"Changing the Schedule","text":"<p>Via the Airflow UI: Admin -&gt; Variables -&gt; <code>dag_107_schedule</code> -&gt; edit value.</p> <p>Via the REST API (Airflow 3.x):</p> <pre><code>curl -X PATCH http://localhost:8081/api/v2/variables/dag_107_schedule \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"value\": \"@hourly\"}'\n</code></pre>"},{"location":"integrations/#dag-107-task-flow","title":"DAG 107 Task Flow","text":"<pre><code>show_schedule_info      (@task -- print current schedule and change instructions)\n  |\nset_new_schedule        (@task -- Variable.set() to demonstrate programmatic updates)\n  |\nfetch_weather           (@task -- fetch Nordic weather via Open-Meteo)\n  |\nsave_result             (@task -- write JSON output to target/variable_schedule/)\n</code></pre>"},{"location":"integrations/#quick-reference","title":"Quick Reference","text":"DAG Key Concept 107 Variable-sourced schedule, changeable via REST API or UI without code edits"},{"location":"operators/","title":"Core Operators","text":""},{"location":"operators/#operator-reference","title":"Operator Reference","text":""},{"location":"operators/#core-operators_1","title":"Core Operators","text":"<p>Operators are the building blocks of every DAG. Each one knows how to do one thing -- run a shell command, call a Python function, send an email. You configure them with parameters and Airflow handles the rest (scheduling, retries, logging).</p> <p>All of these come from <code>airflow.providers.standard</code>:</p>"},{"location":"operators/#bashoperator","title":"BashOperator","text":"<p>Runs a command in the terminal (just like typing it in a shell). The command can be anything: a script, a one-liner, even <code>python my_script.py</code>:</p> <pre><code>from airflow.providers.standard.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id=\"run_script\",\n    bash_command=\"python /opt/scripts/etl.py --date {{ ds }}\",\n    env={\"API_KEY\": \"{{ var.value.api_key }}\"},\n)\n</code></pre>"},{"location":"operators/#pythonoperator","title":"PythonOperator","text":"<p>Calls a regular Python function. If you know Python, this will feel the most natural. (The <code>@task</code> decorator is the modern shorthand for this -- more on that later.)</p> <pre><code>from airflow.providers.standard.operators.python import PythonOperator\n\ndef my_function(name, **context):\n    print(f\"Hello {name}, running for {context['ds']}\")\n\ntask = PythonOperator(\n    task_id=\"python_task\",\n    python_callable=my_function,\n    op_args=[\"world\"],\n)\n</code></pre>"},{"location":"operators/#shortcircuitoperator","title":"ShortCircuitOperator","text":"<p>Skips all downstream tasks if the callable returns <code>False</code>:</p> <pre><code>from airflow.providers.standard.operators.python import ShortCircuitOperator\n\ndef check_data_exists():\n    return os.path.exists(\"/data/input.csv\")\n\ncheck = ShortCircuitOperator(\n    task_id=\"check\",\n    python_callable=check_data_exists,\n)\n\ncheck &gt;&gt; process &gt;&gt; load  # process and load are skipped if check returns False\n</code></pre>"},{"location":"operators/#docker-operators","title":"Docker Operators","text":"<p>Remember the Docker explanation from earlier? Docker operators let you run a task inside a container -- a throwaway mini-computer with exactly the packages you need. This is useful when your task needs libraries that conflict with Airflow's own dependencies, or when you want perfect reproducibility.</p> <p>You do not need Docker operators to use Airflow. They are covered in DAGs 21-32 and are an intermediate topic. Feel free to skip this section on your first read.</p>"},{"location":"operators/#dockeroperator","title":"DockerOperator","text":"<p>Run a command inside a Docker container:</p> <pre><code>from airflow.providers.docker.operators.docker import DockerOperator\n\n# Run a Python script inside a container\nprocess_data = DockerOperator(\n    task_id=\"process_data\",\n    image=\"python:3.12-slim\",\n    command=\"python /scripts/process.py\",\n    mounts=[\n        Mount(source=\"/host/data\", target=\"/data\", type=\"bind\"),\n        Mount(source=\"/host/scripts\", target=\"/scripts\", type=\"bind\"),\n    ],\n    auto_remove=\"success\",\n    docker_url=\"unix://var/run/docker.sock\",\n)\n\n# Run a custom image with environment variables\ntrain_model = DockerOperator(\n    task_id=\"train_model\",\n    image=\"my-ml-pipeline:latest\",\n    command=\"python train.py --epochs 50\",\n    environment={\n        \"DATA_PATH\": \"/data/training\",\n        \"MODEL_OUTPUT\": \"/models/latest\",\n    },\n    mounts=[\n        Mount(source=\"/host/data\", target=\"/data\", type=\"bind\"),\n        Mount(source=\"/host/models\", target=\"/models\", type=\"bind\"),\n    ],\n    auto_remove=\"success\",\n)\n</code></pre> <p>Key parameters:</p> Parameter Description <code>image</code> Docker image to use <code>command</code> Command to run inside the container <code>environment</code> Environment variables passed to the container <code>mounts</code> Volume mounts (bind mounts, named volumes) <code>auto_remove</code> Remove container after completion (<code>\"success\"</code>, <code>\"force\"</code>, <code>\"never\"</code>) <code>docker_url</code> Docker daemon URL (<code>unix://var/run/docker.sock</code> for local) <code>network_mode</code> Docker network mode (<code>\"bridge\"</code>, <code>\"host\"</code>, custom network name) <code>cpus</code> CPU limit for the container <code>mem_limit</code> Memory limit (e.g., <code>\"512m\"</code>, <code>\"2g\"</code>)"},{"location":"operators/#complete-docker-pipeline-example","title":"Complete Docker Pipeline Example","text":"<pre><code>from datetime import datetime\nfrom airflow.sdk import DAG\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom docker.types import Mount\n\nwith DAG(\n    dag_id=\"docker_pipeline\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False,\n) as dag:\n    # Step 1: Download data using a curl container\n    download = DockerOperator(\n        task_id=\"download\",\n        image=\"curlimages/curl:latest\",\n        command=\"curl -o /data/raw.csv https://example.com/data.csv\",\n        mounts=[Mount(source=\"/tmp/pipeline\", target=\"/data\", type=\"bind\")],\n        auto_remove=\"success\",\n    )\n\n    # Step 2: Process data using a Python container\n    process = DockerOperator(\n        task_id=\"process\",\n        image=\"python:3.12-slim\",\n        command='pip install pandas &amp;&amp; python -c \"import pandas as pd; df = pd.read_csv(\\'/data/raw.csv\\')\"',\n        mounts=[Mount(source=\"/tmp/pipeline\", target=\"/data\", type=\"bind\")],\n        auto_remove=\"success\",\n    )\n\n    # Step 3: Generate report using a custom image\n    report = DockerOperator(\n        task_id=\"report\",\n        image=\"my-reporting:latest\",\n        command=\"python generate_report.py\",\n        mounts=[Mount(source=\"/tmp/pipeline\", target=\"/data\", type=\"bind\")],\n        environment={\"REPORT_FORMAT\": \"html\"},\n        auto_remove=\"success\",\n    )\n\n    download &gt;&gt; process &gt;&gt; report\n</code></pre>"},{"location":"operators/#sensors","title":"Sensors","text":"<p>A sensor is a task that waits instead of doing. \"Wait until this file exists\", \"wait until this API is reachable\", \"wait 10 minutes.\" Once the condition is met, the next task runs:</p> <pre><code>from airflow.providers.standard.sensors.filesystem import FileSensor\nfrom airflow.providers.standard.sensors.time_delta import TimeDeltaSensor\nfrom datetime import timedelta\n\n# Wait for a file to appear\nwait_for_file = FileSensor(\n    task_id=\"wait_for_file\",\n    filepath=\"/data/input.csv\",\n    poke_interval=30,       # Check every 30 seconds\n    timeout=3600,           # Give up after 1 hour\n    mode=\"reschedule\",      # Free worker slot between pokes\n)\n\n# Wait for a time offset from the logical date\nwait_10min = TimeDeltaSensor(\n    task_id=\"wait_10min\",\n    delta=timedelta(minutes=10),\n    mode=\"poke\",            # Keep worker slot (faster, uses resources)\n)\n</code></pre> <p>Poke vs Reschedule mode:</p> Mode Behavior Best For <code>poke</code> Holds worker slot, checks periodically Short waits, frequent checks <code>reschedule</code> Releases worker, re-schedules check Long waits, resource-conscious"},{"location":"operators/#cross-dag-operators","title":"Cross-DAG Operators","text":"<pre><code>from airflow.providers.standard.operators.trigger_dagrun import TriggerDagRunOperator\n\n# Trigger another DAG\ntrigger = TriggerDagRunOperator(\n    task_id=\"trigger_downstream\",\n    trigger_dag_id=\"downstream_dag\",\n    wait_for_completion=True,\n    reset_dag_run=True,\n)\n</code></pre>"},{"location":"operators/#bashoperator-deep-dive","title":"BashOperator Deep Dive","text":"<p><code>BashOperator</code> runs a terminal command as an Airflow task. If you have ever typed a command in a terminal (like <code>ls</code>, <code>python script.py</code>, or <code>curl https://api.example.com</code>), you already know enough to use it. This section covers the details for when you want to do more than simple one-liners.</p>"},{"location":"operators/#import","title":"Import","text":"<pre><code>from airflow.providers.standard.operators.bash import BashOperator\n</code></pre>"},{"location":"operators/#basic-usage","title":"Basic Usage","text":"<pre><code># Single command\nhello = BashOperator(\n    task_id=\"hello\",\n    bash_command='echo \"Hello from Airflow\"',\n)\n\n# Multi-line script\nscript = BashOperator(\n    task_id=\"script\",\n    bash_command=\"\"\"\n        echo \"Step 1: check system\"\n        date '+%Y-%m-%d %H:%M:%S'\n        hostname\n        echo \"Step 2: done\"\n    \"\"\",\n)\n</code></pre>"},{"location":"operators/#key-parameters","title":"Key Parameters","text":"Parameter Type Description <code>bash_command</code> <code>str</code> Shell command or script to execute. Supports Jinja templates <code>env</code> <code>dict</code> Environment variables passed to the subprocess <code>append_env</code> <code>bool</code> If <code>True</code>, merge <code>env</code> into existing env instead of replacing it <code>cwd</code> <code>str</code> Working directory for the command <code>skip_on_exit_code</code> <code>int/list</code> Mark task as SKIPPED (not FAILED) for these exit codes <code>do_xcom_push</code> <code>bool</code> Push last stdout line to XCom (default: <code>True</code>) <code>output_encoding</code> <code>str</code> Encoding for stdout/stderr (default: <code>utf-8</code>)"},{"location":"operators/#working-directory","title":"Working Directory","text":"<pre><code># Run command in a specific directory\nprocess_data = BashOperator(\n    task_id=\"process_data\",\n    bash_command='echo \"Working in: $(pwd)\" &amp;&amp; ls -la',\n    cwd=\"/tmp/data\",\n)\n</code></pre>"},{"location":"operators/#exit-codes-and-skip-logic","title":"Exit Codes and Skip Logic","text":"<p>BashOperator fails the task on non-zero exit codes. Use <code>skip_on_exit_code</code> for conditional execution:</p> <pre><code># Skip task (don't fail) when data file doesn't exist\ncheck_data = BashOperator(\n    task_id=\"check_data\",\n    bash_command=\"\"\"\n        if [ ! -f /tmp/data.csv ]; then\n            echo \"No data file found -- skipping\"\n            exit 99\n        fi\n        echo \"Data file found\"\n    \"\"\",\n    skip_on_exit_code=99,\n)\n</code></pre> <p>See: <code>dags/43_bash_basics.py</code></p>"},{"location":"operators/#environment-variables","title":"Environment Variables","text":"<p>Use the <code>env</code> parameter to pass variables. With <code>append_env=True</code>, custom variables are merged into the existing environment (preserving PATH, HOME, etc.):</p> <pre><code>process = BashOperator(\n    task_id=\"process\",\n    bash_command=\"\"\"\n        echo \"Stage: $PIPELINE_STAGE\"\n        echo \"Date:  $EXEC_DATE\"\n        echo \"PATH still works: $(which python)\"\n    \"\"\",\n    env={\n        \"PIPELINE_STAGE\": \"transform\",\n        \"EXEC_DATE\": \"{{ ds }}\",         # Jinja template in env value\n    },\n    append_env=True,\n)\n</code></pre> <p>Without <code>append_env=True</code>, the <code>env</code> dict replaces the entire environment -- PATH, HOME, USER, etc. are all lost. Always use <code>append_env=True</code> unless you need a completely clean environment.</p> <p>See: <code>dags/44_bash_environment.py</code></p>"},{"location":"operators/#xcom-output-capture","title":"XCom Output Capture","text":"<p>By default (<code>do_xcom_push=True</code>), BashOperator pushes the last line of stdout to XCom. Pull it in downstream tasks:</p> <pre><code># Task 1: produce output (last line is captured)\nproduce = BashOperator(\n    task_id=\"produce\",\n    bash_command=\"\"\"\n        echo \"Processing...\"\n        echo \"42\"\n    \"\"\",\n)\n\n# Task 2: consume via Jinja template\nconsume = BashOperator(\n    task_id=\"consume\",\n    bash_command='echo \"Got: {{ ti.xcom_pull(task_ids=\\'produce\\') }}\"',\n)\n\nproduce &gt;&gt; consume\n</code></pre>"},{"location":"operators/#advanced-scripting-patterns","title":"Advanced Scripting Patterns","text":"<p>BashOperator supports full bash scripting -- functions, loops, arrays, error handling:</p> <pre><code># Strict mode with functions\npipeline = BashOperator(\n    task_id=\"pipeline\",\n    bash_command=\"\"\"\n        set -euo pipefail\n\n        log() {\n            echo \"[$(date '+%H:%M:%S')] $*\"\n        }\n\n        WORKDIR=$(mktemp -d)\n        trap 'rm -rf \"$WORKDIR\"' EXIT\n\n        log \"Generating data\"\n        for i in $(seq 1 10); do\n            echo \"$i,$((RANDOM % 30))\" &gt;&gt; \"$WORKDIR/data.csv\"\n        done\n\n        log \"Processing $(wc -l &lt; \"$WORKDIR/data.csv\") records\"\n        log \"Done\"\n    \"\"\",\n)\n</code></pre>"},{"location":"operators/#error-handling","title":"Error Handling","text":"Pattern Purpose <code>set -e</code> Exit on first error <code>set -u</code> Error on undefined variables <code>set -o pipefail</code> Catch failures in pipes <code>trap 'cleanup' EXIT</code> Run cleanup on exit <code>cmd \\|\\| true</code> Ignore command failure <code>cmd \\|\\| exit 99</code> Fail with specific exit code"},{"location":"operators/#file-io","title":"File I/O","text":"<pre><code>file_task = BashOperator(\n    task_id=\"file_task\",\n    bash_command=\"\"\"\n        WORKDIR=$(mktemp -d)\n\n        # Write CSV data\n        cat &gt; \"$WORKDIR/data.csv\" &lt;&lt; 'EOF'\n        station,temp_c,humidity\n        oslo_01,12.5,68.0\n        bergen_01,9.8,82.0\nEOF\n\n        # Process\n        tail -n +2 \"$WORKDIR/data.csv\" | while IFS=',' read -r station temp hum; do\n            echo \"Station: $station, Temp: $temp\"\n        done\n\n        rm -rf \"$WORKDIR\"\n    \"\"\",\n)\n</code></pre> <p>See: <code>dags/45_bash_scripting.py</code></p>"},{"location":"operators/#jinja-templates-in-bashoperator","title":"Jinja Templates in BashOperator","text":"<p><code>bash_command</code> is a Jinja-templated field. You can use all Airflow macros, params, and even Jinja control structures:</p> <pre><code># Date arithmetic\ndates = BashOperator(\n    task_id=\"dates\",\n    bash_command=\"\"\"\n        echo \"Today:     {{ ds }}\"\n        echo \"Yesterday: {{ macros.ds_add(ds, -1) }}\"\n        echo \"Year:      {{ logical_date.strftime('%Y') }}\"\n    \"\"\",\n)\n\n# Dynamic paths from params\npaths = BashOperator(\n    task_id=\"paths\",\n    bash_command=\"\"\"\n        INPATH=\"/data/raw/{{ ds_nodash }}/{{ params.station }}\"\n        OUTPATH=\"/data/processed/{{ logical_date.strftime('%Y/%m') }}\"\n        echo \"Input:  $INPATH\"\n        echo \"Output: $OUTPATH\"\n    \"\"\",\n)\n\n# Jinja for-loop generates bash at render time\ngenerate = BashOperator(\n    task_id=\"generate\",\n    bash_command=\"\"\"\n        {% for city in [\"oslo\", \"bergen\", \"tromso\"] %}\n        echo \"Processing: {{ city }}\"\n        {% endfor %}\n    \"\"\",\n)\n</code></pre>"},{"location":"operators/#common-template-variables","title":"Common Template Variables","text":"Variable Example Description <code>{{ ds }}</code> <code>2024-01-01</code> Execution date (YYYY-MM-DD) <code>{{ ds_nodash }}</code> <code>20240101</code> Execution date without dashes <code>{{ logical_date }}</code> datetime object Full datetime object <code>{{ run_id }}</code> <code>manual__2024...</code> DAG run identifier <code>{{ dag.dag_id }}</code> <code>46_bash_templating</code> DAG ID <code>{{ task.task_id }}</code> <code>core_macros</code> Task ID <code>{{ params.key }}</code> user-defined DAG parameter value <code>{{ macros.ds_add(ds, -1) }}</code> <code>2023-12-31</code> Date arithmetic <code>{{ ti.xcom_pull(...) }}</code> any Pull XCom from another task <p>See: <code>dags/46_bash_templating.py</code></p>"},{"location":"operators/#taskflow-decorator-variants","title":"TaskFlow Decorator Variants","text":"<p>Beyond the basic <code>@task</code> decorator, Airflow 3.x provides specialized decorators that combine the convenience of TaskFlow with specific operator behavior.</p>"},{"location":"operators/#taskbash","title":"@task.bash","text":"<p>Runs a bash command returned by a Python function. The function returns a string that Airflow executes as a shell command:</p> <pre><code>from airflow.sdk import task\n\n@task.bash\ndef get_system_info() -&gt; str:\n    \"\"\"Return a bash command string to execute.\"\"\"\n    return 'echo \"Hostname: $(hostname)\" &amp;&amp; date'\n\n@task.bash\ndef create_report() -&gt; str:\n    \"\"\"Dynamically construct a bash command.\"\"\"\n    output_dir = \"/tmp/reports\"\n    return f'mkdir -p {output_dir} &amp;&amp; echo \"Report generated\" &gt; {output_dir}/status.txt'\n</code></pre>"},{"location":"operators/#taskshort_circuit","title":"@task.short_circuit","text":"<p>Returns <code>True</code> or <code>False</code>. If <code>False</code>, all downstream tasks are skipped (not failed):</p> <pre><code>@task.short_circuit\ndef check_data_available() -&gt; bool:\n    \"\"\"Skip downstream if no data is ready.\"\"\"\n    data_exists = True  # Check your data source\n    return data_exists\n\n# Downstream tasks only run if check returns True\ncheck = check_data_available()\nprocess = process_data()\ncheck &gt;&gt; process\n</code></pre>"},{"location":"operators/#taskvirtualenv","title":"@task.virtualenv","text":"<p>Runs a task in an isolated Python virtual environment with specified packages. The function must be self-contained (no imports from the DAG file):</p> <pre><code>@task.virtualenv(requirements=[\"requests&gt;=2.31\"])\ndef fetch_api_data() -&gt; str:\n    \"\"\"Fetch data using requests (installed in a temporary virtualenv).\"\"\"\n    import requests\n    resp = requests.get(\"https://api.example.com/data\", timeout=10)\n    return f\"status={resp.status_code}\"\n</code></pre> <p>The classic operator form:</p> <pre><code>from airflow.providers.standard.operators.python import PythonVirtualenvOperator\n\ndef compute_stats() -&gt; str:\n    import numpy as np\n    data = np.array([12.5, 13.1, 9.8])\n    return f\"mean={np.mean(data):.2f}\"\n\nstats = PythonVirtualenvOperator(\n    task_id=\"compute_stats\",\n    python_callable=compute_stats,\n    requirements=[\"numpy&gt;=1.26\"],\n)\n</code></pre> <p>See: <code>dags/47_taskflow_decorators.py</code>, <code>dags/48_virtualenv_tasks.py</code></p>"},{"location":"operators/#emptyoperator-and-branchpythonoperator","title":"EmptyOperator and BranchPythonOperator","text":""},{"location":"operators/#emptyoperator","title":"EmptyOperator","text":"<p><code>EmptyOperator</code> (the Airflow 3.x replacement for <code>DummyOperator</code>) is a no-op task used as a structural element: join points after branching, explicit start/end markers, or fork points.</p> <pre><code>from airflow.providers.standard.operators.empty import EmptyOperator\n\nstart = EmptyOperator(task_id=\"start\")\njoin = EmptyOperator(\n    task_id=\"join\",\n    trigger_rule=\"none_failed_min_one_success\",\n)\nend = EmptyOperator(task_id=\"end\")\n\nstart &gt;&gt; [branch_a, branch_b] &gt;&gt; join &gt;&gt; end\n</code></pre>"},{"location":"operators/#branchpythonoperator-classic","title":"BranchPythonOperator (Classic)","text":"<p>The operator-style alternative to <code>@task.branch</code>. The callable returns one or more task IDs to execute:</p> <pre><code>from airflow.providers.standard.operators.python import BranchPythonOperator\n\ndef choose_path(**context) -&gt; str:\n    day = context[\"logical_date\"].day\n    if day &lt;= 15:\n        return \"first_half\"\n    return \"second_half\"\n\nbranch = BranchPythonOperator(\n    task_id=\"branch\",\n    python_callable=choose_path,\n)\n\n# Return a list to execute multiple branches simultaneously:\ndef choose_validations(**context) -&gt; list[str]:\n    return [\"validate_schema\", \"validate_values\"]\n</code></pre> <p>See: <code>dags/49_empty_and_branch_operators.py</code></p>"},{"location":"operators/#filesensor","title":"FileSensor","text":"<p><code>FileSensor</code> waits for a file to appear at a specified path before proceeding:</p> <pre><code>from airflow.providers.standard.sensors.filesystem import FileSensor\n\nwait = FileSensor(\n    task_id=\"wait_for_data\",\n    filepath=\"/data/incoming/weather.csv\",\n    poke_interval=10,       # Check every 10 seconds\n    timeout=300,            # Give up after 5 minutes\n    mode=\"poke\",            # Or \"reschedule\" to free the worker slot\n)\n</code></pre> Parameter Description <code>filepath</code> Path to the file to detect <code>poke_interval</code> Seconds between checks <code>timeout</code> Maximum seconds to wait before failing <code>mode</code> <code>\"poke\"</code> (holds worker) or <code>\"reschedule\"</code> (frees worker between checks) <p>See: <code>dags/50_file_sensor.py</code></p>"},{"location":"pipelines/","title":"Pipelines","text":""},{"location":"pipelines/#python-pipelines","title":"Python Pipelines","text":"<p>This is probably the section most relevant to you. The <code>@task</code> decorator lets you write plain Python functions -- no operator classes, no boilerplate. Airflow automatically tracks dependencies, passes return values between tasks, and handles retries.</p>"},{"location":"pipelines/#simple-etl-pipeline","title":"Simple ETL Pipeline","text":"<pre><code>from datetime import datetime\nfrom airflow.sdk import DAG, task\n\n@task\ndef extract() -&gt; list[dict]:\n    \"\"\"Pull data from source system.\"\"\"\n    return [\n        {\"id\": 1, \"name\": \"Alice\", \"score\": 85},\n        {\"id\": 2, \"name\": \"Bob\", \"score\": 92},\n    ]\n\n@task\ndef transform(records: list[dict]) -&gt; list[dict]:\n    \"\"\"Normalize scores to 0-1 range.\"\"\"\n    return [\n        {**r, \"score_normalized\": r[\"score\"] / 100}\n        for r in records\n    ]\n\n@task\ndef load(records: list[dict]) -&gt; None:\n    \"\"\"Write processed records to destination.\"\"\"\n    for r in records:\n        print(f\"Loading: {r}\")\n\nwith DAG(\n    dag_id=\"python_etl\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False,\n) as dag:\n    raw = extract()\n    cleaned = transform(raw)\n    load(cleaned)\n</code></pre>"},{"location":"pipelines/#multi-source-pipeline-with-dynamic-tasks","title":"Multi-Source Pipeline with Dynamic Tasks","text":"<pre><code>@task\ndef get_sources() -&gt; list[str]:\n    return [\"postgres\", \"mysql\", \"mongodb\"]\n\n@task\ndef extract_from(source: str) -&gt; dict:\n    print(f\"Extracting from {source}\")\n    return {\"source\": source, \"rows\": 1000}\n\n@task\ndef merge(datasets: list[dict]) -&gt; dict:\n    total = sum(d[\"rows\"] for d in datasets)\n    return {\"total_rows\": total, \"sources\": len(datasets)}\n\nwith DAG(dag_id=\"multi_source\", ...):\n    sources = get_sources()\n    extracted = extract_from.expand(source=sources)  # Dynamic!\n    merge(extracted)\n</code></pre>"},{"location":"pipelines/#pipeline-with-error-handling","title":"Pipeline with Error Handling","text":"<pre><code>@task\ndef extract_with_retry() -&gt; dict:\n    \"\"\"Extract with built-in retry logic.\"\"\"\n    import requests\n    resp = requests.get(\"https://api.example.com/data\", timeout=30)\n    resp.raise_for_status()\n    return resp.json()\n\n@task(retries=3, retry_delay=timedelta(minutes=1))\ndef transform_with_validation(data: dict) -&gt; dict:\n    \"\"\"Transform with validation.\"\"\"\n    if not data.get(\"records\"):\n        raise ValueError(\"No records in response\")\n    return {\"processed\": len(data[\"records\"])}\n</code></pre>"},{"location":"pipelines/#docker-pipelines","title":"Docker Pipelines","text":"<p>This is an intermediate topic. If you are new to Airflow, you can skip this section entirely and come back later.</p> <p>Docker pipelines run each task in its own container -- a mini-computer with exactly the packages that task needs. This solves the \"works on my machine\" problem and avoids dependency conflicts (e.g., when task A needs pandas 1.x and task B needs pandas 2.x).</p>"},{"location":"pipelines/#why-docker-pipelines","title":"Why Docker Pipelines?","text":"Benefit Description Isolation Each task has its own Python version, packages, and system libraries Reproducibility Same container image runs identically everywhere No dependency conflicts Task A can use pandas 1.x while Task B uses pandas 2.x Language agnostic Run R, Julia, Rust, or any language in containers Production parity Same images used in dev, staging, and production"},{"location":"pipelines/#pattern-1-custom-image-per-step","title":"Pattern 1: Custom Image per Step","text":"<p>Build specialized images for each pipeline stage:</p> <pre><code># Dockerfile.extract\nFROM python:3.12-slim\nRUN pip install requests boto3\nCOPY scripts/extract.py /app/\nWORKDIR /app\nENTRYPOINT [\"python\", \"extract.py\"]\n</code></pre> <pre><code>from airflow.providers.docker.operators.docker import DockerOperator\nfrom docker.types import Mount\n\nwith DAG(dag_id=\"docker_etl\", ...):\n    extract = DockerOperator(\n        task_id=\"extract\",\n        image=\"my-pipeline/extract:latest\",\n        command=\"--source s3://bucket/raw --date {{ ds }}\",\n        mounts=[Mount(source=\"/data/shared\", target=\"/data\", type=\"bind\")],\n        auto_remove=\"success\",\n    )\n\n    transform = DockerOperator(\n        task_id=\"transform\",\n        image=\"my-pipeline/transform:latest\",\n        command=\"--input /data/raw.parquet --output /data/clean.parquet\",\n        mounts=[Mount(source=\"/data/shared\", target=\"/data\", type=\"bind\")],\n        auto_remove=\"success\",\n    )\n\n    load = DockerOperator(\n        task_id=\"load\",\n        image=\"my-pipeline/load:latest\",\n        command=\"--input /data/clean.parquet --table warehouse.clean_data\",\n        mounts=[Mount(source=\"/data/shared\", target=\"/data\", type=\"bind\")],\n        auto_remove=\"success\",\n    )\n\n    extract &gt;&gt; transform &gt;&gt; load\n</code></pre>"},{"location":"pipelines/#pattern-2-shared-volume-communication","title":"Pattern 2: Shared Volume Communication","text":"<p>Tasks pass data through shared mounted volumes:</p> <pre><code>with DAG(dag_id=\"docker_shared_volume\", ...):\n    # All tasks mount the same host directory\n    SHARED = [Mount(source=\"/tmp/pipeline/{{ ds }}\", target=\"/data\", type=\"bind\")]\n\n    step1 = DockerOperator(\n        task_id=\"download\",\n        image=\"curlimages/curl\",\n        command=\"curl -o /data/input.json https://api.example.com/data\",\n        mounts=SHARED,\n        auto_remove=\"success\",\n    )\n\n    step2 = DockerOperator(\n        task_id=\"process\",\n        image=\"python:3.12\",\n        command='python -c \"import json; data=json.load(open(\\'/data/input.json\\')); ...\"',\n        mounts=SHARED,\n        auto_remove=\"success\",\n    )\n\n    step1 &gt;&gt; step2\n</code></pre>"},{"location":"pipelines/#pattern-3-docker-compose-for-multi-container-tasks","title":"Pattern 3: Docker Compose for Multi-Container Tasks","text":"<p>For tasks requiring multiple services (e.g., a web app + database):</p> <pre><code># Use BashOperator to run docker compose for complex setups\nintegration_test = BashOperator(\n    task_id=\"integration_test\",\n    bash_command=(\n        \"cd /opt/project &amp;&amp; \"\n        \"docker compose -f docker-compose.test.yml up --abort-on-container-exit --exit-code-from test\"\n    ),\n)\n</code></pre>"},{"location":"pipelines/#pattern-4-gpu-workloads","title":"Pattern 4: GPU Workloads","text":"<pre><code>train = DockerOperator(\n    task_id=\"train_model\",\n    image=\"nvidia/cuda:12.0-runtime\",\n    command=\"python train.py --epochs 100 --batch-size 32\",\n    device_requests=[DeviceRequest(count=-1, capabilities=[[\"gpu\"]])],\n    mem_limit=\"16g\",\n    auto_remove=\"success\",\n)\n</code></pre>"},{"location":"providers/","title":"Provider Operators","text":""},{"location":"providers/#http-operators","title":"HTTP Operators","text":"<p>HTTP operators let Airflow make HTTP requests and poll endpoints. Useful for triggering external APIs, checking service health, and downloading data.</p>"},{"location":"providers/#httpoperator","title":"HttpOperator","text":"<p>Makes HTTP requests (GET, POST, PUT, DELETE) to any HTTP endpoint:</p> <pre><code>from airflow.providers.http.operators.http import HttpOperator\n\n# GET request\nget = HttpOperator(\n    task_id=\"get_data\",\n    http_conn_id=\"http_default\",\n    method=\"GET\",\n    endpoint=\"/api/data\",\n    log_response=True,\n)\n\n# POST with JSON body\npost = HttpOperator(\n    task_id=\"post_data\",\n    http_conn_id=\"http_default\",\n    method=\"POST\",\n    endpoint=\"/api/data\",\n    data='{\"key\": \"value\"}',\n    headers={\"Content-Type\": \"application/json\"},\n    log_response=True,\n)\n</code></pre> <p>Key parameters:</p> Parameter Description <code>http_conn_id</code> Airflow Connection ID with host/port/auth <code>method</code> HTTP method (<code>GET</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>) <code>endpoint</code> URL path appended to the connection's host <code>data</code> Request body (string or dict) <code>headers</code> HTTP headers dict <code>log_response</code> Log the response body <code>response_check</code> Callable to validate response <code>response_filter</code> Callable to extract/transform response for XCom <p>See: <code>dags/33_http_requests.py</code></p>"},{"location":"providers/#httpsensor","title":"HttpSensor","text":"<p>Polls an HTTP endpoint until a condition is met. Supports both poke and deferrable modes:</p> <pre><code>from airflow.providers.http.sensors.http import HttpSensor\n\n# Poke mode: holds worker slot\nwait_poke = HttpSensor(\n    task_id=\"wait_poke\",\n    http_conn_id=\"http_default\",\n    endpoint=\"/health\",\n    response_check=lambda response: response.status_code == 200,\n    poke_interval=30,\n    timeout=600,\n    mode=\"poke\",\n)\n\n# Deferrable mode: releases worker slot (recommended for long waits)\nwait_defer = HttpSensor(\n    task_id=\"wait_defer\",\n    http_conn_id=\"http_default\",\n    endpoint=\"/health\",\n    response_check=lambda response: response.status_code == 200,\n    poke_interval=30,\n    timeout=600,\n    deferrable=True,\n)\n</code></pre> <p>Poke vs Deferrable:</p> Mode Worker Slot Triggerer Best For <code>mode=\"poke\"</code> Held during wait Not used Short waits (&lt;5 min) <code>mode=\"reschedule\"</code> Released between pokes Not used Medium waits <code>deferrable=True</code> Released immediately Handles async wait Long waits, many sensors <p>See: <code>dags/34_http_sensor.py</code></p>"},{"location":"providers/#sql-operators","title":"SQL Operators","text":"<p>If your workflow needs to talk to a database (PostgreSQL, MySQL, SQLite), SQL operators let you run queries as Airflow tasks. You do not need to know SQL to use the rest of this project -- these are covered in DAGs 35-37 and you can skip them if SQL is not your thing.</p>"},{"location":"providers/#sqlexecutequeryoperator","title":"SQLExecuteQueryOperator","text":"<p>The universal SQL operator. Works with any database that has an Airflow provider:</p> <pre><code>from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator\n\n# DDL: Create a table\ncreate = SQLExecuteQueryOperator(\n    task_id=\"create_table\",\n    conn_id=\"postgres_default\",\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS measurements (\n            id SERIAL PRIMARY KEY,\n            station VARCHAR(50),\n            temp_c REAL,\n            recorded_at TIMESTAMP DEFAULT NOW()\n        );\n    \"\"\",\n)\n\n# DML: Insert data\ninsert = SQLExecuteQueryOperator(\n    task_id=\"insert_data\",\n    conn_id=\"postgres_default\",\n    sql=\"\"\"\n        INSERT INTO measurements (station, temp_c) VALUES\n        ('oslo_01', 12.5),\n        ('bergen_01', 9.8);\n    \"\"\",\n)\n\n# Query with Jinja templates\nquery = SQLExecuteQueryOperator(\n    task_id=\"query_data\",\n    conn_id=\"postgres_default\",\n    sql=\"SELECT * FROM measurements WHERE recorded_at &gt;= '{{ ds }}';\",\n)\n</code></pre> <p>Key parameters:</p> Parameter Description <code>conn_id</code> Airflow Connection ID for the database <code>sql</code> SQL query string or path to <code>.sql</code> file <code>parameters</code> Query parameters (prevents SQL injection) <code>autocommit</code> Auto-commit after each statement <code>handler</code> Custom result handler function <p>Airflow 3.x migration note: <code>PostgresOperator</code>, <code>MySqlOperator</code>, and other provider-specific SQL operators are deprecated. Use <code>SQLExecuteQueryOperator</code> with the appropriate <code>conn_id</code> instead.</p> <p>See: <code>dags/35_postgres_queries.py</code></p>"},{"location":"providers/#sql-etl-pipeline","title":"SQL ETL Pipeline","text":"<p>A complete SQL-based ETL pipeline with staging, transformation, and summary tables:</p> <pre><code>[setup: create tables] -&gt; [extract: load staging] -&gt; [transform: clean + validate] -&gt; [summarize: aggregates] -&gt; [teardown: drop tables]\n</code></pre> <p>Each stage uses TaskGroups for organization. The transform step uses SQL CASE expressions to validate data (reject non-numeric values, out-of-range readings) and cast types:</p> <pre><code>INSERT INTO prod_measurements (sensor_id, value_numeric, unit, is_valid, measured_at)\nSELECT\n    sensor_id,\n    CASE WHEN raw_value ~ '^-?[0-9.]+$' THEN raw_value::REAL ELSE NULL END,\n    unit,\n    CASE\n        WHEN raw_value ~ '^-?[0-9.]+$' AND raw_value::REAL BETWEEN -50 AND 60\n        THEN TRUE ELSE FALSE\n    END,\n    ts\nFROM stg_sensors;\n</code></pre> <p>See: <code>dags/36_sql_pipeline.py</code></p>"},{"location":"providers/#generic-sql-transfer","title":"Generic SQL Transfer","text":"<p>Transfer data between tables with SQL-based transformation. A common pattern for database-to-database ETL where the source and destination have different schemas:</p> <pre><code>INSERT INTO transfer_dest (city, country, population, size_category)\nSELECT\n    city, country, population,\n    CASE\n        WHEN population &gt;= 900000 THEN 'large'\n        WHEN population &gt;= 500000 THEN 'medium'\n        ELSE 'small'\n    END\nFROM transfer_source\nORDER BY population DESC;\n</code></pre> <p>See: <code>dags/37_generic_transfer.py</code></p>"},{"location":"providers/#email-operators","title":"Email Operators","text":"<p>Email operators send notifications from DAG tasks. This project uses Mailpit as a local SMTP server -- all captured emails are viewable at http://localhost:8025.</p>"},{"location":"providers/#emailoperator","title":"EmailOperator","text":"<p>Send plain text or HTML emails with Jinja-templated content:</p> <pre><code>from airflow.providers.smtp.operators.smtp import EmailOperator\n\n# Plain email\nsend_plain = EmailOperator(\n    task_id=\"send_plain\",\n    conn_id=\"smtp_default\",\n    to=\"team@example.com\",\n    subject=\"Pipeline started ({{ ds }})\",\n    html_content=\"&lt;p&gt;The pipeline has started for {{ ds }}.&lt;/p&gt;\",\n    from_email=\"airflow@example.com\",\n)\n\n# Rich HTML email with tables\nsend_html = EmailOperator(\n    task_id=\"send_html\",\n    conn_id=\"smtp_default\",\n    to=\"team@example.com\",\n    subject=\"Pipeline report ({{ ds }})\",\n    html_content=\"\"\"\n    &lt;h2&gt;Pipeline Report&lt;/h2&gt;\n    &lt;table border=\"1\"&gt;\n        &lt;tr&gt;&lt;td&gt;DAG&lt;/td&gt;&lt;td&gt;{{ dag.dag_id }}&lt;/td&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Date&lt;/td&gt;&lt;td&gt;{{ ds }}&lt;/td&gt;&lt;/tr&gt;\n        &lt;tr&gt;&lt;td&gt;Run ID&lt;/td&gt;&lt;td&gt;{{ run_id }}&lt;/td&gt;&lt;/tr&gt;\n    &lt;/table&gt;\n    \"\"\",\n    from_email=\"airflow@example.com\",\n)\n</code></pre> <p>Key parameters:</p> Parameter Description <code>conn_id</code> SMTP Connection ID <code>to</code> Recipient email (string or list) <code>subject</code> Email subject (supports Jinja) <code>html_content</code> Email body as HTML (supports Jinja) <code>from_email</code> Sender email address <code>cc</code> CC recipients <code>bcc</code> BCC recipients <code>files</code> List of file paths to attach <p>Common patterns:</p> <ul> <li>Pipeline start/end notifications: Send at DAG start and end</li> <li>Failure alerts: Use <code>on_failure_callback</code> to send on task failure</li> <li>Summary reports: Build HTML tables with pipeline metrics</li> </ul> <p>See: <code>dags/38_email_notifications.py</code></p>"},{"location":"providers/#ssh-operators","title":"SSH Operators","text":"<p>SSH operators execute commands on remote hosts. This project uses a local OpenSSH server container for demonstrations.</p>"},{"location":"providers/#sshoperator","title":"SSHOperator","text":"<p>Execute commands on a remote host via SSH:</p> <pre><code>from airflow.providers.ssh.operators.ssh import SSHOperator\n\n# Simple command\nremote_uname = SSHOperator(\n    task_id=\"remote_uname\",\n    ssh_conn_id=\"ssh_default\",\n    command=\"uname -a\",\n    cmd_timeout=10,\n)\n\n# Multi-command script\nremote_script = SSHOperator(\n    task_id=\"remote_script\",\n    ssh_conn_id=\"ssh_default\",\n    command=(\n        \"echo 'User: '$(whoami) &amp;&amp; \"\n        \"echo 'PWD: '$(pwd) &amp;&amp; \"\n        \"echo 'Date: '$(date)\"\n    ),\n    cmd_timeout=10,\n)\n\n# File operations on remote host\nremote_file = SSHOperator(\n    task_id=\"remote_file\",\n    ssh_conn_id=\"ssh_default\",\n    command=(\n        \"echo 'Hello from Airflow' &gt; /tmp/test.txt &amp;&amp; \"\n        \"cat /tmp/test.txt &amp;&amp; \"\n        \"rm /tmp/test.txt\"\n    ),\n    cmd_timeout=10,\n)\n</code></pre> <p>Key parameters:</p> Parameter Description <code>ssh_conn_id</code> Airflow Connection ID with host/port/credentials <code>command</code> Shell command to execute remotely <code>cmd_timeout</code> Timeout in seconds for the command <code>environment</code> Environment variables to set on remote <code>get_pty</code> Request a pseudo-terminal (needed for sudo) <p>Connection setup:</p> <pre><code>airflow connections add ssh_default \\\n    --conn-type ssh \\\n    --conn-host remote-host \\\n    --conn-port 22 \\\n    --conn-login username \\\n    --conn-password password\n</code></pre> <p>See: <code>dags/39_ssh_commands.py</code></p>"},{"location":"providers/#latestonlyoperator","title":"LatestOnlyOperator","text":"<p>The <code>LatestOnlyOperator</code> skips all downstream tasks when the current DAG run is not the most recent. This prevents backfill runs from executing side-effect tasks like notifications, deployments, or cache refreshes.</p> <pre><code>from airflow.providers.standard.operators.latest_only import LatestOnlyOperator\n\nprocess_data = PythonOperator(task_id=\"process_data\", ...)\nlatest_check = LatestOnlyOperator(task_id=\"latest_check\")\nsend_notification = PythonOperator(task_id=\"send_notification\", ...)\nupdate_dashboard = BashOperator(task_id=\"update_dashboard\", ...)\n\n# process_data always runs; notification and dashboard only on latest run\nprocess_data &gt;&gt; latest_check &gt;&gt; [send_notification, update_dashboard]\n</code></pre> <p>When to use:</p> Scenario Use LatestOnlyOperator? Sending Slack/email notifications Yes -- don't spam during backfills Updating a dashboard or cache Yes -- only latest data matters Writing to a data warehouse No -- backfill data is needed Processing historical data No -- every run matters <p>See: <code>dags/40_latest_only.py</code></p>"},{"location":"providers/#deferrable-operators-and-triggers","title":"Deferrable Operators and Triggers","text":"<p>This is an advanced topic -- feel free to skip on your first read.</p> <p>Some tasks just wait: \"wait until the file appears\", \"wait until the API responds.\" Normally, a waiting task occupies a worker slot the whole time (like a phone call on hold). Deferrable operators are smarter -- they say \"call me back when the condition is met\" and free up the slot for other tasks. Airflow's triggerer process handles the waiting and wakes the task up when it is time.</p>"},{"location":"providers/#how-deferrable-execution-works","title":"How Deferrable Execution Works","text":"<pre><code>1. Task.execute() calls self.defer(trigger=..., method_name=\"execute_complete\")\n2. Worker slot is RELEASED\n3. Trigger runs in the triggerer process (async event loop)\n4. When trigger fires, task resumes via execute_complete()\n5. Worker slot is RE-ACQUIRED only for execute_complete()\n</code></pre> <p>Benefits:</p> <ul> <li>Thousands of waiting tasks with minimal worker slots</li> <li>Triggerer process handles all async waits efficiently</li> <li>No wasted resources during long sensor waits</li> </ul>"},{"location":"providers/#built-in-deferrable-sensors","title":"Built-in Deferrable Sensors","text":"<p>Many standard sensors support <code>deferrable=True</code> in Airflow 3.x:</p> <pre><code>from airflow.providers.standard.sensors.time_delta import TimeDeltaSensor\n\n# Traditional: holds worker slot\npoke_sensor = TimeDeltaSensor(\n    task_id=\"poke\",\n    delta=timedelta(minutes=5),\n    mode=\"poke\",\n)\n\n# Deferrable: releases worker slot\ndefer_sensor = TimeDeltaSensor(\n    task_id=\"defer\",\n    delta=timedelta(minutes=5),\n    deferrable=True,\n)\n</code></pre> <p>Sensors with deferrable support:</p> Sensor Parameter <code>TimeDeltaSensor</code> <code>deferrable=True</code> <code>TimeSensor</code> <code>deferrable=True</code> <code>HttpSensor</code> <code>deferrable=True</code> <code>FileSensor</code> <code>deferrable=True</code> <code>ExternalTaskSensor</code> <code>deferrable=True</code> <p>See: <code>dags/41_deferrable_sensors.py</code></p>"},{"location":"providers/#writing-a-custom-trigger","title":"Writing a Custom Trigger","text":"<p>A trigger runs in the triggerer's async event loop. It must implement <code>serialize()</code> for persistence and <code>run()</code> as an async generator:</p> <pre><code>import asyncio\nfrom airflow.triggers.base import BaseTrigger, TriggerEvent\n\nclass CountdownTrigger(BaseTrigger):\n    \"\"\"Waits for N seconds then fires.\"\"\"\n\n    def __init__(self, seconds: int, message: str) -&gt; None:\n        super().__init__()\n        self.seconds = seconds\n        self.message = message\n\n    def serialize(self) -&gt; tuple[str, dict]:\n        \"\"\"Serialize for storage (classpath + kwargs).\"\"\"\n        return (\n            \"airflow_examples.triggers.CountdownTrigger\",\n            {\"seconds\": self.seconds, \"message\": self.message},\n        )\n\n    async def run(self):\n        \"\"\"Async generator that yields a TriggerEvent.\"\"\"\n        await asyncio.sleep(self.seconds)\n        yield TriggerEvent({\"status\": \"complete\", \"message\": self.message})\n</code></pre>"},{"location":"providers/#writing-a-custom-deferrable-operator","title":"Writing a Custom Deferrable Operator","text":"<p>A deferrable operator has two execution phases:</p> <ol> <li><code>execute()</code>: Starts the task and defers to a trigger</li> <li><code>execute_complete()</code>: Resumes when the trigger fires</li> </ol> <pre><code>from airflow.sdk import BaseOperator\n\nclass CountdownOperator(BaseOperator):\n    def __init__(self, seconds: int = 2, message: str = \"default\", **kwargs):\n        super().__init__(**kwargs)\n        self.seconds = seconds\n        self.message = message\n\n    def execute(self, context):\n        \"\"\"Start the operator -- defer to triggerer.\"\"\"\n        self.defer(\n            trigger=CountdownTrigger(seconds=self.seconds, message=self.message),\n            method_name=\"execute_complete\",\n        )\n\n    def execute_complete(self, context, event):\n        \"\"\"Resume after trigger fires -- event contains the TriggerEvent payload.\"\"\"\n        print(f\"Trigger fired: {event['message']} ({event['elapsed_seconds']}s)\")\n        return f\"Complete: {event['message']}\"\n</code></pre> <p>Deferrable operator lifecycle:</p> <pre><code>execute() --[defer]--&gt; Triggerer (async) --[TriggerEvent]--&gt; execute_complete() --[return]--&gt; XCom\n    |                       |                                       |\n    v                       v                                       v\n Worker released      Runs in event loop                     Worker re-acquired\n</code></pre> <p>See: <code>dags/42_custom_deferrable.py</code> and <code>src/airflow_examples/triggers.py</code></p>"},{"location":"quality-testing/","title":"Quality &amp; Testing","text":""},{"location":"quality-testing/#data-quality-validation","title":"Data Quality &amp; Validation","text":"<p>DAGs 68--72 demonstrate systematic data quality checking: schema validation, statistical anomaly detection, freshness and completeness monitoring, cross-dataset consistency, and consolidated quality reporting. These patterns catch data issues early, before they propagate downstream.</p>"},{"location":"quality-testing/#quality-dimensions","title":"Quality Dimensions","text":"Dimension What It Checks DAG Schema Column names, data types, nullable constraints 68 Accuracy Physical bounds, statistical outliers, distribution shifts 69 Freshness File modification time within expected window 70 Completeness Row counts, date coverage, null rates 70 Consistency Referential integrity, cross-table rules, temporal validity 71"},{"location":"quality-testing/#shared-quality-module","title":"Shared Quality Module","text":"<p>All quality DAGs share <code>src/airflow_examples/quality.py</code> which provides a <code>QualityResult</code> dataclass and reusable check functions:</p> <pre><code>@dataclass\nclass QualityResult:\n    check_name: str       # \"schema_columns\", \"bounds_temperature_c\", etc.\n    passed: bool          # True if check passed\n    details: str          # Human-readable description\n    severity: str         # \"info\", \"warning\", \"critical\"\n</code></pre> <p>Check functions return <code>QualityResult</code> objects that can be aggregated:</p> <pre><code>check_schema(df, expected_columns) -&gt; QualityResult\ncheck_bounds(df, column, min_val, max_val) -&gt; QualityResult\ncheck_nulls(df, columns, max_null_pct) -&gt; QualityResult\ncheck_freshness(file_path, max_age_seconds) -&gt; QualityResult\ncheck_row_count(df, min_rows, max_rows) -&gt; QualityResult\n</code></pre> <p>Why a dataclass? Uniform structure makes it easy to aggregate results from different checks, compute overall scores, and format reports. Every check returns the same shape regardless of what it validates.</p>"},{"location":"quality-testing/#schema-as-code","title":"Schema-as-Code","text":"<p>DAG 68 validates data against an expected schema definition. The schema is defined in code (not a separate file), making it version-controlled and testable:</p> <pre><code>EXPECTED_COLUMNS = [\"station\", \"date\", \"temperature_c\", \"humidity_pct\", \"pressure_hpa\"]\n\n# Column presence check\nresult = check_schema(df, EXPECTED_COLUMNS)\n\n# Type validation per column\nnumeric_temp = pd.to_numeric(df[\"temperature_c\"], errors=\"coerce\")\nnon_numeric = numeric_temp.isna() &amp; df[\"temperature_c\"].notna()\n</code></pre>"},{"location":"quality-testing/#statistical-methods","title":"Statistical Methods","text":"<p>DAG 69 uses three statistical techniques:</p> <p>Bounds checking: Verify values fall within physically possible ranges. Temperature can't be 100C, humidity can't be negative:</p> <pre><code>check_bounds(df, \"temperature_c\", min_val=-50.0, max_val=60.0)\n</code></pre> <p>Z-score outlier detection: Flag individual values more than 3 standard deviations from the column mean:</p> <pre><code>z = |x - mean| / std\noutlier if z &gt; 3\n</code></pre> <p>Distribution shift detection: Compare first-half vs second-half statistics. A sudden change in the mean indicates a data source problem:</p> <pre><code>mean_diff = abs(second_half.mean() - first_half.mean())\nshifted = mean_diff &gt; first_half.std() * 1.5\n</code></pre>"},{"location":"quality-testing/#traffic-light-reporting","title":"Traffic-Light Reporting","text":"<p>DAG 70 uses a green/yellow/red reporting pattern for at-a-glance status:</p> <pre><code>if passed == total:\n    light = \"GREEN\"    # All checks passed\nelif passed &gt;= total // 2:\n    light = \"YELLOW\"   # Some failures\nelse:\n    light = \"RED\"      # Majority failing\n</code></pre>"},{"location":"quality-testing/#cross-dataset-validation","title":"Cross-Dataset Validation","text":"<p>DAG 71 validates relationships between master (stations) and detail (observations) tables:</p> <ul> <li>Referential integrity: All <code>station_id</code> values in observations exist in stations master</li> <li>Value consistency: Arctic stations (lat &gt; 60) shouldn't report tropical temperatures</li> <li>Temporal consistency: Observation dates fall within station's <code>active_from..active_to</code></li> </ul>"},{"location":"quality-testing/#quality-scoring","title":"Quality Scoring","text":"<p>DAG 72 aggregates all check types and computes an overall quality score:</p> <pre><code>score = (passed / total * 100)\nstatus = \"HEALTHY\" if score &gt;= 90 else \"DEGRADED\" if score &gt;= 70 else \"CRITICAL\"\n</code></pre>"},{"location":"quality-testing/#per-dag-reference","title":"Per-DAG Reference","text":""},{"location":"quality-testing/#68-schema-validation","title":"68 -- Schema Validation","text":"<p>Column names, value types, and nullability checks against expected schema.</p> <pre><code>generate_data -&gt; validate_columns \u2500\u2510\n              -&gt; validate_types \u2500\u2500\u2500\u253c-&gt; schema_report -&gt; cleanup\n              -&gt; validate_nullability \u2500\u2518\n</code></pre> <p>See: <code>dags/68_schema_validation.py</code></p>"},{"location":"quality-testing/#69-statistical-checks","title":"69 -- Statistical Checks","text":"<p>Bounds checking, z-score outlier detection, and distribution shift detection.</p> <pre><code>generate_data -&gt; check_bounds \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              -&gt; check_zscore \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c-&gt; anomaly_report -&gt; cleanup\n              -&gt; check_distribution \u2500\u2500\u2500\u2518\n</code></pre> <p>See: <code>dags/69_statistical_checks.py</code></p>"},{"location":"quality-testing/#70-freshness-completeness","title":"70 -- Freshness &amp; Completeness","text":"<p>File freshness, row counts, date gap detection, and null rate thresholds.</p> <pre><code>generate_data -&gt; check_freshness \u2500\u2500\u2500\u2500\u2510\n              -&gt; check_row_counts \u2500\u2500\u2500\u253c-&gt; quality_summary -&gt; cleanup\n              -&gt; check_date_coverage \u2524\n              -&gt; check_null_rates \u2500\u2500\u2500\u2518\n</code></pre> <p>See: <code>dags/70_freshness_completeness.py</code></p>"},{"location":"quality-testing/#71-cross-dataset-validation","title":"71 -- Cross-Dataset Validation","text":"<p>Referential integrity, value consistency, and temporal consistency between datasets.</p> <pre><code>generate_datasets -&gt; check_referential_integrity \u2500\u2510\n                  -&gt; check_value_consistency \u2500\u2500\u2500\u2500\u2500\u2500\u253c-&gt; integrity_report -&gt; cleanup\n                  -&gt; check_temporal_consistency \u2500\u2500\u2500\u2518\n</code></pre> <p>See: <code>dags/71_cross_dataset_validation.py</code></p>"},{"location":"quality-testing/#72-quality-dashboard","title":"72 -- Quality Dashboard","text":"<p>Consolidated report aggregating schema, statistical, and freshness checks with overall scoring.</p> <pre><code>generate_data -&gt; run_schema_checks \u2500\u2500\u2500\u2500\u2500\u2510\n              -&gt; run_statistical_checks \u2500\u253c-&gt; compile_report -&gt; cleanup\n              -&gt; run_freshness_checks \u2500\u2500\u2500\u2518\n</code></pre> <p>See: <code>dags/72_quality_report.py</code></p>"},{"location":"quality-testing/#alerting-sla-monitoring","title":"Alerting &amp; SLA Monitoring","text":"<p>DAGs 73--76 demonstrate production alerting patterns: SLA tracking with execution timeouts, webhook notifications to external services, progressive failure escalation with callbacks, and pipeline health monitoring as a meta-DAG.</p>"},{"location":"quality-testing/#airflow-callback-types","title":"Airflow Callback Types","text":"<p>Airflow provides several callback hooks at the task and DAG level:</p> Callback Trigger Use Case <code>on_success_callback</code> Task completes successfully Log success, clear alerts, send notification <code>on_failure_callback</code> Task fails after all retries Page on-call, send critical alert <code>on_retry_callback</code> Task fails but will retry Log warning, increment retry counter DAG <code>on_success_callback</code> All tasks in DAG complete Send pipeline-complete notification DAG <code>on_failure_callback</code> Any task in DAG fails permanently Send pipeline-failed alert <p>Callbacks receive a <code>context</code> dict with task instance, execution date, and other metadata:</p> <pre><code>def on_failure_callback(context):\n    ti = context[\"task_instance\"]\n    print(f\"Task {ti.task_id} failed on attempt {ti.try_number}\")\n</code></pre>"},{"location":"quality-testing/#execution-timeout-vs-sla","title":"Execution Timeout vs SLA","text":"Mechanism Scope Behavior <code>execution_timeout</code> Single task Kills task if it exceeds duration, raises <code>AirflowTaskTimeout</code> Duration tracking Custom Compare actual vs expected timing in a report task <code>on_failure_callback</code> Single task Fires when task fails (including timeout) <pre><code>@task(execution_timeout=timedelta(seconds=30))\ndef slow_task():\n    \"\"\"Will be killed if it runs longer than 30 seconds.\"\"\"\n    time.sleep(5)\n</code></pre>"},{"location":"quality-testing/#webhook-integration","title":"Webhook Integration","text":"<p>DAG 74 uses <code>requests.post()</code> to send JSON payloads to httpbin (simulating Slack/Teams):</p> <pre><code>payload = {\n    \"event\": \"pipeline_success\",\n    \"dag_id\": \"74_webhook_notifications\",\n    \"timestamp\": datetime.now().isoformat(),\n    \"stats\": {\"rows_processed\": 500},\n}\nrequests.post(\"http://httpbin:8080/post\", json=payload, timeout=10)\n</code></pre> <p>Conditional notifications use <code>trigger_rule</code>:</p> <ul> <li><code>trigger_rule=\"one_failed\"</code> -- only runs if an upstream task failed (failure handler)</li> <li><code>trigger_rule=\"all_done\"</code> -- always runs regardless of upstream success/failure (final status)</li> </ul>"},{"location":"quality-testing/#failure-escalation-strategy","title":"Failure Escalation Strategy","text":"<p>DAG 75 demonstrates progressive escalation:</p> <pre><code>Attempt 1: Fail -&gt; on_retry_callback (log warning)\nAttempt 2: Fail -&gt; on_retry_callback (escalate warning)\nAttempt 3: Succeed -&gt; on_success_callback (log recovery)\n     or\nAttempt 3: Fail -&gt; on_failure_callback (critical alert, page on-call)\n</code></pre> <p>The <code>try_number</code> attribute on the task instance tracks which attempt is running:</p> <pre><code>@task(retries=2, retry_delay=timedelta(seconds=2),\n      on_retry_callback=on_retry, on_failure_callback=on_failure)\ndef unreliable_task():\n    ...\n</code></pre>"},{"location":"quality-testing/#health-check-pattern","title":"Health Check Pattern","text":"<p>DAG 76 is a meta-DAG that monitors other pipelines by checking their output artifacts:</p> <ol> <li>Existence: Do expected output files exist?</li> <li>Freshness: Were they modified within the expected time window?</li> <li>Size: Are they non-empty and within expected bounds?</li> </ol> <pre><code>EXPECTED_OUTPUTS = [\n    {\"path\": \"/output/pipeline_a.csv\", \"max_age_s\": 3600, \"min_size\": 100},\n    {\"path\": \"/output/pipeline_b.parquet\", \"max_age_s\": 3600, \"min_size\": 200},\n]\n</code></pre> <p>If any checks fail, the health status is computed (healthy/degraded/critical) and a webhook alert is sent conditionally.</p>"},{"location":"quality-testing/#per-dag-reference_1","title":"Per-DAG Reference","text":""},{"location":"quality-testing/#73-sla-monitoring","title":"73 -- SLA Monitoring","text":"<p>Task-level <code>execution_timeout</code>, duration tracking, and SLA breach reporting.</p> <pre><code>fast_task \u2500\u2500\u2500\u2500\u2500\u2510\nslow_task \u2500\u2500\u2500\u2500\u2500\u253c-&gt; sla_report\ncritical_task \u2500\u2518\n</code></pre> <p>See: <code>dags/73_sla_monitoring.py</code></p>"},{"location":"quality-testing/#74-webhook-notifications","title":"74 -- Webhook Notifications","text":"<p>POST JSON alerts to httpbin on start, success, and conditional failure.</p> <pre><code>send_start -&gt; process_data -&gt; send_success \u2500\u2500\u2510\n                           -&gt; handle_failure \u2500\u253c-&gt; final_status\n</code></pre> <p>See: <code>dags/74_webhook_notifications.py</code></p>"},{"location":"quality-testing/#75-failure-escalation","title":"75 -- Failure Escalation","text":"<p>Retry -&gt; warn -&gt; alert -&gt; recover with progressive callbacks.</p> <pre><code>unreliable_task (retries=2) \u2500\u2510\nalways_succeeds \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c-&gt; escalation_summary\n</code></pre> <p>See: <code>dags/75_failure_escalation.py</code></p>"},{"location":"quality-testing/#76-pipeline-health-check","title":"76 -- Pipeline Health Check","text":"<p>Meta-monitoring: file existence, freshness, size, conditional webhook alerting.</p> <pre><code>setup_test_files -&gt; check_file_existence \u2500\u2510\n                 -&gt; check_file_freshness \u2500\u2500\u253c-&gt; health_summary -&gt; alert_if_unhealthy -&gt; cleanup\n                 -&gt; check_file_sizes \u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>See: <code>dags/76_pipeline_health_check.py</code></p>"},{"location":"quality-testing/#dag-testing-patterns","title":"DAG Testing Patterns","text":"<p>DAGs 77--80 demonstrate how to structure DAGs for testability: separating business logic from DAG wiring, mocking external services, using <code>dag.test()</code> for local development, and building parameterized pipelines driven by DAG <code>params</code>.</p>"},{"location":"quality-testing/#testable-dag-architecture","title":"Testable DAG Architecture","text":"<p>The key principle: keep DAG files thin. All business logic lives in importable Python modules that have no Airflow dependencies. DAG files are pure wiring:</p> <pre><code>src/airflow_examples/etl_logic.py    &lt;-- Pure functions (testable)\ndags/77_testable_dag_pattern.py      &lt;-- Thin wiring (calls etl_logic)\ntests/test_etl_logic.py              &lt;-- Unit tests (no Airflow needed)\n</code></pre> <p>The helper module <code>src/airflow_examples/etl_logic.py</code> provides pure functions:</p> <pre><code>def extract_weather_data(n_records, seed) -&gt; list[dict]: ...\ndef validate_records(records, required_fields, temp_range) -&gt; tuple[list, list]: ...\ndef transform_records(records) -&gt; list[dict]: ...\ndef load_records(records, output_path) -&gt; dict: ...\ndef fetch_api_data(url, timeout) -&gt; dict: ...\n</code></pre> <p>The DAG file simply calls these functions from <code>@task</code> decorated wrappers:</p> <pre><code>@task\ndef extract():\n    from airflow_examples.etl_logic import extract_weather_data\n    return extract_weather_data(n_records=50, seed=42)\n</code></pre> <p>Why deferred imports? Importing <code>etl_logic</code> inside the <code>@task</code> body means the module is loaded at task execution time, not at DAG parse time. This keeps DAG parsing fast and avoids import errors if the module has heavy dependencies.</p>"},{"location":"quality-testing/#mocking-external-services","title":"Mocking External Services","text":"<p>DAG 78 calls <code>etl_logic.fetch_api_data()</code> which wraps <code>requests.get()</code>. Tests can mock this without touching Airflow:</p> <pre><code>from unittest.mock import patch, MagicMock\n\n@patch(\"airflow_examples.etl_logic.requests.get\")\ndef test_fetch(mock_get):\n    mock_response = MagicMock()\n    mock_response.json.return_value = {\"data\": \"test\"}\n    mock_get.return_value = mock_response\n\n    result = fetch_api_data(\"http://test.com/api\")\n    assert result[\"data\"] == \"test\"\n    mock_get.assert_called_once()\n</code></pre> <p>The key is that <code>fetch_api_data</code> is a thin wrapper around <code>requests.get</code>, making the mock target predictable and the test simple.</p>"},{"location":"quality-testing/#dagtest-for-local-development","title":"dag.test() for Local Development","text":"<p>DAG 79 includes an <code>if __name__ == \"__main__\"</code> block:</p> <pre><code>with DAG(...) as dag:\n    data = generate_data()\n    processed = process(data=data)\n    ...\n\nif __name__ == \"__main__\":\n    dag.test()\n</code></pre> <p>This lets you run the DAG directly: <code>python dags/79_dag_test_runner.py</code>. The <code>dag.test()</code> method executes all tasks sequentially without the scheduler, webserver, or database -- perfect for quick iteration during development.</p>"},{"location":"quality-testing/#parameterized-pipelines","title":"Parameterized Pipelines","text":"<p>DAG 80 uses DAG <code>params</code> for configuration with defaults:</p> <pre><code>DAG_PARAMS = {\n    \"n_records\": 50,\n    \"seed\": 42,\n    \"temp_unit\": \"celsius\",\n    \"output_format\": \"csv\",\n}\n\nwith DAG(..., params=DAG_PARAMS) as dag:\n    ...\n\n@task\ndef extract(**context):\n    params = context[\"params\"]\n    records = extract_weather_data(n_records=params[\"n_records\"])\n</code></pre> <p>Tests can override parameters via <code>run_conf</code>:</p> <pre><code>dag.test(run_conf={\"n_records\": 10, \"output_format\": \"parquet\"})\n</code></pre> <p>This enables config-driven pipelines where the same DAG serves multiple use cases with different parameters.</p>"},{"location":"quality-testing/#per-dag-reference_2","title":"Per-DAG Reference","text":""},{"location":"quality-testing/#77-testable-dag-pattern","title":"77 -- Testable DAG Pattern","text":"<p>Thin DAG wiring with all logic in <code>etl_logic.py</code>.</p> <pre><code>extract -&gt; validate -&gt; transform -&gt; load -&gt; report -&gt; cleanup\n</code></pre> <p>See: <code>dags/77_testable_dag_pattern.py</code></p>"},{"location":"quality-testing/#78-mock-api-pipeline","title":"78 -- Mock API Pipeline","text":"<p>API pipeline with mockable <code>fetch_api_data()</code> for testing.</p> <pre><code>fetch_from_api -&gt; parse_response -&gt; write_output -&gt; report -&gt; cleanup\n</code></pre> <p>See: <code>dags/78_mock_api_pipeline.py</code></p>"},{"location":"quality-testing/#79-dag-test-runner","title":"79 -- DAG Test Runner","text":"<p><code>dag.test()</code> and <code>__main__</code> pattern for local development.</p> <pre><code>generate_data -&gt; process -&gt; validate_output -&gt; summary\n</code></pre> <p>See: <code>dags/79_dag_test_runner.py</code></p>"},{"location":"quality-testing/#80-parameterized-pipeline","title":"80 -- Parameterized Pipeline","text":"<p>Config-driven pipeline with DAG <code>params</code> and overridable defaults.</p> <pre><code>extract -&gt; transform -&gt; validate -&gt; load -&gt; cleanup\n</code></pre> <p>See: <code>dags/80_parameterized_pipeline.py</code></p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#executors-how-tasks-actually-run","title":"Executors: How Tasks Actually Run","text":"<p>You do not need to understand executors to use this project -- <code>LocalExecutor</code> is configured automatically. This section is useful if you are deploying Airflow in production and need to choose how to scale.</p>"},{"location":"reference/#localexecutor","title":"LocalExecutor","text":"<ul> <li>Tasks run as separate processes on the same machine as the scheduler</li> <li>Good for development and small-to-medium workloads</li> <li>Used in this project's <code>compose.yml</code></li> </ul>"},{"location":"reference/#celeryexecutor","title":"CeleryExecutor","text":"<ul> <li>Tasks dispatched to a pool of Celery workers (can span multiple machines)</li> <li>Requires a message broker (Redis/RabbitMQ)</li> <li>Good for medium-to-large workloads with horizontal scaling</li> </ul>"},{"location":"reference/#kubernetesexecutor","title":"KubernetesExecutor","text":"<ul> <li>Each task runs in its own Kubernetes pod</li> <li>Maximum isolation: each task gets its own container, resources, and dependencies</li> <li>Good for heterogeneous workloads with varying resource requirements</li> <li>Tasks can use different Docker images</li> </ul>"},{"location":"reference/#comparison","title":"Comparison","text":"Feature LocalExecutor CeleryExecutor KubernetesExecutor Setup complexity Low Medium High Scalability Single machine Multi-machine Cluster-wide Isolation Process-level Process-level Container-level Resource efficiency High Medium Variable Cold start None None Pod spin-up time Best for Dev/small prod Medium prod Large/varied prod"},{"location":"reference/#airflow-3x-migration-notes","title":"Airflow 3.x Migration Notes","text":"<p>Airflow 3.0 (April 2025) introduced significant breaking changes from 2.x:</p> 2.x 3.x <code>from airflow import DAG</code> <code>from airflow.sdk import DAG</code> <code>from airflow.operators.bash import BashOperator</code> <code>from airflow.providers.standard.operators.bash import BashOperator</code> <code>from airflow.operators.python import PythonOperator</code> <code>from airflow.providers.standard.operators.python import PythonOperator</code> <code>Dataset</code> <code>Asset</code> <code>execution_date</code> in context <code>logical_date</code> in context <code>schedule_interval=\"@daily\"</code> <code>schedule=\"@daily\"</code> SubDAGs TaskGroups (SubDAGs removed)"},{"location":"reference/#architecture-diagrams","title":"Architecture Diagrams","text":""},{"location":"reference/#project-infrastructure","title":"Project Infrastructure","text":"<pre><code>graph TB\n    subgraph Docker Compose\n        PG[(PostgreSQL)]\n        WS[Web Server :8081]\n        SC[Scheduler]\n        TR[Triggerer]\n        HB[httpbin :8088]\n        MP[Mailpit :8025]\n        SSH[SSH Server :2222]\n        RFS[RustFS :9000]\n        SE[StatsD Exporter]\n        PR[Prometheus :9090]\n        GR[Grafana :3000]\n    end\n\n    SC --&gt; PG\n    WS --&gt; PG\n    TR --&gt; PG\n    SC --&gt; HB\n    SC --&gt; MP\n    SC --&gt; SSH\n    SC --&gt; RFS\n    SC --&gt; SE\n    SE --&gt; PR\n    PR --&gt; GR</code></pre>"},{"location":"reference/#data-flow-pattern","title":"Data Flow Pattern","text":"<pre><code>graph LR\n    API[Public API] --&gt; F[Fetch Task]\n    F --&gt; T[Transform Task]\n    T --&gt; V[Validate Task]\n    V --&gt; L[Load Task]\n    L --&gt; CSV[CSV / Parquet]\n    L --&gt; PG[(PostgreSQL)]\n    L --&gt; S3[RustFS / S3]</code></pre>"},{"location":"reference/#dag-84-marine-flood-risk-fan-out-fan-in","title":"DAG 84: Marine + Flood Risk (Fan-out / Fan-in)","text":"<pre><code>graph TD\n    FM[fetch_marine] --&gt; CMR[compute_marine_risk]\n    FF[fetch_flood] --&gt; CFR[compute_flood_risk]\n    CMR --&gt; CR[composite_risk]\n    CFR --&gt; CR\n    CMR --&gt; R[report]\n    CFR --&gt; R\n    CR --&gt; R</code></pre>"},{"location":"reference/#dag-91-earthquake-analysis-parallel-analysis-branches","title":"DAG 91: Earthquake Analysis (Parallel Analysis Branches)","text":"<pre><code>graph TD\n    FE[fetch_earthquakes] --&gt; PG[parse_geojson]\n    PG --&gt; MA[magnitude_analysis]\n    PG --&gt; SA[spatial_analysis]\n    PG --&gt; TA[temporal_analysis]\n    MA --&gt; R[report]\n    SA --&gt; R\n    TA --&gt; R</code></pre>"},{"location":"reference/#dag-98-multi-api-dashboard-6-source-fan-in","title":"DAG 98: Multi-API Dashboard (6-Source Fan-In)","text":"<pre><code>graph TD\n    RC[REST Countries] --&gt; S[stage_raw]\n    OM[Open-Meteo Forecast] --&gt; S\n    AQ[Air Quality API] --&gt; S\n    FR[Frankfurter API] --&gt; S\n    WB[World Bank API] --&gt; S\n    S --&gt; I[integrate]\n    I --&gt; DM[dashboard_metrics]\n    DM --&gt; R[report]</code></pre>"},{"location":"reference/#dag-100-capstone-etl-diamond-dependencies","title":"DAG 100: Capstone ETL (Diamond Dependencies)","text":"<pre><code>graph TD\n    EC[extract_countries] --&gt; BS[build_staging]\n    EI[extract_indicators] --&gt; BS\n    EC --&gt; GSK[generate_surrogate_keys]\n    GSK --&gt; SCD[apply_scd_type2]\n    SCD --&gt; BF[build_fact_table]\n    EI --&gt; BF\n    GSK --&gt; BF\n    BS --&gt; AT[audit_trail]\n    SCD --&gt; AT\n    BF --&gt; AT\n    SCD --&gt; V[validate]\n    BF --&gt; V\n    AT --&gt; R[report]\n    V --&gt; R\n    SCD --&gt; R</code></pre>"},{"location":"reference/#dbt-integration-flow","title":"dbt Integration Flow","text":"<pre><code>graph LR\n    subgraph Airflow DAGs\n        L[101: Load Raw]\n        D[102: dbt Transform]\n    end\n    subgraph dbt Models\n        STG[Staging Models]\n        MART[Mart Models]\n    end\n    subgraph PostgreSQL\n        RAW[raw_countries / raw_indicators]\n        DIM[dim_country]\n        FCT[fct_indicators]\n        AGG[agg_nordic_summary]\n    end\n    L --&gt; RAW\n    RAW --&gt; STG\n    STG --&gt; MART\n    MART --&gt; DIM\n    MART --&gt; FCT\n    MART --&gt; AGG\n    D -.-&gt;|orchestrates| STG\n    D -.-&gt;|orchestrates| MART</code></pre>"},{"location":"reference/#dag-105-objectstoragepath-pipeline","title":"DAG 105: ObjectStoragePath Pipeline","text":"<pre><code>graph TD\n    EB[ensure_bucket] --&gt; WP[write_parquet]\n    WP --&gt; LF[list_files]\n    WP --&gt; RT[read_and_transform]\n    WP --&gt; CG[copy_to_gold]\n    LF --&gt; R[report]\n    RT --&gt; R\n    CG --&gt; R</code></pre>"},{"location":"reference/#data-lake-layers-rustfs","title":"Data Lake Layers (RustFS)","text":"<pre><code>graph LR\n    API[APIs] --&gt; B[Bronze Layer]\n    B --&gt; S[Silver Layer]\n    S --&gt; G[Gold Layer]\n\n    subgraph RustFS Buckets\n        B --&gt;|Raw JSON| B1[airflow-data/bronze/]\n        S --&gt;|Cleaned Parquet| B2[airflow-data/silver/]\n        G --&gt;|Aggregated| B3[airflow-data/gold/]\n    end</code></pre>"},{"location":"reference/#dag-106-hitl-weather-quality-review","title":"DAG 106: HITL Weather Quality Review","text":"<pre><code>graph TD\n    FW[fetch_weather_data] --&gt; AN[add_analyst_note]\n    AN --&gt; QR[choose_quality_rating]\n    QR --&gt; AP[approve_publication]\n    AP --&gt; CO[choose_output_format]\n    CO --&gt; EC[export_csv]\n    CO --&gt; EJ[export_json]\n    CO --&gt; EP[export_parquet]\n    EC --&gt; JE[join_exports]\n    EJ --&gt; JE\n    EP --&gt; JE\n    JE --&gt; PR[publish_report]\n\n    style AN fill:#f9e79f\n    style QR fill:#f9e79f\n    style AP fill:#f9e79f\n    style CO fill:#f9e79f</code></pre>"},{"location":"reference/#infrastructure","title":"Infrastructure","text":""},{"location":"reference/#how-make-run-works","title":"How <code>make run</code> Works","text":"<p>Behind the scenes, <code>make run</code> does the following:</p> <ol> <li>Starts Docker containers -- PostgreSQL (database), Airflow (scheduler + web server +    triggerer), plus helper services (mock HTTP server, mail catcher, SSH server)</li> <li>Initializes Airflow -- creates the database tables, the admin user, and default    connections</li> <li>Runs all 107 DAGs -- executes each one in order using <code>airflow dags test</code></li> <li>Shows logs -- tails the scheduler logs so you can watch what is happening</li> <li>Cleans up on Ctrl+C -- stops and removes all containers</li> </ol> <pre><code>make run            # Does everything above\nmake stop           # Force-stop services if something goes wrong\n</code></pre>"},{"location":"reference/#running-a-single-dag","title":"Running a Single DAG","text":"<p>If you want to run just one DAG (while services are running from <code>make run</code>):</p> <pre><code># The pattern is: docker compose exec airflow-scheduler airflow dags test &lt;dag_id&gt; &lt;date&gt;\ndocker compose exec airflow-scheduler airflow dags test 01_hello_world 2025-01-01\n</code></pre>"},{"location":"reference/#testing-without-docker","title":"Testing Without Docker","text":"<p>Unit tests do not need Docker -- they use a lightweight in-memory database:</p> <pre><code>make test           # pytest with SQLite, no Docker needed\n</code></pre>"},{"location":"reference/#links","title":"Links","text":"<ul> <li>Apache Airflow documentation</li> <li>Airflow on GitHub</li> <li>Airflow 3.0 release notes</li> <li>Airflow providers index</li> <li>Docker provider</li> </ul>"}]}