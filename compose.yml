x-airflow-env: &airflow-env
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  AIRFLOW__CORE__FERNET_KEY: ""
  AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
  AIRFLOW__API__HIDE_PAUSED_DAGS_BY_DEFAULT: "false"
  AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: "admin:admin"
  AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS: "true"
  AIRFLOW__API__SECRET_KEY: "airflow-examples-secret-key"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
  AIRFLOW__SMTP__SMTP_HOST: mailpit
  AIRFLOW__SMTP__SMTP_PORT: "1025"
  AIRFLOW__SMTP__SMTP_MAIL_FROM: airflow@example.com
  AIRFLOW__METRICS__STATSD_ON: "true"
  AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
  AIRFLOW__METRICS__STATSD_PORT: "9125"
  _PIP_ADDITIONAL_REQUIREMENTS: "dbt-core dbt-postgres boto3 s3fs"
  AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
  AIRFLOW__API_AUTH__JWT_SECRET: airflow-examples-jwt-secret
  PYTHONPATH: /opt/airflow/src

x-airflow-volumes: &airflow-volumes
  - ./dags:/opt/airflow/dags
  - ./src:/opt/airflow/src
  - ./logs:/opt/airflow/logs
  - ./target:/opt/airflow/target
  - ./dbt_project:/opt/airflow/dbt_project
  - /var/run/docker.sock:/var/run/docker.sock

services:
  postgres:
    image: postgres:17-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      timeout: 3s
      retries: 5

  httpbin:
    image: mccutchen/go-httpbin:v2.15.0
    ports:
      - "8088:8080"

  mailpit:
    image: axllent/mailpit:v1.21
    ports:
      - "8025:8025"
      - "1025:1025"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8025"]
      interval: 5s
      timeout: 3s
      retries: 5

  ssh-server:
    image: lscr.io/linuxserver/openssh-server:latest
    environment:
      USER_NAME: airflow
      USER_PASSWORD: airflow
      PASSWORD_ACCESS: "true"
    ports:
      - "2222:2222"
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 2222"]
      interval: 5s
      timeout: 3s
      retries: 5

  rustfs:
    image: rustfs/rustfs:latest
    init: true
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      RUSTFS_ACCESS_KEY: airflow
      RUSTFS_SECRET_KEY: airflow123

  statsd-exporter:
    image: prom/statsd-exporter:v0.27.2
    ports:
      - "9102:9102"
      - "9125:9125/udp"
    command: --statsd.mapping-config=/etc/statsd/mapping.yml
    volumes:
      - ./monitoring/statsd-mapping.yml:/etc/statsd/mapping.yml

  prometheus:
    image: prom/prometheus:v3.2.1
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:11.5.2
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_AUTH_ANONYMOUS_ENABLED: "true"
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards

  airflow-init:
    image: apache/airflow:3.1.7
    depends_on:
      postgres:
        condition: service_healthy
      httpbin:
        condition: service_started
      mailpit:
        condition: service_healthy
    environment:
      <<: *airflow-env
    command: ["bash", "-c", "airflow db migrate && airflow connections delete postgres_default 2>/dev/null; airflow connections add postgres_default --conn-uri 'postgresql://airflow:airflow@postgres:5432/airflow' && airflow connections delete http_default 2>/dev/null; airflow connections add http_default --conn-uri 'http://httpbin:8080' && airflow connections delete smtp_default 2>/dev/null; airflow connections add smtp_default --conn-uri 'smtp://mailpit:1025' --conn-extra '{\"disable_tls\": true, \"disable_ssl\": true}' && airflow connections delete ssh_default 2>/dev/null; airflow connections add ssh_default --conn-uri 'ssh://airflow:airflow@ssh-server:2222' && airflow connections delete fs_default 2>/dev/null; airflow connections add fs_default --conn-type fs --conn-extra '{\"path\": \"/\"}' && airflow connections delete aws_default 2>/dev/null; airflow connections add aws_default --conn-type aws --conn-extra '{\"endpoint_url\": \"http://rustfs:9000\", \"aws_access_key_id\": \"airflow\", \"aws_secret_access_key\": \"airflow123\"}' && airflow connections delete dhis2_default 2>/dev/null; airflow connections add dhis2_default --conn-type http --conn-host 'https://play.im.dhis2.org/dev' --conn-login admin --conn-password district && echo 'All connections configured'"]

  airflow-scheduler:
    image: apache/airflow:3.1.7
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    command: bash -c "airflow dags reserialize && exec airflow scheduler"
    volumes: *airflow-volumes
    restart: unless-stopped

  airflow-webserver:
    image: apache/airflow:3.1.7
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    command: airflow api-server
    ports:
      - "8081:8080"
    volumes: *airflow-volumes
    restart: unless-stopped

  airflow-triggerer:
    image: apache/airflow:3.1.7
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-env
    command: airflow triggerer
    volumes: *airflow-volumes
    restart: unless-stopped
